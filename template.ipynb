{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/chorltonm/fa-cup-upsets/blob/main/template.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nEuX80dPfnRR"
      },
      "outputs": [],
      "source": [
        "# Import Libaries\n",
        "import os\n",
        "import sys\n",
        "import json\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import importlib\n",
        "import pandas_gbq\n",
        "\n",
        "\n",
        "from google.cloud import bigquery\n",
        "from google.oauth2 import service_account\n",
        "from google.colab import drive\n",
        "from google.colab import userdata\n",
        "\n",
        "# Install scikit learn\n",
        "from sklearn.model_selection import StratifiedShuffleSplit, StratifiedKFold\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gl-LEV7vfuVs"
      },
      "outputs": [],
      "source": [
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Change default ouput directory\n",
        "os.chdir('/content/drive/MyDrive/birkbeck_msc-project/output_files')\n",
        "\n",
        "# Change default ouput directory\n",
        "os.chdir('/content/drive/MyDrive/birkbeck_msc-project/python_files')\n",
        "\n",
        "\n",
        "# Import user defined python functions\n",
        "import model_evaluation_functions\n",
        "import ratings_functions\n",
        "importlib.reload(model_evaluation_functions)\n",
        "importlib.reload(ratings_functions)\n",
        "\n",
        "from ratings_functions import *\n",
        "from model_evaluation_functions import *\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VeWnIKHFi_uN"
      },
      "outputs": [],
      "source": [
        "# Authentication credentials and keys\n",
        "\n",
        "# Google Service Account\n",
        "\n",
        "# Load the JSON key from local Google Collab file\n",
        "key = json.load(open('/content/drive/MyDrive/service_account.json', 'r'))\n",
        "\n",
        "# Authenticate using the loaded key\n",
        "credentials = service_account.Credentials.from_service_account_info(key)\n",
        "\n",
        "# Set up the BigQuery client with the credentials to project\n",
        "client = bigquery.Client(credentials=credentials, project='birkbeck-msc-project-422917')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def load_predictions_and_ratings_to_bigquery(client, predictions_df, fold_ratings_df, ratings_model):\n",
        "\n",
        "  # Load predictions to Big Query\n",
        "  # Specify the target table\n",
        "  load_dataset_name = 'analysis_layer'\n",
        "  load_table_name = 'ratings_predictions'\n",
        "  load_table_ref = f\"{load_dataset_name}.{load_table_name}\"\n",
        "\n",
        "  # Delete predicitons method already inserted\n",
        "  delete_query = f\"DELETE FROM `{load_dataset_name}.{load_table_name}` WHERE ratings_model = '{ratings_model}'\"\n",
        "  delete_job = client.query(delete_query)\n",
        "  delete_result = delete_job.result()\n",
        "  predictions_total_rows_deleted = delete_result.num_dml_affected_rows\n",
        "  print(f\"Big Query target predictions table {load_dataset_name}.{load_table_name} rows deleted: {predictions_total_rows_deleted}\")\n",
        "\n",
        "  # Insert predictions for ratings model data to the existing table\n",
        "  job_config = bigquery.LoadJobConfig(\n",
        "          write_disposition=\"WRITE_APPEND\")\n",
        "  load_job = client.load_table_from_dataframe(\n",
        "          predictions_df, load_table_ref, job_config=job_config)\n",
        "  # Wait for the job to complete\n",
        "  load_job.result()\n",
        "  predictions_num_rows_inserted = load_job.output_rows\n",
        "  print(f\"{predictions_num_rows_inserted} rows appended to predictions table {load_table_ref} successfully.\")\n",
        "\n",
        "  # Load fold ratings to Big Query\n",
        "\n",
        "  # Specify the target table\n",
        "  load_dataset_name = 'analysis_layer'\n",
        "  load_table_name = 'ratings'\n",
        "  load_table_ref = f\"{load_dataset_name}.{load_table_name}\"\n",
        "\n",
        "  # Delete ratings method already inserted\n",
        "  delete_query = f\"DELETE FROM `{load_dataset_name}.{load_table_name}` WHERE ratings_model = '{ratings_model}'\"\n",
        "  delete_job = client.query(delete_query)\n",
        "  delete_result = delete_job.result()\n",
        "  ratings_total_rows_deleted = delete_result.num_dml_affected_rows\n",
        "  print(f\"Big Query target ratings table {load_dataset_name}.{load_table_name} rows deleted: {ratings_total_rows_deleted}\")\n",
        "\n",
        "\n",
        "  # Insert ratings for ratings model data to the existing table\n",
        "  job_config = bigquery.LoadJobConfig(\n",
        "          write_disposition=\"WRITE_APPEND\")\n",
        "  load_job = client.load_table_from_dataframe(\n",
        "          ratings_df, load_table_ref, job_config=job_config)\n",
        "\n",
        "  load_job.result()  # Wait for the job to complete\n",
        "\n",
        "  ratings_num_rows_inserted = load_job.output_rows\n",
        "  print(f\"{ratings_num_rows_inserted} rows appended to ratings table {load_table_ref} successfully.\")\n",
        "\n",
        "  return\n"
      ],
      "metadata": {
        "id": "IJkpPc-4oBLF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zkSB4iD1gE8P"
      },
      "outputs": [],
      "source": [
        "# FA Cup Data 13 season from 08/09 to 21/20\n",
        "\n",
        "fa_cup_scores = \"\"\"\n",
        "    select * from preparation_layer.view_fa_cup_scores order by sort_order ASC\n",
        "\"\"\"\n",
        "\n",
        "fa_cup_scores_df = client.query(fa_cup_scores).to_dataframe()\n",
        "display(fa_cup_scores_df)\n",
        "\n",
        "# Results & Rank Dataframes\n",
        "all_ranks_df = pd.DataFrame(columns=['team_no','fold_number'])\n",
        "display(all_ranks_df)\n",
        "#def create_all_ranks_df():\n",
        " #   global all_ranks_df\n",
        "  #  all_ranks_df = pd.DataFrame(columns=['team_no'])\n",
        "\n",
        "#create_all_ranks_df()\n",
        "#display(all_ranks_df)\n",
        "\n",
        "all_results_df = pd.DataFrame(columns=['metric_id', 'metric'])\n",
        "display(all_results_df)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Basic Position Ratings\n",
        "ratings_model = 'basic_position'\n",
        "ratings_function = basic_position_ratings\n",
        "num_folds = 5\n",
        "\n",
        "# Call ratings function and run cross validation\n",
        "\n",
        "results = run_rating_models_cross_validation (fa_cup_scores_df, ratings_function, ratings_model, num_folds)\n",
        "\n",
        "# Display results\n",
        "display(results['predictions_df'])\n",
        "display(results['upset_accuracy_scores_df'])\n",
        "display(results['ratings_df'])\n",
        "print(f\"Mean Upset Accuracy: {results['mean_upset_accuracy']:.4f}\")\n",
        "print(f\"Standard Deviation of Upset Accuracy: {results['std_upset_accuracy']:.4f}\")\n",
        "\n",
        "# Load predictions and ratinbgs to Google Big Query\n",
        "predictions_df = results['predictions_df']\n",
        "ratings_df = results['ratings_df']\n",
        "load_predictions_and_ratings_to_bigquery(client, predictions_df, ratings_df, ratings_model)\n",
        "\n",
        "# Create dataframe to compare rank and use for final rank aggregation\n",
        "rank_df = ratings_df[['team_no', 'fold_number', 'rank']]\n",
        "rank_df = rank_df.rename(columns={'rank': ratings_model})\n",
        "all_ranks_df = all_ranks_df.merge(rank_df, on=['team_no','fold_number'], how='outer')\n",
        "display(all_ranks_df)\n",
        "\n",
        "\n",
        "# Create confusion matrix from results\n",
        "results_df, cm_fig, roc_fig = create_model_results_df(results['all_actual_upsets'],results['all_predicted_upsets'], results['accuracies'], results['all_upset_probabilities'], ratings_model)\n",
        "results_df = results_df.reset_index()\n",
        "results_df['metric_id'] = results_df.index + 1\n",
        "results_df = results_df[['metric_id', 'metric', ratings_model]]\n",
        "\n",
        "all_results_df = all_results_df.merge(results_df, on=['metric_id', 'metric'], how='outer')\n",
        "display(all_results_df)\n",
        "\n"
      ],
      "metadata": {
        "id": "OK204TqPmAsc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Massey Ratings\n",
        "ratings_model = 'massey'\n",
        "ratings_function = massey_ratings\n",
        "num_folds = 5\n",
        "\n",
        "# Call ratings function and run cross validation\n",
        "results = run_rating_models_cross_validation (fa_cup_scores_df, ratings_function, ratings_model, num_folds)\n",
        "\n",
        "# Display results\n",
        "display(results['predictions_df'])\n",
        "display(results['upset_accuracy_scores_df'])\n",
        "display(results['ratings_df'])\n",
        "print(f\"Mean Upset Accuracy: {results['mean_upset_accuracy']:.4f}\")\n",
        "print(f\"Standard Deviation of Upset Accuracy: {results['std_upset_accuracy']:.4f}\")\n",
        "\n",
        "# Load predictions and ratinbgs to Google Big Query\n",
        "predictions_df = results['predictions_df']\n",
        "ratings_df = results['ratings_df']\n",
        "load_predictions_and_ratings_to_bigquery(client, predictions_df, ratings_df, ratings_model)\n",
        "\n",
        "# Create dataframe to compare rank and use for final rank aggregation\n",
        "rank_df = ratings_df[['team_no', 'fold_number', 'rank']]\n",
        "rank_df = rank_df.rename(columns={'rank': ratings_model})\n",
        "all_ranks_df = all_ranks_df.merge(rank_df, on=['team_no','fold_number'], how='outer')\n",
        "display(all_ranks_df)\n",
        "\n",
        "# Create confusion matrix from results\n",
        "results_df, cm_fig, roc_fig = create_model_results_df(results['all_actual_upsets'],results['all_predicted_upsets'], results['accuracies'], results['all_upset_probabilities'], ratings_model)\n",
        "results_df = results_df.reset_index()\n",
        "results_df['metric_id'] = results_df.index + 1\n",
        "results_df = results_df[['metric_id', 'metric', ratings_model]]\n",
        "\n",
        "all_results_df = all_results_df.merge(results_df, on=['metric_id', 'metric'], how='outer')\n",
        "display(all_results_df)"
      ],
      "metadata": {
        "id": "vBLs14eJFTeT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Colley Ratings\n",
        "ratings_model = 'colley'\n",
        "ratings_function = colley_ratings\n",
        "num_folds = 5\n",
        "\n",
        "# Call ratings function and run cross validation\n",
        "results = run_rating_models_cross_validation (fa_cup_scores_df, ratings_function, ratings_model, num_folds)\n",
        "\n",
        "# Display results\n",
        "display(results['predictions_df'])\n",
        "display(results['upset_accuracy_scores_df'])\n",
        "display(results['ratings_df'])\n",
        "print(f\"Mean Upset Accuracy: {results['mean_upset_accuracy']:.4f}\")\n",
        "print(f\"Standard Deviation of Upset Accuracy: {results['std_upset_accuracy']:.4f}\")\n",
        "\n",
        "# Load predictions and ratinbgs to Google Big Query\n",
        "predictions_df = results['predictions_df']\n",
        "ratings_df = results['ratings_df']\n",
        "load_predictions_and_ratings_to_bigquery(client, predictions_df, ratings_df, ratings_model)\n",
        "\n",
        "# Create dataframe to compare rank and use for final rank aggregation\n",
        "rank_df = ratings_df[['team_no', 'fold_number', 'rank']]\n",
        "rank_df = rank_df.rename(columns={'rank': ratings_model})\n",
        "all_ranks_df = all_ranks_df.merge(rank_df, on=['team_no','fold_number'], how='outer')\n",
        "display(all_ranks_df)\n",
        "\n",
        "# Create confusion matrix from results\n",
        "results_df, cm_fig, roc_fig = create_model_results_df(results['all_actual_upsets'],results['all_predicted_upsets'], results['accuracies'], results['all_upset_probabilities'], ratings_model)\n",
        "results_df = results_df.reset_index()\n",
        "results_df['metric_id'] = results_df.index + 1\n",
        "results_df = results_df[['metric_id', 'metric', ratings_model]]\n",
        "\n",
        "all_results_df = all_results_df.merge(results_df, on=['metric_id', 'metric'], how='outer')\n",
        "display(all_results_df)\n",
        "\n"
      ],
      "metadata": {
        "id": "QUqGW9jxJNB5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Keener Ratings\n",
        "ratings_model = 'keener'\n",
        "ratings_function = keener_ratings\n",
        "num_folds = 5\n",
        "\n",
        "# Call ratings function and run cross validation\n",
        "results = run_rating_models_cross_validation (fa_cup_scores_df, ratings_function, ratings_model, num_folds)\n",
        "\n",
        "# Display results\n",
        "display(results['predictions_df'])\n",
        "display(results['upset_accuracy_scores_df'])\n",
        "display(results['ratings_df'])\n",
        "print(f\"Mean Upset Accuracy: {results['mean_upset_accuracy']:.4f}\")\n",
        "print(f\"Standard Deviation of Upset Accuracy: {results['std_upset_accuracy']:.4f}\")\n",
        "\n",
        "# Load predictions and ratinbgs to Google Big Query\n",
        "predictions_df = results['predictions_df']\n",
        "ratings_df = results['ratings_df']\n",
        "load_predictions_and_ratings_to_bigquery(client, predictions_df, ratings_df, ratings_model)\n",
        "\n",
        "# Create dataframe to compare rank and use for final rank aggregation\n",
        "rank_df = ratings_df[['team_no', 'fold_number', 'rank']]\n",
        "rank_df = rank_df.rename(columns={'rank': ratings_model})\n",
        "all_ranks_df = all_ranks_df.merge(rank_df, on=['team_no','fold_number'], how='outer')\n",
        "display(all_ranks_df)\n",
        "\n",
        "# Create confusion matrix from results\n",
        "results_df, cm_fig, roc_fig = create_model_results_df(results['all_actual_upsets'],results['all_predicted_upsets'], results['accuracies'], results['all_upset_probabilities'], ratings_model)\n",
        "results_df = results_df.reset_index()\n",
        "results_df['metric_id'] = results_df.index + 1\n",
        "results_df = results_df[['metric_id', 'metric', ratings_model]]\n",
        "\n",
        "all_results_df = all_results_df.merge(results_df, on=['metric_id', 'metric'], how='outer')\n",
        "display(all_results_df)"
      ],
      "metadata": {
        "id": "fsnmZySZRMvk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Trueskill Ratings\n",
        "ratings_model = 'trueskill'\n",
        "ratings_function = trueskill_ratings\n",
        "num_folds = 5\n",
        "\n",
        "# Call ratings function and run cross validation\n",
        "results = run_rating_models_cross_validation (fa_cup_scores_df, ratings_function, ratings_model, num_folds)\n",
        "\n",
        "# Display results\n",
        "display(results['predictions_df'])\n",
        "display(results['upset_accuracy_scores_df'])\n",
        "display(results['ratings_df'])\n",
        "print(f\"Mean Upset Accuracy: {results['mean_upset_accuracy']:.4f}\")\n",
        "print(f\"Standard Deviation of Upset Accuracy: {results['std_upset_accuracy']:.4f}\")\n",
        "\n",
        "# Load predictions and ratinbgs to Google Big Query\n",
        "predictions_df = results['predictions_df']\n",
        "ratings_df = results['ratings_df']\n",
        "load_predictions_and_ratings_to_bigquery(client, predictions_df, ratings_df, ratings_model)\n",
        "\n",
        "# Create dataframe to compare rank and use for final rank aggregation\n",
        "rank_df = ratings_df[['team_no', 'fold_number', 'rank']]\n",
        "rank_df = rank_df.rename(columns={'rank': ratings_model})\n",
        "all_ranks_df = all_ranks_df.merge(rank_df, on=['team_no','fold_number'], how='outer')\n",
        "display(all_ranks_df)\n",
        "\n",
        "# Create confusion matrix from results\n",
        "results_df, cm_fig, roc_fig = create_model_results_df(results['all_actual_upsets'],results['all_predicted_upsets'], results['accuracies'], results['all_upset_probabilities'], ratings_model)\n",
        "results_df = results_df.reset_index()\n",
        "results_df['metric_id'] = results_df.index + 1\n",
        "results_df = results_df[['metric_id', 'metric', ratings_model]]\n",
        "\n",
        "all_results_df = all_results_df.merge(results_df, on=['metric_id', 'metric'], how='outer')\n",
        "display(all_results_df)"
      ],
      "metadata": {
        "id": "JCG_oWmsI2SZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kOer_ThessNN"
      },
      "outputs": [],
      "source": [
        "# Load all ranks df to Big Query\n",
        "load_dataset_name = 'analysis_layer'\n",
        "load_table_name = 'ratings_model_ranks'\n",
        "full_table_name = f\"{load_dataset_name}.{load_table_name}\"\n",
        "pandas_gbq.to_gbq(all_ranks_df, full_table_name, project_id='birkbeck-msc-project-422917', if_exists='replace')\n",
        "\n",
        "# Load confusion matrix df to Big Query\n",
        "load_dataset_name = 'analysis_layer'\n",
        "load_table_name = 'ratings_model_confusion_matrix_results'\n",
        "full_table_name = f\"{load_dataset_name}.{load_table_name}\"\n",
        "pandas_gbq.to_gbq(all_results_df, full_table_name, project_id='birkbeck-msc-project-422917', if_exists='replace')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Rank Aggregation functions called after ratings function have run and results inserted into Big Query\n",
        "\n",
        "def borda_count_aggregation (fold_counter):\n",
        "\n",
        "    from google.cloud import bigquery\n",
        "\n",
        "    # Authenticate using the loaded key\n",
        "    credentials = service_account.Credentials.from_service_account_info(key)\n",
        "    # Set up the BigQuery client with the credentials to project\n",
        "    client = bigquery.Client(credentials=credentials, project='birkbeck-msc-project-422917')\n",
        "\n",
        "    fold_number = fold_counter\n",
        "    all_ranks = \"\"\"\n",
        "              select * from analysis_layer.ratings_model_ranks\n",
        "              \"\"\"\n",
        "\n",
        "    all_ranks_df = client.query(all_ranks).to_dataframe()\n",
        "    all_ranks_df_fold = all_ranks_df[all_ranks_df['fold_number'] == fold_number].sort_values('team_no', ascending=True)\n",
        "    all_ranks_df_fold['rank_total'] = all_ranks_df_fold[['basic_position','massey','colley','keener','trueskill']].sum(axis=1)\n",
        "    all_ranks_df_fold['overall_rank'] = all_ranks_df_fold['rank_total'].rank(method='first', ascending=True).astype(int)\n",
        "    all_ranks_df_fold['rating'] = 1 / all_ranks_df_fold['overall_rank']\n",
        "\n",
        "\n",
        "    # Create a list of tuples in the format (team_no, rating) for function\n",
        "    ratings = list(zip(all_ranks_df_fold['team_no'], all_ranks_df_fold['rating']))\n",
        "\n",
        "    return ratings\n",
        "\n",
        "def average_rank_aggregation (fold_counter):\n",
        "\n",
        "    from google.cloud import bigquery\n",
        "\n",
        "    # Authenticate using the loaded key\n",
        "    credentials = service_account.Credentials.from_service_account_info(key)\n",
        "    # Set up the BigQuery client with the credentials to project\n",
        "    client = bigquery.Client(credentials=credentials, project='birkbeck-msc-project-422917')\n",
        "\n",
        "    fold_number = fold_counter\n",
        "    all_ranks = \"\"\"\n",
        "              select * from analysis_layer.ratings_model_ranks\n",
        "              \"\"\"\n",
        "\n",
        "    all_ranks_df = client.query(all_ranks).to_dataframe()\n",
        "    all_ranks_df_fold = all_ranks_df[all_ranks_df['fold_number'] == fold_number].sort_values('team_no', ascending=True)\n",
        "    all_ranks_df_fold['rank_average'] = all_ranks_df_fold[['basic_position','massey','colley','keener','trueskill']].mean(axis=1)\n",
        "    all_ranks_df_fold['overall_rank'] = all_ranks_df_fold['rank_average'].rank(method='first', ascending=True).astype(int)\n",
        "    all_ranks_df_fold['rating'] = 1 / all_ranks_df_fold['overall_rank']\n",
        "    display(all_ranks_df_fold)\n",
        "\n",
        "\n",
        "    # Create a list of tuples in the format (team_no, rating) for function\n",
        "    ratings = list(zip(all_ranks_df_fold['team_no'], all_ranks_df_fold['rating']))\n",
        "\n",
        "    return ratings\n"
      ],
      "metadata": {
        "id": "5wHYox1G34XH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Borda Count Aggregation\n",
        "ratings_model = 'borda_count'\n",
        "ratings_function = borda_count_aggregation\n",
        "num_folds = 5\n",
        "\n",
        "# Call ratings function and run cross validation\n",
        "results = run_rating_models_cross_validation (fa_cup_scores_df, ratings_function, ratings_model, num_folds)\n",
        "\n",
        "# Display results\n",
        "display(results['predictions_df'])\n",
        "display(results['upset_accuracy_scores_df'])\n",
        "display(results['ratings_df'])\n",
        "print(f\"Mean Upset Accuracy: {results['mean_upset_accuracy']:.4f}\")\n",
        "print(f\"Standard Deviation of Upset Accuracy: {results['std_upset_accuracy']:.4f}\")\n",
        "\n",
        "# Load predictions and ratinbgs to Google Big Query\n",
        "predictions_df = results['predictions_df']\n",
        "ratings_df = results['ratings_df']\n",
        "load_predictions_and_ratings_to_bigquery(client, predictions_df, ratings_df, ratings_model)\n",
        "\n",
        "# Create dataframe to compare rank and use for final rank aggregation\n",
        "rank_df = ratings_df[['team_no', 'fold_number', 'rank']]\n",
        "rank_df = rank_df.rename(columns={'rank': ratings_model})\n",
        "all_ranks_df = all_ranks_df.merge(rank_df, on=['team_no','fold_number'], how='outer')\n",
        "display(all_ranks_df)\n",
        "\n",
        "print(results['all_actual_upsets'])\n",
        "print(ratings_model)\n",
        "\n",
        "# Create confusion matrix from results\n",
        "results_df, cm_fig, roc_fig = create_model_results_df(results['all_actual_upsets'],results['all_predicted_upsets'], results['accuracies'], results['all_upset_probabilities'], ratings_model)\n",
        "results_df = results_df.reset_index()\n",
        "results_df['metric_id'] = results_df.index + 1\n",
        "results_df = results_df[['metric_id', 'metric', ratings_model]]\n",
        "\n",
        "all_results_df = all_results_df.merge(results_df, on=['metric_id', 'metric'], how='outer')\n",
        "display(all_results_df)"
      ],
      "metadata": {
        "id": "NhwPdT5QfVKq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Average Rank Aggregation\n",
        "ratings_model = 'average_rank'\n",
        "ratings_function = average_rank_aggregation\n",
        "num_folds = 5\n",
        "\n",
        "# Call ratings function and run cross validation\n",
        "results = run_rating_models_cross_validation (fa_cup_scores_df, ratings_function, ratings_model, num_folds)\n",
        "\n",
        "# Display results\n",
        "display(results['predictions_df'])\n",
        "display(results['upset_accuracy_scores_df'])\n",
        "display(results['ratings_df'])\n",
        "print(f\"Mean Upset Accuracy: {results['mean_upset_accuracy']:.4f}\")\n",
        "print(f\"Standard Deviation of Upset Accuracy: {results['std_upset_accuracy']:.4f}\")\n",
        "\n",
        "# Load predictions and ratinbgs to Google Big Query\n",
        "predictions_df = results['predictions_df']\n",
        "ratings_df = results['ratings_df']\n",
        "load_predictions_and_ratings_to_bigquery(client, predictions_df, ratings_df, ratings_model)\n",
        "\n",
        "# Create dataframe to compare rank and use for final rank aggregation\n",
        "rank_df = ratings_df[['team_no', 'fold_number', 'rank']]\n",
        "rank_df = rank_df.rename(columns={'rank': ratings_model})\n",
        "all_ranks_df = all_ranks_df.merge(rank_df, on=['team_no','fold_number'], how='outer')\n",
        "display(all_ranks_df)\n",
        "\n",
        "print(results['all_actual_upsets'])\n",
        "print(ratings_model)\n",
        "\n",
        "# Create confusion matrix from results\n",
        "results_df, cm_fig, roc_fig = create_model_results_df(results['all_actual_upsets'],results['all_predicted_upsets'], results['accuracies'], results['all_upset_probabilities'], ratings_model)\n",
        "results_df = results_df.reset_index()\n",
        "results_df['metric_id'] = results_df.index + 1\n",
        "results_df = results_df[['metric_id', 'metric', ratings_model]]\n",
        "\n",
        "all_results_df = all_results_df.merge(results_df, on=['metric_id', 'metric'], how='outer')\n",
        "display(all_results_df)"
      ],
      "metadata": {
        "id": "utXVU5-gVh-Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load updated all ranks df to Big Query so Borda Count can be subsequently queried in local kemeny optimisation\n",
        "load_dataset_name = 'analysis_layer'\n",
        "load_table_name = 'ratings_model_ranks'\n",
        "full_table_name = f\"{load_dataset_name}.{load_table_name}\"\n",
        "pandas_gbq.to_gbq(all_ranks_df, full_table_name, project_id='birkbeck-msc-project-422917', if_exists='replace')"
      ],
      "metadata": {
        "id": "LtmLSzRA5Cuh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Local Kemeny Optimisation\n",
        "\n",
        "def local_kemeny_optimisation (fold_number, max_iterations=10):\n",
        "\n",
        "    def kendall_tau_distance(ranking1, ranking2):\n",
        "        distance = 0\n",
        "        for i in range(len(ranking1)):\n",
        "            for j in range(i + 1, len(ranking1)):\n",
        "                if (ranking1[i] < ranking1[j]) != (ranking2[i] < ranking2[j]):\n",
        "                    distance += 1\n",
        "        return distance\n",
        "\n",
        "    def total_kendall_tau_distance(candidate, rankings):\n",
        "        return sum(kendall_tau_distance(candidate, ranking) for ranking in rankings.values())\n",
        "\n",
        "    def local_kemeny_optimisation(rankings, initial_ranking, max_iterations):\n",
        "        n_items = len(initial_ranking)\n",
        "\n",
        "        current_ranking = initial_ranking.copy()\n",
        "        current_distance = total_kendall_tau_distance(current_ranking, rankings)\n",
        "\n",
        "        for iteration in range(max_iterations):\n",
        "            improved = False\n",
        "            for i in range(n_items - 1):\n",
        "                new_ranking = current_ranking.copy()\n",
        "                new_ranking[i], new_ranking[i+1] = new_ranking[i+1], new_ranking[i]\n",
        "                new_distance = total_kendall_tau_distance(new_ranking, rankings)\n",
        "\n",
        "                if new_distance < current_distance:\n",
        "                    current_ranking = new_ranking\n",
        "                    current_distance = new_distance\n",
        "                    improved = True\n",
        "                    print(f\"Iteration {iteration + 1}: Improved distance to {current_distance}\")\n",
        "                    break\n",
        "\n",
        "            if not improved:\n",
        "                print(f\"Stopped after {iteration + 1} iterations: No improvement\")\n",
        "                break\n",
        "\n",
        "        if iteration == max_iterations - 1:\n",
        "            print(f\"Stopped after reaching maximum iterations ({max_iterations})\")\n",
        "\n",
        "        return current_ranking\n",
        "\n",
        "    # Authenticate using the loaded key\n",
        "    #credentials = service_account.Credentials.from_service_account_info(key)\n",
        "    # Set up the BigQuery client with the credentials to project\n",
        "    client = bigquery.Client(credentials=credentials, project='birkbeck-msc-project-422917')\n",
        "\n",
        "    # Query to get the data\n",
        "    all_ranks_query = f\"\"\"\n",
        "        SELECT * FROM analysis_layer.ratings_model_ranks WHERE fold_number = {fold_number}\n",
        "    \"\"\"\n",
        "\n",
        "    all_ranks_df = client.query(all_ranks_query).to_dataframe()\n",
        "\n",
        "    # Create dictionary of rankings\n",
        "    rankings = {\n",
        "        'basic_position': all_ranks_df['basic_position'].tolist(),\n",
        "        'massey': all_ranks_df['massey'].tolist(),\n",
        "        'colley': all_ranks_df['colley'].tolist(),\n",
        "        'keener': all_ranks_df['keener'].tolist(),\n",
        "        'trueskill': all_ranks_df['trueskill'].tolist()\n",
        "    }\n",
        "\n",
        "    # Use Borda count as initial ranking\n",
        "    borda_count = all_ranks_df['borda_count'].tolist()\n",
        "\n",
        "    # Perform local Kemeny optimization\n",
        "    optimized_ranking = local_kemeny_optimisation(rankings, borda_count, max_iterations)\n",
        "    print(\"\\nTotal Kendall tau distance (Borda Count):\", total_kendall_tau_distance(borda_count, rankings))\n",
        "    print(\"Total Kendall tau distance (Optimized):\", total_kendall_tau_distance(optimized_ranking, rankings))\n",
        "\n",
        "    # Show which teams changed positions\n",
        "    team_nos = all_ranks_df['team_no'].tolist()\n",
        "\n",
        "    # Create a mapping from team number to rank for both rankings\n",
        "    borda_mapping = {team: rank for rank, team in enumerate(borda_count)}\n",
        "    optimized_mapping = {team: rank for rank, team in enumerate(optimized_ranking)}\n",
        "\n",
        "    changes = []\n",
        "    for team in team_nos:\n",
        "        borda_rank = borda_mapping.get(team, -1)\n",
        "        optimized_rank = optimized_mapping.get(team, -1)\n",
        "        if borda_rank != optimized_rank:\n",
        "            changes.append((team, borda_rank, optimized_rank))\n",
        "\n",
        "    print(\"\\nTeams that changed positions (Team, Old Position, New Position):\")\n",
        "    for change in changes:\n",
        "        print(f\"Team {change[0]}: {change[1]} -> {change[2]}\")\n",
        "\n",
        "    # Create a DataFrame with the optimized ranking\n",
        "    optimized_df = pd.DataFrame({\n",
        "        'team_no': team_nos,\n",
        "        'overall_rank': optimized_ranking\n",
        "    })\n",
        "\n",
        "    # Calculate the rating based on the optimized ranking\n",
        "    optimized_df['rating'] = 1 / optimized_df['overall_rank']\n",
        "\n",
        "    # Create a list of tuples in the format (team_no, rating) for function\n",
        "    ratings = list(zip(optimized_df['team_no'], optimized_df['rating']))\n",
        "\n",
        "    return ratings\n"
      ],
      "metadata": {
        "id": "ibBrmuaX163V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Local Kemeny Optimisation\n",
        "\n",
        "ratings_model = 'local_kemeny_optimisation'\n",
        "ratings_function = local_kemeny_optimisation\n",
        "num_folds = 5\n",
        "\n",
        "# Call ratings function and run cross validation\n",
        "results = run_rating_models_cross_validation (fa_cup_scores_df, ratings_function, ratings_model, num_folds)\n",
        "\n",
        "# Display results\n",
        "display(results['predictions_df'])\n",
        "display(results['upset_accuracy_scores_df'])\n",
        "display(results['ratings_df'])\n",
        "print(f\"Mean Upset Accuracy: {results['mean_upset_accuracy']:.4f}\")\n",
        "print(f\"Standard Deviation of Upset Accuracy: {results['std_upset_accuracy']:.4f}\")\n",
        "\n",
        "# Load predictions and ratinbgs to Google Big Query\n",
        "predictions_df = results['predictions_df']\n",
        "ratings_df = results['ratings_df']\n",
        "load_predictions_and_ratings_to_bigquery(client, predictions_df, ratings_df, ratings_model)\n",
        "\n",
        "# Create dataframe to compare rank and use for final rank aggregation\n",
        "rank_df = ratings_df[['team_no', 'fold_number', 'rank']]\n",
        "rank_df = rank_df.rename(columns={'rank': ratings_model})\n",
        "all_ranks_df = all_ranks_df.merge(rank_df, on=['team_no','fold_number'], how='outer')\n",
        "display(all_ranks_df)\n",
        "\n",
        "# Create confusion matrix from results\n",
        "results_df, cm_fig, roc_fig = create_model_results_df(results['all_actual_upsets'],results['all_predicted_upsets'], results['accuracies'], results['all_upset_probabilities'], ratings_model)\n",
        "results_df = results_df.reset_index()\n",
        "results_df['metric_id'] = results_df.index + 1\n",
        "results_df = results_df[['metric_id', 'metric', ratings_model]]\n",
        "\n",
        "all_results_df = all_results_df.merge(results_df, on=['metric_id', 'metric'], how='outer')\n",
        "display(all_results_df)"
      ],
      "metadata": {
        "id": "-l44e7fi5y4n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load updated all ranks df to Big Query so Borda Count can be subsequently queried in local kemeny optimisation\n",
        "load_dataset_name = 'analysis_layer'\n",
        "load_table_name = 'ratings_model_ranks'\n",
        "full_table_name = f\"{load_dataset_name}.{load_table_name}\"\n",
        "pandas_gbq.to_gbq(all_ranks_df, full_table_name, project_id='birkbeck-msc-project-422917', if_exists='replace')"
      ],
      "metadata": {
        "id": "wdaot3i69jpv"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPgXlfg9VUpocz4eybdOB8V",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}