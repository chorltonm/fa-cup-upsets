{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/chorltonm/fa-cup-upsets/blob/main/notebooks/source_apis/football_data.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5JIdr0pAQuut"
      },
      "outputs": [],
      "source": [
        "# Import Libaries\n",
        "\n",
        "import requests\n",
        "import math\n",
        "import json\n",
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "from google.cloud import bigquery\n",
        "from google.oauth2 import service_account\n",
        "from google.colab import drive\n",
        "from google.colab import userdata"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Change default ouput directory\n",
        "os.chdir('/content/drive/MyDrive/birkbeck_msc-project/output_files')"
      ],
      "metadata": {
        "id": "7eECUc2E_DWB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FBQ6M4kkQ4-B"
      },
      "outputs": [],
      "source": [
        "# SportDevs Bearer Token API Authentication\n",
        "userdata.get('sportdevs_bearer')\n",
        "bearer_token = userdata.get('sportdevs_bearer')\n",
        "\n",
        "headers = {\n",
        "    \"Accept\": \"application/json\",\n",
        "    \"Authorization\": f\"Bearer {bearer_token}\"\n",
        "}\n",
        "\n",
        "# Google Service Account\n",
        "\n",
        "# Load the JSON key from local Google Collab file\n",
        "key = json.load(open('/content/drive/MyDrive/service_account.json', 'r'))\n",
        "\n",
        "# Authenticate using the loaded key\n",
        "credentials = service_account.Credentials.from_service_account_info(key)\n",
        "\n",
        "# Set up the BigQuery client with the credentials to project\n",
        "client = bigquery.Client(credentials=credentials, project='birkbeck-msc-project-422917')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XI1qwI9p4e4p"
      },
      "outputs": [],
      "source": [
        "# Set filter variables\n",
        "country = 'England'\n",
        "league_name = 'League One'\n",
        "\n",
        "# LEAGUE  DATA\n",
        "# Get League ID\n",
        "leagues_endpoint = f\"https://football.sportdevs.com/leagues?class_name=eq.{country}&name=eq.{league_name}\"\n",
        "leagues_response = requests.get(leagues_endpoint, headers=headers)\n",
        "leagues_data = leagues_response.json()\n",
        "\n",
        "print(leagues_data)\n",
        "\n",
        "# Get Seasons Data for League One and save seasons data to extract file\n",
        "from_date = '2008-07-01'\n",
        "\n",
        "seasons_list = []\n",
        "\n",
        " # SEASONS DATA\n",
        "with open(f\"{league_name}_seasons_data.json\", 'w') as seasons_file:\n",
        "  for league in leagues_data:\n",
        "      league_id = league['id']\n",
        "\n",
        "      # Get seasons for the league\n",
        "      seasons_url = f\"https://football.sportdevs.com/seasons?league_id=eq.{league_id}&start_time=gte.{from_date}\"\n",
        "      seasons_response = requests.get(seasons_url, headers=headers)\n",
        "      seasons_data = seasons_response.json()\n",
        "\n",
        "      matches_list = []\n",
        "      lineups_list = []\n",
        "      season_standings_list = []\n",
        "\n",
        "      for season in seasons_data:\n",
        "          seasons_file.write(f\"{json.dumps(season)}\\n\")\n",
        "          seasons_list.append(season)\n",
        "          season_id = season['id']\n",
        "\n",
        "          # LEAGUE STANDINDS DATA\n",
        "          # Only run the season_standings section if the league is not FA Cup\n",
        "          if league_name != 'FA Cup':\n",
        "            season_standings_url = f\"https://football.sportdevs.com/standings?type=eq.total&season_id=eq.{season_id}\"\n",
        "            season_standings_response = requests.get(season_standings_url, headers=headers)\n",
        "            season_standings_data = season_standings_response.json()\n",
        "\n",
        "            for season in season_standings_data:\n",
        "              season_standings_list.append(season)\n",
        "              print(*season_standings_list, sep=\"\\n\")\n",
        "\n",
        "          # MATCH DATA\n",
        "          # Get all matches for season ID and maximum 50 results per request as per api documentation\n",
        "            limit = 50\n",
        "            offset = 0\n",
        "            number_of_matches = 0\n",
        "\n",
        "          while True:\n",
        "            matches_url = f\"https://football.sportdevs.com/matches?season_id=eq.{season_id}&limit={limit}&offset={offset}\"\n",
        "            matches_response = requests.get(matches_url, headers=headers)\n",
        "            matches_data = matches_response.json()\n",
        "\n",
        "            for matches in matches_data:\n",
        "                matches_list.append(matches)\n",
        "\n",
        "                if 'lineups_id' in matches:\n",
        "                    lineups_list.append(matches['lineups_id'])\n",
        "\n",
        "            offset += limit\n",
        "            number_of_matches += len(matches_data)\n",
        "            print(number_of_matches)\n",
        "\n",
        "            if len(matches_data) < limit:\n",
        "                break\n",
        "\n",
        "# Write all season teams from list to file at once\n",
        "with open(f\"{league_name}_season_standings_data.json\", 'w') as season_standings_file:\n",
        "  for season in season_standings_list:\n",
        "      season_standings_file.write(f\"{json.dumps(season)}\\n\")\n",
        "\n",
        "# Write all matches from list to file at once\n",
        "with open(f\"{league_name}_matches_data.json\", 'w') as matches_file:\n",
        "  for match in matches_list:\n",
        "      matches_file.write(f\"{json.dumps(match)}\\n\")\n",
        "\n",
        "lineup_ids = sorted(lineups_list)\n",
        "total_seasons = len(seasons_list)\n",
        "total_matches = len(matches_list)\n",
        "total_lineup_ids = len(lineup_ids)\n",
        "print(f\"Total seasons: {total_seasons}\")\n",
        "print(f\"Total mactches: {total_matches}\")\n",
        "print(f\"Total line up ids: {total_lineup_ids}\")\n",
        "\n",
        "#print(*matches_list, sep=\"\\n\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-wI_kTnmGdb6"
      },
      "outputs": [],
      "source": [
        "# Load seasons file to BIQ QUERY extract_layer seasons table\n",
        "\n",
        "# Set the dataset and table name\n",
        "dataset_name = 'extract_layer'\n",
        "table_name = 'api_sportdevs_fb_seasons'\n",
        "table_ref = client.dataset(dataset_name).table(table_name)\n",
        "\n",
        "# Load the final data file into the BiqQuery table\n",
        "job_config = bigquery.LoadJobConfig(\n",
        "  source_format=bigquery.SourceFormat.NEWLINE_DELIMITED_JSON,\n",
        "  write_disposition=bigquery.WriteDisposition.WRITE_TRUNCATE,\n",
        "  autodetect=True\n",
        ")\n",
        "\n",
        "with open(f\"{league_name}_seasons_data.json\", 'rb') as lineups_file:\n",
        "              job = client.load_table_from_file(\n",
        "                    lineups_file, table_ref, job_config=job_config\n",
        "                )\n",
        "# Wait for the job to complete\n",
        "job.result()\n",
        "total_rows = job.output_rows\n",
        "print(f\"Big Query Rows {total_rows} processed into {dataset_name}.{table_name}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fAqak_ypHjnz"
      },
      "outputs": [],
      "source": [
        "# Load league\n",
        "seasons data into BIQ QUERY load_layer seasons table\n",
        "\n",
        "# Set the target load dataset and table names\n",
        "load_dataset_name = 'extract_layer'\n",
        "load_table_name = 'all_api_sportdevs_fb_seasons'\n",
        "table_ref = client.dataset(load_dataset_name).table(load_table_name)\n",
        "\n",
        "# Delete rows already inserted\n",
        "delete_query = f\"DELETE FROM `{load_dataset_name}.{load_table_name}` WHERE id IN (SELECT DISTINCT id FROM `{dataset_name}.{table_name}`)\"\n",
        "delete_job = client.query(delete_query)\n",
        "delete_result = delete_job.result()\n",
        "total_rows_deleted = delete_result.num_dml_affected_rows\n",
        "\n",
        "if total_rows_deleted == 0:\n",
        "    print(f\"No rows were deleted from {load_dataset_name}.{load_table_name}.\")\n",
        "else:\n",
        "   print(f\"Total rows deleted from {load_dataset_name}.{load_table_name}: {total_rows_deleted}\")\n",
        "\n",
        "# Define the load job configuration\n",
        "job_config = bigquery.QueryJobConfig(\n",
        "    destination=table_ref,\n",
        "    write_disposition=bigquery.WriteDisposition.WRITE_APPEND,\n",
        "    schema_update_options=[\n",
        "        bigquery.SchemaUpdateOption.ALLOW_FIELD_ADDITION\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Define the SQL query\n",
        "insert_query = f\"SELECT * FROM `{dataset_name}.{table_name}`\"\n",
        "\n",
        "# Run the load job and wait for the job to complete\n",
        "load_job = client.query(insert_query, job_config=job_config)\n",
        "job.result()\n",
        "total_rows = job.output_rows\n",
        "print(f\"Big Query Rows {total_rows} processed into {load_dataset_name}.{load_table_name}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sDp8rl91usqV"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load standings teams file to BIQ QUERY extract_layer season teams table\n",
        "\n",
        "if league_name != 'FA Cup':\n",
        "  # Set the dataset and table name\n",
        "  dataset_name = 'extract_layer'\n",
        "  table_name = 'api_sportdevs_fb_standings'\n",
        "  table_ref = client.dataset(dataset_name).table(table_name)\n",
        "\n",
        "  # Load the final data file into the BiqQuery table\n",
        "  job_config = bigquery.LoadJobConfig(\n",
        "    source_format=bigquery.SourceFormat.NEWLINE_DELIMITED_JSON,\n",
        "    write_disposition=bigquery.WriteDisposition.WRITE_TRUNCATE,\n",
        "    autodetect=True\n",
        "  )\n",
        "\n",
        "  with open(f\"{league_name}_season_standings_data.json\", 'rb') as lineups_file:\n",
        "                job = client.load_table_from_file(\n",
        "                      lineups_file, table_ref, job_config=job_config\n",
        "                  )\n",
        "  # Wait for the job to complete\n",
        "  job.result()\n",
        "  total_rows = job.output_rows\n",
        "  print(f\"Big Query Rows {total_rows} processed into {dataset_name}.{table_name}\")"
      ],
      "metadata": {
        "id": "2HuyNcFougjG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load standings data into BIQ QUERY load_layer season teams table\n",
        "\n",
        "if league_name != 'FA Cup':\n",
        "  # Set the target load dataset and table names\n",
        "  load_dataset_name = 'load_layer'\n",
        "  load_table_name = 'all_api_sportdevs_fb_standings'\n",
        "  table_ref = client.dataset(load_dataset_name).table(load_table_name)\n",
        "\n",
        "  # Delete rows already inserted\n",
        "  delete_query = f\"DELETE FROM `{load_dataset_name}.{load_table_name}` WHERE id IN (SELECT DISTINCT id FROM `{dataset_name}.{table_name}`)\"\n",
        "  delete_job = client.query(delete_query)\n",
        "  delete_result = delete_job.result()\n",
        "  total_rows_deleted = delete_result.num_dml_affected_rows\n",
        "\n",
        "  if total_rows_deleted == 0:\n",
        "      print(f\"No rows were deleted from {load_dataset_name}.{load_table_name}.\")\n",
        "  else:\n",
        "    print(f\"Total rows deleted from {load_dataset_name}.{load_table_name}: {total_rows_deleted}\")\n",
        "\n",
        "  # Define the load job configuration\n",
        "  job_config = bigquery.QueryJobConfig(\n",
        "      destination=table_ref,\n",
        "      write_disposition=bigquery.WriteDisposition.WRITE_APPEND,\n",
        "      schema_update_options=[\n",
        "          bigquery.SchemaUpdateOption.ALLOW_FIELD_ADDITION\n",
        "      ]\n",
        "  )\n",
        "\n",
        "  # Define the SQL query\n",
        "  insert_query = f\"SELECT * FROM `{dataset_name}.{table_name}`\"\n",
        "\n",
        "  # Run the load job and wait for the job to complete\n",
        "  load_job = client.query(insert_query, job_config=job_config)\n",
        "  job.result()\n",
        "  total_rows = job.output_rows\n",
        "  print(f\"Big Query Rows {total_rows} processed into {load_dataset_name}.{load_table_name}\")"
      ],
      "metadata": {
        "id": "MIGnVdFnutZk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B7f_-X1nfuwh"
      },
      "outputs": [],
      "source": [
        "# Load matches file to BIQ QUERY extract_layer matches table\n",
        "\n",
        "# Set the dataset and table names\n",
        "dataset_name = 'extract_layer'\n",
        "table_name = 'api_sportdevs_fb_matches'\n",
        "table_ref = client.dataset(dataset_name).table(table_name)\n",
        "\n",
        "#table = client.get_table(table_ref)\n",
        "\n",
        "# Truncate the table to remove all existing rows\n",
        "client.query(f\"TRUNCATE TABLE `{dataset_name}.{table_name}`\").result()\n",
        "\n",
        "\n",
        "# Load the final data file into the BiqQuery table\n",
        "job_config = bigquery.LoadJobConfig(\n",
        "  source_format=bigquery.SourceFormat.NEWLINE_DELIMITED_JSON,\n",
        "  write_disposition=bigquery.WriteDisposition.WRITE_TRUNCATE,\n",
        "  autodetect=True\n",
        ")\n",
        "\n",
        "with open(f\"{league_name}_matches_data.json\", 'rb') as lineups_file:\n",
        "              job = client.load_table_from_file(\n",
        "                    lineups_file, table_ref, job_config=job_config\n",
        "                )\n",
        "# Wait for the job to complete\n",
        "job.result()\n",
        "total_rows = job.output_rows\n",
        "print(f\"Big Query Rows {total_rows} processed into {dataset_name}.{table_name}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CKVSVu3DyYGO"
      },
      "outputs": [],
      "source": [
        "# Load matches extract data into BIQ QUERY load_layer matches table\n",
        "\n",
        "# Set the table reference\n",
        "load_dataset_name = 'load_layer'\n",
        "load_table_name = 'all_api_sportdevs_fb_matches'\n",
        "table_ref = client.dataset(load_dataset_name).table(load_table_name)\n",
        "\n",
        "# Delete rows already inserted\n",
        "delete_query = f\"DELETE FROM `{load_dataset_name}.{load_table_name}` WHERE id IN (SELECT DISTINCT id FROM `{dataset_name}.{table_name}`)\"\n",
        "delete_job = client.query(delete_query)\n",
        "delete_result = delete_job.result()\n",
        "total_rows_deleted = delete_result.num_dml_affected_rows\n",
        "\n",
        "if total_rows_deleted == 0:\n",
        "    print(f\"No rows were deleted from {load_dataset_name}.{load_table_name}.\")\n",
        "else:\n",
        "   print(f\"Total rows deleted from {load_dataset_name}.{load_table_name}: {total_rows_deleted}\")\n",
        "\n",
        "# Define the load job configuration\n",
        "job_config = bigquery.QueryJobConfig(\n",
        "    destination=table_ref,\n",
        "    write_disposition=bigquery.WriteDisposition.WRITE_APPEND,\n",
        "    schema_update_options=[\n",
        "        bigquery.SchemaUpdateOption.ALLOW_FIELD_ADDITION\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Define the SQL query\n",
        "insert_query = f\"SELECT * FROM `{dataset_name}.{table_name}`\"\n",
        "\n",
        "# Run the load job and wait for the job to complete\n",
        "load_job = client.query(insert_query, job_config=job_config)\n",
        "\n",
        "# Wait for the job to complete\n",
        "job.result()\n",
        "total_rows = job.output_rows\n",
        "print(f\"Big Query Rows {total_rows} processed into {load_dataset_name}.{load_table_name}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DPqpSdRaMDbf"
      },
      "outputs": [],
      "source": [
        "# Create and write to line ups file extract data into BIQ QUERY extract_layer lineups table\n",
        "\n",
        "# Set the target dataset and table names\n",
        "dataset_name = 'extract_layer'\n",
        "table_name = 'api_sportdevs_fb_lineups'\n",
        "table_ref = client.dataset(dataset_name).table(table_name)\n",
        "\n",
        "def send_requests_in_batches(ids, api_endpoint, batch_size=50):\n",
        "    total_ids = len(ids)\n",
        "    total_batches = math.ceil(total_ids / batch_size)\n",
        "    print(f\"Total ids: {total_ids}\")\n",
        "    print(f\"Total batches: {total_batches}\")\n",
        "\n",
        "    batches = [ids[i:i+batch_size] for i in range(0, len(ids), batch_size)]\n",
        "    batch_count = 1\n",
        "\n",
        "    with open(f\"{league_name}_lineups_data.json\", 'w') as lineups_file:\n",
        "        for batch in batches:\n",
        "            batch_ids = ','.join(map(str, batch))\n",
        "            url = f\"{api_endpoint}?id=in.({batch_ids})\"\n",
        "            response = requests.get(url, headers=headers)\n",
        "            lineups_data = response.json()\n",
        "\n",
        "            # Prepare the data to be inserted\n",
        "            for lineup in lineups_data:\n",
        "                lineups_file.write(json.dumps(lineup) + '\\n')\n",
        "\n",
        "            print(f\"API Batch {batch_count}/{total_batches} processed.\")\n",
        "\n",
        "            batch_count += 1\n",
        "\n",
        "# Call batch requests function\n",
        "ids = lineup_ids\n",
        "api_endpoint = \"https://football.sportdevs.com/matches-lineups\"\n",
        "send_requests_in_batches(ids, api_endpoint)\n",
        "\n",
        "# Load the final extract data file into the BIGQUERY table\n",
        "job_config = bigquery.LoadJobConfig(\n",
        "  source_format=bigquery.SourceFormat.NEWLINE_DELIMITED_JSON,\n",
        "  write_disposition=bigquery.WriteDisposition.WRITE_TRUNCATE,\n",
        "  autodetect=True\n",
        ")\n",
        "\n",
        "with open(f\"{league_name}_lineups_data.json\", 'rb') as lineups_file:\n",
        "              job = client.load_table_from_file(\n",
        "                    lineups_file, table_ref, job_config=job_config\n",
        "                )\n",
        "# Wait for the job to complete and print message\n",
        "job.result()\n",
        "total_rows = job.output_rows\n",
        "print(f\"Big Query Rows {total_rows} processed into {dataset_name}.{table_name}.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A0gFHEOqqG5R"
      },
      "outputs": [],
      "source": [
        "# LOAD line ups extract data into BIQ QUERY load_layer table\n",
        "\n",
        "# Set the target load dataset and table names\n",
        "load_dataset_name = 'load_layer'\n",
        "load_table_name = 'all_api_sportdevs_fb_lineups'\n",
        "table_ref = client.dataset(load_dataset_name).table(load_table_name)\n",
        "\n",
        "\n",
        "# Delete rows already inserted\n",
        "delete_query = f\"DELETE FROM `{load_dataset_name}.{load_table_name}` WHERE id IN (SELECT DISTINCT id FROM `{dataset_name}.{table_name}`)\"\n",
        "delete_job = client.query(delete_query)\n",
        "delete_result = delete_job.result()\n",
        "total_rows_deleted = delete_result.num_dml_affected_rows\n",
        "\n",
        "if total_rows_deleted == 0:\n",
        "    print(f\"No rows were deleted from {load_dataset_name}.{load_table_name}.\")\n",
        "else:\n",
        "   print(f\"Total rows deleted from {load_dataset_name}.{load_table_name}: {total_rows_deleted}\")\n",
        "\n",
        "# Define the load job configuration\n",
        "job_config = bigquery.QueryJobConfig(\n",
        "    destination=table_ref,\n",
        "    write_disposition=bigquery.WriteDisposition.WRITE_APPEND,\n",
        "    schema_update_options=[\n",
        "        bigquery.SchemaUpdateOption.ALLOW_FIELD_ADDITION\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Define the SQL query\n",
        "insert_query = f\"SELECT * FROM `{dataset_name}.{table_name}`\"\n",
        "\n",
        "# Run the load job and wait for the job to complete\n",
        "load_job = client.query(insert_query, job_config=job_config)\n",
        "\n",
        "# Wait for the job to complete\n",
        "job.result()\n",
        "total_rows = job.output_rows\n",
        "\n",
        "print(f\"Big Query Rows {total_rows} processed into {load_dataset_name}.{load_table_name}\")\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}