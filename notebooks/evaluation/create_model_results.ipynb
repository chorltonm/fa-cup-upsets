{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNJrMXMJ3qsvRMErXD4E54i"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import accuracy_score, recall_score, confusion_matrix, classification_report, roc_auc_score, roc_curve, RocCurveDisplay\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "rDQMfv6vB04x"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_model_results_df (all_y_train_true, all_y_train_pred, all_y_test_true, all_y_test_pred, train_accuracies, train_recalls, test_accuracies, test_recalls, all_y_train_pred_proba, all_y_test_pred_proba, model_name, test_flag):\n",
        "\n",
        "    from sklearn.metrics import log_loss\n",
        "    print(f\"Model name received: '{model_name}'\")\n",
        "\n",
        "    report_dict = classification_report(all_y_test_true, all_y_test_pred, output_dict=True)\n",
        "    cm = confusion_matrix(all_y_test_true, all_y_test_pred)\n",
        "\n",
        "    metrics = []\n",
        "    values = []\n",
        "\n",
        "    # Add cross-validation Train Accuracy Mean\n",
        "    avg_train_accuracy = np.mean(train_accuracies)\n",
        "    metrics.append(f'{test_flag} Train Accuracy Mean')\n",
        "    values.append(avg_train_accuracy)\n",
        "\n",
        "    # Add Cross Validation Train Accuracy Standard Deviation\n",
        "    std_train_accuracy = np.std(train_accuracies)\n",
        "    metrics.append(f'{test_flag} Standard Deviation')\n",
        "    values.append(std_train_accuracy)\n",
        "\n",
        "    # Add Test Accuracy Mean\n",
        "    avg_test_accuracy = np.mean(test_accuracies)\n",
        "    metrics.append(f'{test_flag} Accuracy Mean')\n",
        "    values.append(avg_test_accuracy)\n",
        "\n",
        "    # Add Test Accuracy Standard Deviation\n",
        "    std_test_accuracy = np.std(test_accuracies)\n",
        "    metrics.append(f'{test_flag} Accuracy Standard Deviation')\n",
        "    values.append(std_test_accuracy)\n",
        "\n",
        "    # Add Train Recall Mean\n",
        "    avg_train_recall = np.mean(train_recalls)\n",
        "    metrics.append(f'{test_flag} Train Recall Mean')\n",
        "    values.append(avg_train_recall)\n",
        "\n",
        "    # Add Recall Standard Deviation\n",
        "    std_train_recall = np.std(train_recalls)\n",
        "    metrics.append(f'{test_flag} rain Recall Standard Deviation')\n",
        "    values.append(std_train_recall)\n",
        "\n",
        "    # Add Test Recall Mean\n",
        "    avg_test_recall = np.mean(test_recalls)\n",
        "    metrics.append(f'{test_flag}  Test Recall Mean')\n",
        "    values.append(avg_test_recall)\n",
        "\n",
        "    # Add Recall Standard Deviation\n",
        "    std_test_recall = np.std(test_recalls)\n",
        "    metrics.append(f'{test_flag}  Test Recall Standard Deviation')\n",
        "    values.append(std_test_recall)\n",
        "\n",
        "    # Calculate overall log loss\n",
        "    train_log_loss = log_loss(all_y_train_true, all_y_train_pred_proba)\n",
        "    metrics.append(f'{test_flag} Train Log Loss')\n",
        "    values.append(train_log_loss)\n",
        "    print(train_log_loss)\n",
        "\n",
        "    test_log_loss = log_loss(all_y_test_true, all_y_test_pred_proba)\n",
        "    metrics.append(f'{test_flag} Test Log Loss')\n",
        "    values.append(test_log_loss)\n",
        "    print(test_log_loss)\n",
        "\n",
        "    # Add overall accuracy from the classification report\n",
        "    metrics.append('Overall Accuracy')\n",
        "    values.append(report_dict['accuracy'])\n",
        "\n",
        "    # Add confusion matrix values\n",
        "    cm_labels = ['True Negative (Class 0)', 'False Positive (Class 1)', 'False Negative (Class 0)', 'True Positive (Class 1)']\n",
        "    for label, value in zip(cm_labels, cm.ravel()):\n",
        "        metrics.append(f'Confusion Matrix - {label}')\n",
        "        values.append(value)\n",
        "\n",
        "    # Add precision, recall, and f1-score for each class\n",
        "    for class_label in sorted(report_dict.keys()):\n",
        "        if class_label in  {'0.0', '1.0', '0', '1'}:  # This checks if the key is a class label:  # This checks if the key is a class label\n",
        "        #if class_label.isdigit():  # This checks if the key is a class label\n",
        "            for metric in ['precision', 'recall', 'f1-score']:\n",
        "                metrics.append(f'{metric.capitalize()} (Class {class_label})')\n",
        "                values.append(report_dict[class_label][metric])\n",
        "\n",
        "    # Add macro and weighted averages\n",
        "    for avg_type in ['macro avg', 'weighted avg']:\n",
        "        for metric in ['precision', 'recall', 'f1-score']:\n",
        "            metrics.append(f'{avg_type.capitalize()} {metric.capitalize()}')\n",
        "            values.append(report_dict[avg_type][metric])\n",
        "\n",
        "    # Calculate and add AUC-ROC score\n",
        "    if all_y_test_pred_proba is not None:\n",
        "        auc_roc = roc_auc_score(all_y_test_true, all_y_test_pred_proba)\n",
        "        metrics.append('AUC-ROC')\n",
        "        values.append(auc_roc)\n",
        "\n",
        "    # Create the DataFrame\n",
        "    results_df = pd.DataFrame({\n",
        "        'metric': metrics,\n",
        "        model_name: values\n",
        "    })\n",
        "\n",
        "    # Format the numeric values to 3 decimal places\n",
        "    results_df[model_name] = results_df[model_name].apply(lambda x: f'{x:.3f}' if isinstance(x, (int, float)) else str(x))\n",
        "\n",
        "    # Present the confusion matrix\n",
        "    cm_fig, ax = plt.subplots(figsize=(10,7))\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False, ax=ax)\n",
        "    ax.set_title(f'Confusion Matrix {model_name}')\n",
        "    ax.set_ylabel('Actual Class\\n(0 not upset and 1 upset)')\n",
        "    ax.set_xlabel('Predicted Class\\n(0 not upset and 1 upset)')\n",
        "\n",
        "    roc_fig = None\n",
        "\n",
        "    if all_y_test_pred_proba is not None:\n",
        "      # Plot ROC curve\n",
        "      roc_fig, roc_ax = plt.subplots(figsize=(8, 6))\n",
        "      RocCurveDisplay.from_predictions(\n",
        "            all_y_test_true,\n",
        "            all_y_test_pred_proba,\n",
        "            ax=roc_ax,\n",
        "            name=model_name\n",
        "      )\n",
        "      roc_ax.set_title(f'ROC Curve - {model_name}')\n",
        "      roc_ax.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
        "\n",
        "    return results_df, cm_fig, roc_fig\n"
      ],
      "metadata": {
        "id": "vjDY954pCHx_"
      },
      "execution_count": 5,
      "outputs": []
    }
  ]
}