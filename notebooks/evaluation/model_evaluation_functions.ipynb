{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMwlguyk3rxXUTWOXfw6WTE"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "rDQMfv6vB04x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run_rating_models_cross_validation (fa_cup_scores_df, ratings_function, ratings_model, num_folds):\n",
        "\n",
        "    # Create a StratifiedKFold object and fold counter\n",
        "    skf = StratifiedKFold(n_splits=num_folds, shuffle=True, random_state=42)\n",
        "    fold_counter = 0\n",
        "\n",
        "    # Initialize empty lists to store results\n",
        "    predictions = []\n",
        "    fold_ratings = []\n",
        "    accuracies = []\n",
        "    all_actual_upsets = []\n",
        "    all_predicted_upsets = []\n",
        "\n",
        "    # Iterate over the folds\n",
        "    for fold, (train_idx, test_idx) in enumerate(skf.split(fa_cup_scores_df, fa_cup_scores_df['actual_upset']), start=1):\n",
        "        print(f\"Fold {fold}/{num_folds}\")\n",
        "        fold_counter += 1  # Increment fold_counter\n",
        "\n",
        "        # Split the data into training and test sets\n",
        "        train_data = fa_cup_scores_df.iloc[train_idx]\n",
        "        test_data = fa_cup_scores_df.iloc[test_idx]\n",
        "        print(f\"Train data size: {len(train_data)}\")\n",
        "        print(f\"Test data size: {len(test_data)}\")\n",
        "\n",
        "        # Get distinct teams\n",
        "        all_teams = pd.concat([train_data['home_team_no'], train_data['away_team_no']]).drop_duplicates().sort_values().reset_index(drop=True)\n",
        "        print(ratings_model)\n",
        "        print(ratings_function)\n",
        "        # Call rating function\n",
        "        if ratings_model == 'borda_count':\n",
        "            ratings = ratings_function(fold_counter)\n",
        "        elif ratings_model == 'average_rank':\n",
        "            ratings = ratings_function(fold_counter)\n",
        "        elif ratings_model == 'local_kemeny_optimisation':\n",
        "            ratings = ratings_function(fold_counter)\n",
        "        else:\n",
        "            ratings = ratings_function(train_data)\n",
        "        print(ratings)\n",
        "        # Append the ratings to the list\n",
        "        ratings_list = [(ratings_model, fold, team_no, rating) for team_no, rating in ratings]\n",
        "        fold_ratings.extend(ratings_list)\n",
        "\n",
        "        # Predict upsets and calculate accuracy\n",
        "        actual_upsets = []\n",
        "        predicted_upsets = []\n",
        "\n",
        "        for _, row in test_data.iterrows():\n",
        "            predicted_winner, predicted_upset, home_team_rating, away_team_rating = predict_winner_upset(\n",
        "                row['home_team_no'], row['away_team_no'],\n",
        "                row['home_team_league_level'], row['away_team_league_level'],\n",
        "                ratings, home_advantage=0\n",
        "            )\n",
        "            actual_upset = row['actual_upset']\n",
        "            actual_upsets.append(actual_upset)\n",
        "            predicted_upsets.append(predicted_upset)\n",
        "            predictions.append({\n",
        "                'ratings_model': ratings_model,\n",
        "                'fold_number': fold,\n",
        "                'match_id': row['match_id'],\n",
        "                'home_team_no': row['home_team_no'],\n",
        "                'home_team_league_level': row['home_team_league_level'],\n",
        "                'away_team_no': row['away_team_no'],\n",
        "                'away_team_league_level': row['away_team_league_level'],\n",
        "                'home_team_rating': home_team_rating,\n",
        "                'away_team_rating': away_team_rating,\n",
        "                'predicted_winner': predicted_winner,\n",
        "                'actual_winner': row['actual_winning_team_no'],\n",
        "                'actual_upset': actual_upset,\n",
        "                'predicted_upset': predicted_upset,\n",
        "            })\n",
        "\n",
        "        # Calculate accuracy\n",
        "        upset_accuracy = accuracy_score(actual_upsets, predicted_upsets)\n",
        "        print(f\"Accuracy score: {upset_accuracy}\")\n",
        "        accuracies.append(upset_accuracy)\n",
        "\n",
        "        # Store true and predicted values for later analysis\n",
        "        all_actual_upsets.extend(actual_upsets)\n",
        "        all_predicted_upsets.extend(predicted_upsets)\n",
        "\n",
        "    # Create DataFrames from results\n",
        "    predictions_df = pd.DataFrame(predictions)\n",
        "    predictions_df.index = range(1, len(predictions) + 1)\n",
        "\n",
        "    upset_accuracy_scores_df = pd.DataFrame(accuracies, columns=['accuracy'])\n",
        "    upset_accuracy_scores_df.index = range(1, len(accuracies) + 1)\n",
        "\n",
        "    # Fold ratings dataframe\n",
        "    fold_ratings_df = pd.DataFrame(fold_ratings, columns=['ratings_model','fold_number','team_no', 'rating'])\n",
        "    fold_ratings_df = fold_ratings_df.sort_values('team_no', ascending=True)\n",
        "    #fold_ratings_df.index = range(1, len(fold_ratings) + 1)\n",
        "    fold_ratings_df['rank'] = fold_ratings_df.groupby('fold_number')['rating'].rank(ascending=False, method='dense').astype(int)\n",
        "\n",
        "    # Mean ratings from 5 folds. Assign 0 fold as mean\n",
        "    mean_ratings_df = fold_ratings_df.groupby('team_no')['rating'].mean().reset_index()\n",
        "    mean_ratings_by_team_df = pd.DataFrame({'ratings_model': ratings_model, 'fold_number': 0,'team_no': mean_ratings_df['team_no'],'rating': mean_ratings_df['rating']})\n",
        "    mean_ratings_by_team_df = mean_ratings_by_team_df.sort_values('rating', ascending=False)\n",
        "    #mean_ratings_by_team_df.index = range(1, len(mean_ratings_by_team_df) + 1)\n",
        "    mean_ratings_by_team_df['rank'] = mean_ratings_by_team_df.groupby('fold_number')['rating'].rank(ascending=False, method='dense').astype(int)\n",
        "\n",
        "    # Combine and sort DataFrame\n",
        "    ratings_df = pd.concat([fold_ratings_df, mean_ratings_by_team_df], ignore_index=True)\n",
        "    ratings_df = ratings_df.sort_values(['team_no', 'fold_number'], ascending=[True, True])\n",
        "\n",
        "    # Calculate overall metrics\n",
        "    mean_upset_accuracy = np.mean(accuracies)\n",
        "    std_upset_accuracy = np.std(accuracies)\n",
        "\n",
        "    return {\n",
        "        'predictions_df': predictions_df,\n",
        "        'upset_accuracy_scores_df': upset_accuracy_scores_df,\n",
        "        'ratings_df': ratings_df,\n",
        "        'mean_upset_accuracy': mean_upset_accuracy,\n",
        "        'std_upset_accuracy': std_upset_accuracy,\n",
        "        'accuracies': accuracies,\n",
        "        'all_actual_upsets': all_actual_upsets,\n",
        "        'all_predicted_upsets': all_predicted_upsets\n",
        "    }\n"
      ],
      "metadata": {
        "id": "mJn0APCOCVfx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_winner_upset(home_team_no, away_team_no, home_team_league_level, away_team_league_level, ratings, home_advantage=0):\n",
        "    home_team_rating = next((rating for team_no, rating in ratings if team_no == home_team_no), None) + home_advantage\n",
        "    away_team_rating = next((rating for team_no, rating in ratings if team_no == away_team_no), None)\n",
        "\n",
        "    if home_team_rating is None or away_team_rating is None:\n",
        "        raise ValueError(\"Team number not found in ratings list.\")\n",
        "\n",
        "    if home_team_rating > away_team_rating:\n",
        "        predicted_winner = home_team_no\n",
        "        if home_team_league_level > away_team_league_level:\n",
        "            predicted_upset = 1\n",
        "        else:\n",
        "            predicted_upset = 0\n",
        "    else:\n",
        "        predicted_winner = away_team_no\n",
        "        if away_team_league_level > home_team_league_level:\n",
        "            predicted_upset = 1\n",
        "        else:\n",
        "            predicted_upset = 0\n",
        "\n",
        "    return predicted_winner, predicted_upset, home_team_rating, away_team_rating"
      ],
      "metadata": {
        "id": "BffqlPA-BulG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_model_results_df(all_y_true, all_y_pred, accuracies, model_name):\n",
        "    report_dict = classification_report(all_y_true, all_y_pred, output_dict=True)\n",
        "    cm = confusion_matrix(all_y_true, all_y_pred)\n",
        "\n",
        "    metrics = []\n",
        "    values = []\n",
        "\n",
        "    # Add cross-validation accuracy\n",
        "    avg_accuracy = np.mean(accuracies)\n",
        "    metrics.append('Cross-validation Accuracy')\n",
        "    values.append(avg_accuracy)\n",
        "\n",
        "    # Add overall accuracy from the classification report\n",
        "    metrics.append('Overall Accuracy')\n",
        "    values.append(report_dict['accuracy'])\n",
        "\n",
        "    # Add confusion matrix values\n",
        "    cm_labels = ['True Negative (Class 0)', 'False Positive (Class 1)', 'False Negative (Class 0)', 'True Positive (Class 1)']\n",
        "    for label, value in zip(cm_labels, cm.ravel()):\n",
        "        metrics.append(f'Confusion Matrix - {label}')\n",
        "        values.append(value)\n",
        "\n",
        "    # Add precision, recall, and f1-score for each class\n",
        "    for class_label in sorted(report_dict.keys()):\n",
        "        if class_label.isdigit():  # This checks if the key is a class label\n",
        "            for metric in ['precision', 'recall', 'f1-score']:\n",
        "                metrics.append(f'{metric.capitalize()} (Class {class_label})')\n",
        "                values.append(report_dict[class_label][metric])\n",
        "\n",
        "    # Add macro and weighted averages\n",
        "    for avg_type in ['macro avg', 'weighted avg']:\n",
        "        for metric in ['precision', 'recall', 'f1-score']:\n",
        "            metrics.append(f'{avg_type.capitalize()} {metric.capitalize()}')\n",
        "            values.append(report_dict[avg_type][metric])\n",
        "\n",
        "    # Calculate and add AUC-ROC score\n",
        "    auc_roc = roc_auc_score(all_y_true, all_y_pred)\n",
        "    metrics.append('AUC-ROC')\n",
        "    values.append(auc_roc)\n",
        "\n",
        "    # Create the DataFrame\n",
        "    results_df = pd.DataFrame({\n",
        "        'metric': metrics,\n",
        "        model_name: values\n",
        "    })\n",
        "\n",
        "    # Format the numeric values to 3 decimal places\n",
        "    results_df[model_name] = results_df[model_name].apply(lambda x: f'{x:.3f}' if isinstance(x, (int, float)) else str(x))\n",
        "\n",
        "    # Present the confusion matrix\n",
        "    cm_fig, ax = plt.subplots(figsize=(10,7))\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False, ax=ax)\n",
        "    ax.set_title(f'Confusion Matrix {model_name}')\n",
        "    ax.set_ylabel('Actual Class\\n(0 not upset and 1 upset)')\n",
        "    ax.set_xlabel('Predicted Class\\n(0 not upset and 1 upset)')\n",
        "\n",
        "    # Plot ROC curve\n",
        "    fpr, tpr, _ = roc_curve(all_y_true, all_y_pred)\n",
        "    roc_fig, roc_ax = plt.subplots(figsize=(8,6))\n",
        "    roc_ax.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {auc_roc:.2f})')\n",
        "    roc_ax.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
        "    roc_ax.set_xlim([0.0, 1.0])\n",
        "    roc_ax.set_ylim([0.0, 1.05])\n",
        "    roc_ax.set_xlabel('False Positive Rate')\n",
        "    roc_ax.set_ylabel('True Positive Rate')\n",
        "    roc_ax.set_title(f'Receiver Operating Characteristic (ROC) Curve - {model_name}')\n",
        "    roc_ax.legend(loc=\"lower right\")\n",
        "\n",
        "    return results_df, cm_fig, roc_fi\n",
        "\n",
        "    return results_df"
      ],
      "metadata": {
        "id": "vjDY954pCHx_"
      },
      "execution_count": 1,
      "outputs": []
    }
  ]
}