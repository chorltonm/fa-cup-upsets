{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPtBbUvi+9S8tOJFZghAwig"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import accuracy_score, recall_score, confusion_matrix, classification_report, roc_auc_score, roc_curve, RocCurveDisplay\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "rDQMfv6vB04x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_winner_upset(home_team_no, away_team_no, home_team_league_level, away_team_league_level, ratings, home_advantage=0):\n",
        "    home_team_rating = next((rating for team_no, rating in ratings if team_no == home_team_no), None) + home_advantage\n",
        "    away_team_rating = next((rating for team_no, rating in ratings if team_no == away_team_no), None)\n",
        "\n",
        "    if home_team_rating is None or away_team_rating is None:\n",
        "        raise ValueError(\"Team number not found in ratings list.\")\n",
        "\n",
        "    # Calculate the probability of the home team winning using a simple logistic function\n",
        "    rating_diff = home_team_rating - away_team_rating\n",
        "    home_win_probability = 1 / (1 + np.exp(-rating_diff))\n",
        "\n",
        "    # Determine the predicted winner and if it's an upset\n",
        "    if home_team_rating > away_team_rating:\n",
        "        predicted_winner = home_team_no\n",
        "        predicted_upset = 1 if home_team_league_level > away_team_league_level else 0\n",
        "        upset_probability = home_win_probability  # Probability of away team winning\n",
        "    else:\n",
        "        predicted_winner = away_team_no\n",
        "        predicted_upset = 1 if away_team_league_level > home_team_league_level else 0\n",
        "        upset_probability = 1 - home_win_probability  # Probability of home team winning\n",
        "\n",
        "    return predicted_winner, predicted_upset, home_team_rating, away_team_rating, upset_probability"
      ],
      "metadata": {
        "id": "BffqlPA-BulG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run_rating_models_cross_validation (fa_cup_scores_df, ratings_function, ratings_model, num_folds):\n",
        "\n",
        "    # Create a StratifiedKFold object and fold counter\n",
        "    skf = StratifiedKFold(n_splits=num_folds, shuffle=True, random_state=47)\n",
        "    fold_counter = 0\n",
        "\n",
        "    # Initialize empty lists to store results\n",
        "    predictions = []\n",
        "    fold_ratings = []\n",
        "    accuracies = []\n",
        "    recalls = []\n",
        "    all_actual_upsets = []\n",
        "    all_predicted_upsets = []\n",
        "    all_upset_probabilities = []\n",
        "\n",
        "    # Iterate over the folds\n",
        "    for fold, (train_idx, test_idx) in enumerate(skf.split(fa_cup_scores_df, fa_cup_scores_df['actual_upset']), start=1):\n",
        "        print(f\"Fold {fold}/{num_folds}\")\n",
        "        fold_counter += 1  # Increment fold_counter\n",
        "\n",
        "        # Split the data into training and test sets\n",
        "        train_data = fa_cup_scores_df.iloc[train_idx]\n",
        "        test_data = fa_cup_scores_df.iloc[test_idx]\n",
        "        print(f\"Train data size: {len(train_data)}\")\n",
        "        print(f\"Test data size: {len(test_data)}\")\n",
        "\n",
        "        # Get distinct teams\n",
        "        all_teams = pd.concat([train_data['home_team_no'], train_data['away_team_no']]).drop_duplicates().sort_values().reset_index(drop=True)\n",
        "        print(ratings_model)\n",
        "        print(ratings_function)\n",
        "        # Call rating function\n",
        "        if ratings_model == 'borda_count':\n",
        "            ratings = ratings_function(fold_counter)\n",
        "        elif ratings_model == 'average_rank':\n",
        "            ratings = ratings_function(fold_counter)\n",
        "        elif ratings_model == 'local_kemeny_optimisation':\n",
        "            ratings = ratings_function(fold_counter)\n",
        "        else:\n",
        "            ratings = ratings_function(train_data)\n",
        "        print(ratings)\n",
        "        # Append the ratings to the list\n",
        "        ratings_list = [(ratings_model, fold, team_no, rating) for team_no, rating in ratings]\n",
        "        fold_ratings.extend(ratings_list)\n",
        "\n",
        "        # Predict upsets and calculate accuracy\n",
        "        actual_upsets = []\n",
        "        predicted_upsets = []\n",
        "        upset_probabilities = []\n",
        "\n",
        "        for _, row in test_data.iterrows():\n",
        "            predicted_winner, predicted_upset, home_team_rating, away_team_rating, upset_probability = predict_winner_upset(\n",
        "                row['home_team_no'], row['away_team_no'],\n",
        "                row['home_team_league_level'], row['away_team_league_level'],\n",
        "                ratings, home_advantage=0\n",
        "            )\n",
        "            actual_upset = row['actual_upset']\n",
        "            actual_upsets.append(actual_upset)\n",
        "            predicted_upsets.append(predicted_upset)\n",
        "            upset_probabilities.append(upset_probability)\n",
        "            predictions.append({\n",
        "                'ratings_model': ratings_model,\n",
        "                'fold_number': fold,\n",
        "                'match_id': row['match_id'],\n",
        "                'home_team_no': row['home_team_no'],\n",
        "                'home_team_league_level': row['home_team_league_level'],\n",
        "                'away_team_no': row['away_team_no'],\n",
        "                'away_team_league_level': row['away_team_league_level'],\n",
        "                'home_team_rating': home_team_rating,\n",
        "                'away_team_rating': away_team_rating,\n",
        "                'predicted_winner': predicted_winner,\n",
        "                'actual_winner': row['actual_winning_team_no'],\n",
        "                'actual_upset': actual_upset,\n",
        "                'predicted_upset': predicted_upset,\n",
        "                'upset_probability': upset_probability,\n",
        "            })\n",
        "\n",
        "        # Calculate accuracy\n",
        "        upset_accuracy = accuracy_score(actual_upsets, predicted_upsets)\n",
        "        print(f\"Accuracy score: {upset_accuracy}\")\n",
        "        accuracies.append(upset_accuracy)\n",
        "\n",
        "        # Calculate recall\n",
        "        upset_recall = recall_score(actual_upsets, predicted_upsets)\n",
        "        print(f\"Recall score: {upset_accuracy}\")\n",
        "        recalls.append(upset_recall)\n",
        "\n",
        "        # Store true and predicted values for later analysis\n",
        "        all_actual_upsets.extend(actual_upsets)\n",
        "        all_predicted_upsets.extend(predicted_upsets)\n",
        "        all_upset_probabilities.extend(upset_probabilities)\n",
        "\n",
        "    # Create DataFrames from results\n",
        "    predictions_df = pd.DataFrame(predictions)\n",
        "    predictions_df.index = range(1, len(predictions) + 1)\n",
        "\n",
        "    upset_accuracy_scores_df = pd.DataFrame(accuracies, columns=['accuracy'])\n",
        "    upset_accuracy_scores_df.index = range(1, len(accuracies) + 1)\n",
        "\n",
        "    # Fold ratings dataframe\n",
        "    fold_ratings_df = pd.DataFrame(fold_ratings, columns=['ratings_model','fold_number','team_no', 'rating'])\n",
        "    fold_ratings_df = fold_ratings_df.sort_values('team_no', ascending=True)\n",
        "    #fold_ratings_df.index = range(1, len(fold_ratings) + 1)\n",
        "    fold_ratings_df['rank'] = fold_ratings_df.groupby('fold_number')['rating'].rank(ascending=False, method='dense').astype(int)\n",
        "\n",
        "    # Mean ratings from 5 folds. Assign 0 fold as mean\n",
        "    mean_ratings_df = fold_ratings_df.groupby('team_no')['rating'].mean().reset_index()\n",
        "    mean_ratings_by_team_df = pd.DataFrame({'ratings_model': ratings_model, 'fold_number': 0,'team_no': mean_ratings_df['team_no'],'rating': mean_ratings_df['rating']})\n",
        "    mean_ratings_by_team_df = mean_ratings_by_team_df.sort_values('rating', ascending=False)\n",
        "    mean_ratings_by_team_df['rank'] = mean_ratings_by_team_df.groupby('fold_number')['rating'].rank(ascending=False, method='dense').astype(int)\n",
        "\n",
        "    # Combine and sort DataFrame\n",
        "    ratings_df = pd.concat([fold_ratings_df, mean_ratings_by_team_df], ignore_index=True)\n",
        "    ratings_df = ratings_df.sort_values(['team_no', 'fold_number'], ascending=[True, True])\n",
        "\n",
        "    # Calculate overall metrics\n",
        "    mean_upset_accuracy = np.mean(accuracies)\n",
        "    std_upset_accuracy = np.std(accuracies)\n",
        "\n",
        "    return {\n",
        "        'predictions_df': predictions_df,\n",
        "        'upset_accuracy_scores_df': upset_accuracy_scores_df,\n",
        "        'ratings_df': ratings_df,\n",
        "        'mean_upset_accuracy': mean_upset_accuracy,\n",
        "        'std_upset_accuracy': std_upset_accuracy,\n",
        "        'accuracies': accuracies,\n",
        "        'recalls': recalls,\n",
        "        'all_actual_upsets': all_actual_upsets,\n",
        "        'all_predicted_upsets': all_predicted_upsets,\n",
        "        'all_upset_probabilities': all_upset_probabilities\n",
        "    }\n"
      ],
      "metadata": {
        "id": "mJn0APCOCVfx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run_rating_models_cross_validation_updated (fa_cup_scores_df, ratings_function, ratings_model, num_folds, home_advantage=0, random_state=47):\n",
        "    # Create a StratifiedKFold object\n",
        "    skf = StratifiedKFold(n_splits=num_folds, shuffle=True, random_state=random_state)\n",
        "\n",
        "    # Initialize lists to store results\n",
        "    fold_train_accuracies = []\n",
        "    fold_train_recalls = []\n",
        "    fold_test_accuracies = []\n",
        "    fold_test_recalls = []\n",
        "    all_y_train_true = []\n",
        "    all_y_train_pred = []\n",
        "    all_y_train_pred_proba = []\n",
        "    all_y_test_true = []\n",
        "    all_y_test_pred = []\n",
        "    all_y_test_pred_proba = []\n",
        "    fold_ratings = []\n",
        "\n",
        "    # Iterate over the folds\n",
        "    for fold, (train_idx, test_idx) in enumerate(skf.split(fa_cup_scores_df, fa_cup_scores_df['actual_upset']), start=1):\n",
        "        print(f\"Fold {fold}/{num_folds}\")\n",
        "\n",
        "        # Split the data into training and test sets\n",
        "        train_data = fa_cup_scores_df.iloc[train_idx]\n",
        "        test_data = fa_cup_scores_df.iloc[test_idx]\n",
        "\n",
        "        # Call rating function\n",
        "        if ratings_model in ['borda_count', 'average_rank', 'local_kemeny_optimisation']:\n",
        "            ratings = ratings_function(fold)\n",
        "        else:\n",
        "            ratings = ratings_function(train_data)\n",
        "\n",
        "        # Append the ratings to the list\n",
        "        ratings_list = [(ratings_model, fold, team_no, rating) for team_no, rating in ratings]\n",
        "        fold_ratings.extend(ratings_list)\n",
        "\n",
        "        # Predict upsets for train and test data\n",
        "        for dataset, is_train in [(train_data, True), (test_data, False)]:\n",
        "            actual_upsets = []\n",
        "            predicted_upsets = []\n",
        "            upset_probabilities = []\n",
        "\n",
        "            for _, row in dataset.iterrows():\n",
        "                _, predicted_upset, _, _, upset_probability = predict_winner_upset(\n",
        "                    row['home_team_no'], row['away_team_no'],\n",
        "                    row['home_team_league_level'], row['away_team_league_level'],\n",
        "                    ratings, home_advantage=home_advantage\n",
        "                )\n",
        "                actual_upset = row['actual_upset']\n",
        "                actual_upsets.append(actual_upset)\n",
        "                predicted_upsets.append(predicted_upset)\n",
        "                upset_probabilities.append(upset_probability)\n",
        "\n",
        "            # Calculate metrics\n",
        "            accuracy = accuracy_score(actual_upsets, predicted_upsets)\n",
        "            recall = recall_score(actual_upsets, predicted_upsets)\n",
        "\n",
        "            if is_train:\n",
        "                fold_train_accuracies.append(accuracy)\n",
        "                fold_train_recalls.append(recall)\n",
        "                all_y_train_true.extend(actual_upsets)\n",
        "                all_y_train_pred.extend(predicted_upsets)\n",
        "                all_y_train_pred_proba.extend(upset_probabilities)\n",
        "            else:\n",
        "                fold_test_accuracies.append(accuracy)\n",
        "                fold_test_recalls.append(recall)\n",
        "                all_y_test_true.extend(actual_upsets)\n",
        "                all_y_test_pred.extend(predicted_upsets)\n",
        "                all_y_test_pred_proba.extend(upset_probabilities)\n",
        "\n",
        "    # Calculate overall metrics\n",
        "    #train_log_loss = log_loss(all_y_train_true, all_y_train_pred_proba)\n",
        "    #test_log_loss = log_loss(all_y_test_true, all_y_test_pred_proba)\n",
        "\n",
        "    # Prepare data for results DataFrame\n",
        "    #results_data = {\n",
        "     #  fold_train_accuracies,\n",
        "       # fold_train_recalls,\n",
        "       # fold_test_accuracies,\n",
        "       # fold_test_recalls,\n",
        "       # all_y_train_true,\n",
        "       # all_y_train_pred,\n",
        "       # all_y_train_pred_proba,\n",
        "       # all_y_test_true,\n",
        "       # all_y_test_pred,\n",
        "       # all_y_test_pred_proba\n",
        "    #}\n",
        "\n",
        "    # Create results DataFrame\n",
        "    #results_df = pd.DataFrame(list(results_data.items()), columns=['metric', ratings_model])\n",
        "\n",
        "    # Create ratings DataFrame\n",
        "    ratings_df = pd.DataFrame(fold_ratings, columns=['ratings_model', 'fold_number', 'team_no', 'rating'])\n",
        "    ratings_df = ratings_df.sort_values(['team_no', 'fold_number'])\n",
        "    ratings_df['rank'] = ratings_df.groupby('fold_number')['rating'].rank(ascending=False, method='dense').astype(int)\n",
        "\n",
        "    return ratings_df, all_y_train_true, all_y_train_pred, all_y_test_true, all_y_test_pred, fold_train_accuracies, fold_train_recalls, fold_test_accuracies, fold_test_recalls, all_y_train_pred_proba, all_y_test_pred_proba, ratings_model"
      ],
      "metadata": {
        "id": "PiNKGx0_7iO0"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_model_results_df(all_y_train_true, all_y_train_pred, all_y_test_true, all_y_test_pred, fold_train_accuracies, fold_train_recalls, fold_test_accuracies, fold_test_recalls, all_y_train_pred_proba, all_y_test_pred_proba, model_name):\n",
        "\n",
        "    from sklearn.metrics import log_loss\n",
        "    print(f\"Model name received: '{model_name}'\")\n",
        "\n",
        "    report_dict = classification_report(all_y_test_true, all_y_test_pred, output_dict=True)\n",
        "    cm = confusion_matrix(all_y_test_true, all_y_test_pred)\n",
        "\n",
        "    metrics = []\n",
        "    values = []\n",
        "\n",
        "    # Add cross-validation Train Accuracy Mean\n",
        "    avg_train_accuracy = np.mean(fold_train_accuracies)\n",
        "    metrics.append('Cross-validation Train Accuracy Mean')\n",
        "    values.append(avg_train_accuracy)\n",
        "\n",
        "    # Add Cross Validation Train Accuracy Standard Deviation\n",
        "    std_train_accuracy = np.std(fold_train_accuracies)\n",
        "    metrics.append('Cross-validation Train Accuracy Standard Deviation')\n",
        "    values.append(std_train_accuracy)\n",
        "\n",
        "    # Add cross-validation Test Accuracy Mean\n",
        "    avg_test_accuracy = np.mean(fold_test_accuracies)\n",
        "    metrics.append('Cross-validation Test Accuracy Mean')\n",
        "    values.append(avg_test_accuracy)\n",
        "\n",
        "    # Add Cross Validation Test Accuracy Standard Deviation\n",
        "    std_test_accuracy = np.std(fold_test_accuracies)\n",
        "    metrics.append('Cross-validation Test Accuracy Standard Deviation')\n",
        "    values.append(std_test_accuracy)\n",
        "\n",
        "    # Add cross-validation Train Recall Mean\n",
        "    avg_train_recall = np.mean(fold_train_recalls)\n",
        "    metrics.append('Cross-validation Train Recall Mean')\n",
        "    values.append(avg_train_recall)\n",
        "\n",
        "    # Add Cross Validation Recall Standard Deviation\n",
        "    std_train_recall = np.std(fold_train_recalls)\n",
        "    metrics.append('Cross-validation Train Recall Standard Deviation')\n",
        "    values.append(std_train_recall)\n",
        "\n",
        "    # Add cross-validation Test Recall Mean\n",
        "    avg_test_recall = np.mean(fold_test_recalls)\n",
        "    metrics.append('Cross-validation Test Recall Mean')\n",
        "    values.append(avg_test_recall)\n",
        "\n",
        "    # Add Cross Validation Recall Standard Deviation\n",
        "    std_test_recall = np.std(fold_test_recalls)\n",
        "    metrics.append('Cross-validation Test Recall Standard Deviation')\n",
        "    values.append(std_test_recall)\n",
        "\n",
        "    # Calculate overall log loss\n",
        "    train_log_loss = log_loss(all_y_train_true, all_y_train_pred_proba)\n",
        "    metrics.append('Cross-validation Train Log Loss')\n",
        "    values.append(train_log_loss)\n",
        "    print(train_log_loss)\n",
        "\n",
        "    test_log_loss = log_loss(all_y_test_true, all_y_test_pred_proba)\n",
        "    metrics.append('Cross-validation Test Log Loss')\n",
        "    values.append(test_log_loss)\n",
        "    print(test_log_loss)\n",
        "\n",
        "    # Add overall accuracy from the classification report\n",
        "    metrics.append('Overall Accuracy')\n",
        "    values.append(report_dict['accuracy'])\n",
        "\n",
        "    # Add confusion matrix values\n",
        "    cm_labels = ['True Negative (Class 0)', 'False Positive (Class 1)', 'False Negative (Class 0)', 'True Positive (Class 1)']\n",
        "    for label, value in zip(cm_labels, cm.ravel()):\n",
        "        metrics.append(f'Confusion Matrix - {label}')\n",
        "        values.append(value)\n",
        "\n",
        "    # Add precision, recall, and f1-score for each class\n",
        "    for class_label in sorted(report_dict.keys()):\n",
        "        if class_label in  {'0.0', '1.0', '0', '1'}:  # This checks if the key is a class label:  # This checks if the key is a class label\n",
        "        #if class_label.isdigit():  # This checks if the key is a class label\n",
        "            for metric in ['precision', 'recall', 'f1-score']:\n",
        "                metrics.append(f'{metric.capitalize()} (Class {class_label})')\n",
        "                values.append(report_dict[class_label][metric])\n",
        "\n",
        "    # Add macro and weighted averages\n",
        "    for avg_type in ['macro avg', 'weighted avg']:\n",
        "        for metric in ['precision', 'recall', 'f1-score']:\n",
        "            metrics.append(f'{avg_type.capitalize()} {metric.capitalize()}')\n",
        "            values.append(report_dict[avg_type][metric])\n",
        "\n",
        "    # Calculate and add AUC-ROC score\n",
        "    if all_y_test_pred_proba is not None:\n",
        "        auc_roc = roc_auc_score(all_y_test_true, all_y_test_pred_proba)\n",
        "        metrics.append('AUC-ROC')\n",
        "        values.append(auc_roc)\n",
        "\n",
        "    # Create the DataFrame\n",
        "    results_df = pd.DataFrame({\n",
        "        'metric': metrics,\n",
        "        model_name: values\n",
        "    })\n",
        "\n",
        "    # Format the numeric values to 3 decimal places\n",
        "    results_df[model_name] = results_df[model_name].apply(lambda x: f'{x:.3f}' if isinstance(x, (int, float)) else str(x))\n",
        "\n",
        "    # Present the confusion matrix\n",
        "    cm_fig, ax = plt.subplots(figsize=(10,7))\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False, ax=ax)\n",
        "    ax.set_title(f'Confusion Matrix {model_name}')\n",
        "    ax.set_ylabel('Actual Class\\n(0 not upset and 1 upset)')\n",
        "    ax.set_xlabel('Predicted Class\\n(0 not upset and 1 upset)')\n",
        "\n",
        "    roc_fig = None\n",
        "\n",
        "    if all_y_test_pred_proba is not None:\n",
        "      # Plot ROC curve\n",
        "      roc_fig, roc_ax = plt.subplots(figsize=(8, 6))\n",
        "      RocCurveDisplay.from_predictions(\n",
        "            all_y_test_true,\n",
        "            all_y_test_pred_proba,\n",
        "            ax=roc_ax,\n",
        "            name=model_name\n",
        "      )\n",
        "      roc_ax.set_title(f'ROC Curve - {model_name}')\n",
        "      roc_ax.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
        "\n",
        "    return results_df, cm_fig, roc_fig\n"
      ],
      "metadata": {
        "id": "vjDY954pCHx_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}