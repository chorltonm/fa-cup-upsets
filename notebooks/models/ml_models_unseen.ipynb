{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPAeKrG+STU0hobqlez1bT+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/chorltonm/fa-cup-upsets/blob/main/notebooks/models/ml_models_unseen.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nEuX80dPfnRR"
      },
      "outputs": [],
      "source": [
        "# Import general python libaries\n",
        "import os\n",
        "import json\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import importlib\n",
        "\n",
        "# Google Cloud libraries\n",
        "from google.cloud import bigquery\n",
        "from google.oauth2 import service_account\n",
        "from google.colab import drive\n",
        "from google.colab import userdata\n",
        "import pandas_gbq\n",
        "\n",
        "# Scikit Learn libraries\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import accuracy_score, recall_score, classification_report, confusion_matrix, roc_auc_score, roc_curve, log_loss\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import OneHotEncoder, MinMaxScaler\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import FunctionTransformer\n",
        "\n",
        "# Other\n",
        "from matplotlib import pyplot\n",
        "import seaborn as sns\n",
        "from xgboost import XGBClassifier"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Change default ouput directory\n",
        "os.chdir('/content/drive/MyDrive/birkbeck_msc-project/python_files')\n",
        "\n"
      ],
      "metadata": {
        "id": "gl-LEV7vfuVs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import user defined python functions. Used importlib as having stability issues with simple import and not picking up the files\n",
        "spec = importlib.util.spec_from_file_location(\"create_model_results\", \"/content/drive/MyDrive/birkbeck_msc-project/python_files/create_model_results.py\")\n",
        "create_model_results = importlib.util.module_from_spec(spec)\n",
        "spec.loader.exec_module(create_model_results)\n"
      ],
      "metadata": {
        "id": "UCXpm3dCxU6a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Authentication credentials and keys\n",
        "# Google Service Account\n",
        "\n",
        "# Load the JSON key from local Google Collab file\n",
        "key = json.load(open('/content/drive/MyDrive/service_account.json', 'r'))\n",
        "\n",
        "# Authenticate using the loaded key\n",
        "credentials = service_account.Credentials.from_service_account_info(key)\n",
        "\n",
        "# Set up the BigQuery client with the credentials to project\n",
        "client = bigquery.Client(credentials=credentials, project='birkbeck-msc-project-422917')"
      ],
      "metadata": {
        "id": "VeWnIKHFi_uN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Feature data for FA Cup\n",
        "\n",
        "# Query Google Big Query\n",
        "fa_cup_features_all = \"\"\"\n",
        "    SELECT * FROM preparation_layer.view_fa_cup_round_3_features\n",
        "\"\"\"\n",
        "\n",
        "fa_cup_features_all_df = client.query(fa_cup_features_all).to_dataframe()\n",
        "display(fa_cup_features_all_df)\n",
        "\n",
        "\n",
        "test_flag = 'Unseen'\n"
      ],
      "metadata": {
        "id": "zkSB4iD1gE8P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to calculate home advantage\n",
        "def add_home_advantage(X):\n",
        "\n",
        "    result = X.groupby(['home_team_league_level', 'away_team_league_level'])['home_win'].mean().reset_index()\n",
        "    result.columns = ['home_team_league_level', 'away_team_league_level', 'home_win_factor']\n",
        "    result['home_win_factor'] = result['home_win_factor'].round(3)\n",
        "\n",
        "    X = X.merge(result, on=['home_team_league_level', 'away_team_league_level'], how='left')\n",
        "    return X, 'home_win_factor'"
      ],
      "metadata": {
        "id": "JmaXYhQh2lyx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to calculate weights\n",
        "def calculate_weights(y):\n",
        "    class_counts = y.value_counts()\n",
        "    total_samples = len(y)\n",
        "    return {class_label: int(round((1 - (count / total_samples)) * 100))\n",
        "            for class_label, count in class_counts.items()}\n"
      ],
      "metadata": {
        "id": "mXPDS-HbRvAg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def classifer_models_optimisation_single_split (fa_cup_features_all_df, model_name, home_advt, weighted, model_classifier, random_state):\n",
        "\n",
        "    # Define the ranking systems\n",
        "    ranking_systems = ['no_ranking','round_3_position', 'massey', 'colley', 'keener', 'trueskill', 'borda_count', 'local_kemeny_optimisation']\n",
        "\n",
        "    # Split data into training and test sets\n",
        "    fa_cup_features_train = fa_cup_features_all_df[~fa_cup_features_all_df['season_year'].isin(['21/22', '22/23'])]\n",
        "    fa_cup_features_test = fa_cup_features_all_df[fa_cup_features_all_df['season_year'].isin(['21/22', '22/23'])]\n",
        "\n",
        "    # Drop 'season_year, match id, match name' column from both sets so not included in model training but then retain for vlaidation set for futher analysis and comparision\n",
        "    analysis_columns = ['season_year', 'match_id', 'match_name','match_final_score']\n",
        "\n",
        "    fa_cup_features_train = fa_cup_features_train.drop(analysis_columns, axis=1)\n",
        "\n",
        "    test_analysis_columns = fa_cup_features_test[analysis_columns].copy()\n",
        "    fa_cup_features_test = fa_cup_features_test.drop(analysis_columns, axis=1)\n",
        "\n",
        "    # Get all columns except the target and ranking columns\n",
        "    target_variable = 'target_variable'\n",
        "    base_features = [col for col in fa_cup_features_train.columns if col != target_variable and not any(f\"{team}_{ranking}\" in col for team in ['home_team', 'away_team'] for ranking in ranking_systems)]\n",
        "\n",
        "    # Identify numeric and categorical columns\n",
        "    numeric_features = fa_cup_features_train[base_features].select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
        "    categorical_features = fa_cup_features_train[base_features].select_dtypes(include=['object']).columns.tolist()\n",
        "\n",
        "    # Define preprocessing steps\n",
        "    numeric_transformer = Pipeline(steps=[\n",
        "        ('imputer', SimpleImputer(strategy='median')),\n",
        "        ('scaler', MinMaxScaler())\n",
        "    ])\n",
        "\n",
        "    categorical_transformer = Pipeline(steps=[\n",
        "        ('imputer', SimpleImputer(strategy='most_frequent')),\n",
        "        ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
        "    ])\n",
        "\n",
        "    ranking_transformer = Pipeline(steps=[\n",
        "        ('scaler', MinMaxScaler())\n",
        "    ])\n",
        "\n",
        "    for ranking in ranking_systems:\n",
        "        model_name_ranking = f\"{model_name} {ranking}\"\n",
        "        print(model_name_ranking)\n",
        "\n",
        "        # Initialize lists to store results for the model\n",
        "        train_accuracies = []\n",
        "        train_recalls = []\n",
        "        test_accuracies = []\n",
        "        test_recalls = []\n",
        "        all_y_train_true = []\n",
        "        all_y_train_pred = []\n",
        "        all_y_train_pred_proba = []  # Store probabilities for ROC\n",
        "        all_y_test_true = []\n",
        "        all_y_test_pred = []\n",
        "        all_y_test_pred_proba = []  # Store probabilities for ROC\n",
        "\n",
        "        # Fetch parameter grid from BigQuery for model name ranking\n",
        "        query = \"\"\"\n",
        "        SELECT param_grid FROM analysis_layer.view_ml_models_best_recall_param_grid\n",
        "        WHERE model_name_ranking = @model_name_ranking\n",
        "        \"\"\"\n",
        "\n",
        "        # Set up the query configuration with the parameter\n",
        "        job_config = bigquery.QueryJobConfig(\n",
        "            query_parameters=[\n",
        "                bigquery.ScalarQueryParameter(\"model_name_ranking\", \"STRING\", model_name_ranking)\n",
        "            ]\n",
        "        )\n",
        "\n",
        "        # Execute the query\n",
        "        query_job = client.query(query, job_config=job_config)\n",
        "        results = query_job.result()\n",
        "\n",
        "        # Process the results and remove 'Classifier__' prefix\n",
        "        param_grid_raw = eval(next(iter(results)).param_grid)\n",
        "        param_grid = {k.replace('classifier__', ''): v for k, v in param_grid_raw.items()}\n",
        "\n",
        "        # Update the model_classifier with the fetched parameters\n",
        "        model_classifier.set_params(**param_grid)\n",
        "\n",
        "        print(\"Updated model parameters:\")\n",
        "\n",
        "        for param, value in model_classifier.get_params().items():\n",
        "            print(f\"  {param}: {value}\")\n",
        "\n",
        "        if  ranking == 'no_ranking':\n",
        "            features = base_features\n",
        "        else:\n",
        "            features = base_features + [f'home_team_{ranking}', f'away_team_{ranking}']\n",
        "\n",
        "        # Create X_train and y_train\n",
        "        X_train = fa_cup_features_train[features]\n",
        "        y_train = fa_cup_features_train[target_variable]\n",
        "\n",
        "        # Create X_val and y_val\n",
        "        X_test = fa_cup_features_test[features]\n",
        "        y_test = fa_cup_features_test[target_variable]\n",
        "\n",
        "        if home_advt == 'yes':\n",
        "            X_train, home_advantage_column = add_home_advantage(X_train)\n",
        "            X_test, _ = add_home_advantage(X_test)\n",
        "            numeric_features = [home_advantage_column] + numeric_features\n",
        "\n",
        "        # Drop home win and league level feature so not used in modelling as impact predictability be proviing the winner\n",
        "        columns_to_drop = ['home_win', 'home_team_league_level', 'away_team_league_level']\n",
        "        X_train = X_train.drop(columns_to_drop, axis=1)\n",
        "        X_test = X_test.drop(columns_to_drop, axis=1)\n",
        "        numeric_features = [feat for feat in numeric_features if feat not in columns_to_drop]\n",
        "        print(f'numeric features {numeric_features}')\n",
        "\n",
        "        # Create preprocessor\n",
        "        preprocessor = ColumnTransformer(\n",
        "            transformers=[\n",
        "                ('num', numeric_transformer, numeric_features),\n",
        "                ('cat', categorical_transformer, categorical_features)\n",
        "            ])\n",
        "\n",
        "        if  ranking != 'no_ranking':\n",
        "            preprocessor.transformers.append(('rank', ranking_transformer, [f'home_team_{ranking}', f'away_team_{ranking}']))\n",
        "\n",
        "        # Create pipeline\n",
        "        pipeline = Pipeline([\n",
        "            ('preprocessor', preprocessor),\n",
        "            ('classifier', model_classifier)\n",
        "        ])\n",
        "\n",
        "        # Fit the model\n",
        "        pipeline.fit(X_train, y_train)\n",
        "\n",
        "        # Calculate train metrics\n",
        "\n",
        "        y_train_pred = pipeline.predict(X_train).astype(int)\n",
        "        y_train_pred_proba = pipeline.predict_proba(X_train)[:, 1]\n",
        "\n",
        "        train_accuracy = accuracy_score(y_train, y_train_pred)\n",
        "        train_recall = recall_score(y_train, y_train_pred)\n",
        "\n",
        "        train_accuracies.append(train_accuracy)\n",
        "        train_recalls.append(train_recall)\n",
        "\n",
        "        # Calculate test metrics\n",
        "        y_test_pred = pipeline.predict(X_test).astype(int)\n",
        "        y_test_pred_proba = pipeline.predict_proba(X_test)[:, 1]\n",
        "\n",
        "        test_accuracy = accuracy_score(y_test, y_test_pred)\n",
        "        test_recall = recall_score(y_test, y_test_pred)\n",
        "\n",
        "        test_accuracies.append(test_accuracy)\n",
        "        test_recalls.append(test_recall)\n",
        "\n",
        "        # Store true and predicted testues for later analysis\n",
        "        all_y_train_true.extend(y_train)\n",
        "        all_y_train_pred.extend(y_train_pred)\n",
        "        all_y_train_pred_proba.extend(y_train_pred_proba)\n",
        "        all_y_test_true.extend(y_test)\n",
        "        all_y_test_pred.extend(y_test_pred)\n",
        "        all_y_test_pred_proba.extend(y_test_pred_proba)\n",
        "\n",
        "        print(f\"{model_name_ranking} Train Accuracy: {train_accuracy:.3f}, test Accuracy: {test_accuracy:.3f}\")\n",
        "        print(f\"{model_name_ranking} Train Recall: {train_recall:.3f}, test Recall: {test_recall:.3f}\")\n",
        "\n",
        "        #results_df, cm_fig, roc_fig = create_model_results.create_model_results_df(y_test, y_pred, accuracy, recall, None, model_name_ranking)\n",
        "        results_df, cm_fig, roc_fig = create_model_results.create_model_results_df (all_y_train_true, all_y_train_pred, all_y_test_true, all_y_test_pred, train_accuracies, train_recalls, test_accuracies, test_recalls, all_y_train_pred_proba, all_y_test_pred_proba, model_name_ranking, test_flag)\n",
        "        results_df = results_df.reset_index()\n",
        "        results_df['metric_id'] = results_df.index + 1\n",
        "        results_df = results_df[['metric_id', 'metric', model_name_ranking]]\n",
        "        results_df = results_df[~results_df['metric'].str.startswith('Cross')]\n",
        "\n",
        "        # Create a DataFrame with features, actual target, and predicted target\n",
        "        comparison_df = pd.DataFrame({\n",
        "            'Actual': y_test,\n",
        "            'Predicted': y_test_pred\n",
        "        })\n",
        "\n",
        "        validation_df = X_test.copy()\n",
        "\n",
        "        for col in analysis_columns:\n",
        "          if col in test_analysis_columns.columns:\n",
        "            validation_df[col] =  test_analysis_columns[col]\n",
        "\n",
        "        #display(X_val)\n",
        "\n",
        "        comparison_df_reset = comparison_df.reset_index(drop=True)\n",
        "        validation_data_reset = validation_df.reset_index(drop=True)\n",
        "\n",
        "        all_data_act_pred_df = comparison_df_reset.merge(validation_data_reset, left_index=True, right_index=True)\n",
        "\n",
        "    return results_df, all_data_act_pred_df\n",
        "\n"
      ],
      "metadata": {
        "id": "u-lUmlMN3Asi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Standard, no weights\n",
        "model_name = \"Standard LogisticRegression\"\n",
        "home_advt = 'no'\n",
        "weighted = 'no'\n",
        "random_state = 47\n",
        "model_classifier  = LogisticRegression(max_iter=1000)\n",
        "\n",
        "all_results_df, fold_results_df  = classifer_models_optimisation_single_split (fa_cup_features_all_df,model_name, home_advt, weighted, model_classifier, random_state)\n",
        "\n",
        "all_results_df_slr = all_results_df\n",
        "display(all_results_df_slr)\n",
        "\n",
        "fold_results_df_slr = fold_results_df\n",
        "display(fold_results_df_slr)\n",
        "\n",
        "# Change default ouput directory\n",
        "os.chdir('/content/drive/MyDrive/birkbeck_msc-project/output_files')\n",
        "# Save ranks to excel\n",
        "all_results_df_slr.to_excel(\"all_slr_results_unseen.xlsx\")\n",
        "fold_results_df_slr.to_excel(\"fold_results_slr_unseen.xlsx\")\n",
        "\n"
      ],
      "metadata": {
        "id": "QHP2tiQpA6J7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Standard, no weights with home advantage\n",
        "model_name = \"Standard LogisticRegression with home advantage\"\n",
        "home_advt = 'yes'\n",
        "weighted = 'no'\n",
        "random_state = 47\n",
        "model_classifier  = LogisticRegression(max_iter=1000)\n",
        "\n",
        "\n",
        "all_results_df, fold_results_df  = classifer_models_optimisation_single_split (fa_cup_features_all_df,model_name, home_advt, weighted, model_classifier, random_state)\n",
        "\n",
        "all_results_df_slrh = all_results_df\n",
        "display(all_results_df_slrh)\n",
        "\n",
        "fold_results_df_slrh = fold_results_df\n",
        "display(fold_results_df_slrh)\n",
        "\n",
        "# Change default ouput directory\n",
        "os.chdir('/content/drive/MyDrive/birkbeck_msc-project/output_files')\n",
        "# Save ranks to excel\n",
        "all_results_df_slrh.to_excel(\"all_slrh_results_unseen.xlsx\")\n",
        "fold_results_df_slrh.to_excel(\"fold_results_slrh_unseen.xlsx\")"
      ],
      "metadata": {
        "id": "7SPyc_cqQJFy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "0# Weighted Logistc Regression\n",
        "model_name = \"Weighted LogisticRegression\"\n",
        "home_advt = 'no'\n",
        "weighted = 'yes'\n",
        "random_state = 47\n",
        "weights = {0: 50, 1: 50} # intial value before recalauclating\n",
        "model_classifier  = LogisticRegression(class_weight=weights, max_iter=1000)\n",
        "\n",
        "all_results_df, fold_results_df  = classifer_models_optimisation_single_split (fa_cup_features_all_df,model_name, home_advt, weighted, model_classifier, random_state)\n",
        "\n",
        "all_results_df_wlr = all_results_df\n",
        "display(all_results_df_wlr)\n",
        "\n",
        "fold_results_df_wlr = fold_results_df\n",
        "display(fold_results_df_wlr)\n",
        "\n",
        "# Change default ouput directory\n",
        "os.chdir('/content/drive/MyDrive/birkbeck_msc-project/output_files')\n",
        "# Save ranks to excel\n",
        "all_results_df_wlr.to_excel(\"all_wlr_results_unseen.xlsx\")\n",
        "fold_results_df_wlr.to_excel(\"fold_results_wlr_unseen.xlsx\")"
      ],
      "metadata": {
        "id": "A3qKLvOXRbqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Weighted Logistc Regression with home advatange\n",
        "model_name = \"Weighted LogisticRegression with home advantage\"\n",
        "home_advt = 'yes'\n",
        "weighted = 'yes'\n",
        "random_state = 47\n",
        "model_classifier  = LogisticRegression(class_weight=weights, max_iter=1000)\n",
        "\n",
        "all_results_df, fold_results_df  = classifer_models_optimisation_single_split (fa_cup_features_all_df,model_name, home_advt, weighted, model_classifier, random_state)\n",
        "\n",
        "all_results_df_wlrh = all_results_df\n",
        "display(all_results_df_wlrh)\n",
        "\n",
        "fold_results_df_wlrh = fold_results_df\n",
        "display(fold_results_df_wlrh)\n",
        "\n",
        "# Change default ouput directory\n",
        "os.chdir('/content/drive/MyDrive/birkbeck_msc-project/output_files')\n",
        "# Save ranks to excel\n",
        "all_results_df_wlrh.to_excel(\"all_wlrh_results.xlsx\")\n",
        "fold_results_df_wlrh.to_excel(\"fold_results_wlrh.xlsx\")"
      ],
      "metadata": {
        "id": "u0ZZD6r7T99E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# MCP Neural Network\n",
        "model_name = \"MLP Classifier Neural Network\"\n",
        "home_advt = 'no'\n",
        "weighted = 'no'\n",
        "random_state = 47\n",
        "model_classifier  = MLPClassifier(max_iter = 10000)\n",
        "\n",
        "all_results_df, fold_results_df  = classifer_models_optimisation_single_split (fa_cup_features_all_df,model_name, home_advt, weighted, model_classifier, random_state)\n",
        "\n",
        "all_results_df_nn = all_results_df\n",
        "display(all_results_df_nn)\n",
        "\n",
        "fold_results_df_nn = fold_results_df\n",
        "display(fold_results_df_nn)\n",
        "\n",
        "# Change default ouput directory\n",
        "os.chdir('/content/drive/MyDrive/birkbeck_msc-project/output_files')\n",
        "# Save ranks to excel\n",
        "all_results_df_nn.to_excel(\"all_nn_results.xlsx\")\n",
        "fold_results_df_nn.to_excel(\"fold_results_nn.xlsx\")"
      ],
      "metadata": {
        "id": "66cJsQ4nUzZA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# MCP Neural Network with home advantage\n",
        "model_name = \"MLP Classifier Neural Network with home advantage\"\n",
        "home_advt = 'yes'\n",
        "weighted = 'no'\n",
        "random_state = 47\n",
        "model_classifier  = MLPClassifier(max_iter = 10000)\n",
        "\n",
        "all_results_df, fold_results_df  = classifer_models_optimisation_single_split (fa_cup_features_all_df,model_name, home_advt, weighted, model_classifier, random_state)\n",
        "\n",
        "all_results_df_nnh = all_results_df\n",
        "display(all_results_df_nnh)\n",
        "\n",
        "fold_results_df_nnh = fold_results_df\n",
        "display(fold_results_df_nnh)\n",
        "\n",
        "# Change default ouput directory\n",
        "os.chdir('/content/drive/MyDrive/birkbeck_msc-project/output_files')\n",
        "# Save ranks to excel\n",
        "all_results_df_nnh.to_excel(\"all_nnh_results.xlsx\")\n",
        "fold_results_df_nnh.to_excel(\"fold_results_nnh.xlsx\")"
      ],
      "metadata": {
        "id": "yFEQOi4TfEGZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Random Forest Classifier\n",
        "model_name = \"Random Forest Classifier\"\n",
        "home_advt = 'no'\n",
        "weighted = 'no'\n",
        "random_state = 47\n",
        "model_classifier  = RandomForestClassifier()\n",
        "\n",
        "all_results_df, fold_results_df  = classifer_models_optimisation_single_split (fa_cup_features_all_df,model_name, home_advt, weighted, model_classifier, random_state)\n",
        "\n",
        "\n",
        "all_results_df_rf = all_results_df\n",
        "display(all_results_df_rf)\n",
        "\n",
        "fold_results_df_rf = fold_results_df\n",
        "display(fold_results_df_rf)\n",
        "\n",
        "# Change default ouput directory\n",
        "os.chdir('/content/drive/MyDrive/birkbeck_msc-project/output_files')\n",
        "# Save ranks to excel\n",
        "all_results_df_rf.to_excel(\"all_rf_results.xlsx\")\n",
        "fold_results_df_rf.to_excel(\"fold_results_rf.xlsx\")\n"
      ],
      "metadata": {
        "id": "9ZHauT1PoxhF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Random Forest Classifier with home advantage\n",
        "model_name = \"Random Forest Classifier with home advantage\"\n",
        "home_advt = 'yes'\n",
        "weighted = 'no'\n",
        "random_state = 47\n",
        "model_classifier  = RandomForestClassifier()\n",
        "\n",
        "all_results_df, fold_results_df  = classifer_models_optimisation_single_split (fa_cup_features_all_df,model_name, home_advt, weighted, model_classifier, random_state)\n",
        "\n",
        "all_results_df_rfh = all_results_df\n",
        "display(all_results_df_rfh)\n",
        "\n",
        "fold_results_df_rfh = fold_results_df\n",
        "display(fold_results_df_rfh)\n",
        "\n",
        "# Change default ouput directory\n",
        "os.chdir('/content/drive/MyDrive/birkbeck_msc-project/output_files')\n",
        "# Save ranks to excel\n",
        "all_results_df_rfh.to_excel(\"all_rfh_results.xlsx\")\n",
        "fold_results_df_rfh.to_excel(\"fold_results_rfh.xlsx\")"
      ],
      "metadata": {
        "id": "XRTb5cdRqqNb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# XG Boost\n",
        "model_name = \"XG Boost\"\n",
        "home_advt = 'no'\n",
        "weighted = 'no'\n",
        "random_state = 47\n",
        "model_classifier  = XGBClassifier()\n",
        "\n",
        "all_results_df, fold_results_df  = classifer_models_optimisation_single_split (fa_cup_features_all_df,model_name, home_advt, weighted, model_classifier, random_state)\n",
        "\n",
        "all_results_df_xg = all_results_df\n",
        "display(all_results_df_xg)\n",
        "\n",
        "fold_results_df_xg = fold_results_df\n",
        "display(fold_results_df_xg)\n",
        "\n",
        "# Change default ouput directory\n",
        "os.chdir('/content/drive/MyDrive/birkbeck_msc-project/output_files')\n",
        "# Save ranks to excel\n",
        "all_results_df_xg.to_excel(\"all_xg_results.xlsx\")\n",
        "fold_results_df_xg.to_excel(\"fold_results_xg.xlsx\")"
      ],
      "metadata": {
        "id": "_7kvKXefra4a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# XG Boost with home advantage\n",
        "model_name = \"XG Boost with home advantage\"\n",
        "home_advt = 'yes'\n",
        "weighted = 'no'\n",
        "random_state = 47\n",
        "model_classifier  = XGBClassifier()\n",
        "\n",
        "all_results_df, fold_results_df  = classifer_models_optimisation_single_split (fa_cup_features_all_df,model_name, home_advt, weighted, model_classifier, random_state)\n",
        "\n",
        "all_results_df_xgh = all_results_df\n",
        "display(all_results_df_xgh)\n",
        "\n",
        "fold_results_df_xgh = fold_results_df\n",
        "display(fold_results_df_xgh)\n",
        "\n",
        "# Change default ouput directory\n",
        "os.chdir('/content/drive/MyDrive/birkbeck_msc-project/output_files')\n",
        "# Save ranks to excel\n",
        "all_results_df_xgh.to_excel(\"all_xgh_results.xlsx\")\n",
        "fold_results_df_xgh.to_excel(\"fold_results_xgh.xlsx\")"
      ],
      "metadata": {
        "id": "4haBnZpSrePq",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_results_df = all_results_df_xg.merge(all_results_df_xgh, on=['metric_id', 'metric'], how='outer')\n",
        "all_results_df = all_results_df_rfh.merge(all_results_df, on=['metric_id', 'metric'], how='outer')\n",
        "all_results_df = all_results_df_rf.merge(all_results_df, on=['metric_id', 'metric'], how='outer')\n",
        "all_results_df = all_results_df_nnh.merge(all_results_df, on=['metric_id', 'metric'], how='outer')\n",
        "all_results_df = all_results_df_nn.merge(all_results_df, on=['metric_id', 'metric'], how='outer')\n",
        "all_results_df = all_results_df_wlrh.merge(all_results_df, on=['metric_id', 'metric'], how='outer')\n",
        "all_results_df = all_results_df_wlr.merge(all_results_df, on=['metric_id', 'metric'], how='outer')\n",
        "all_results_df = all_results_df_slrh.merge(all_results_df, on=['metric_id', 'metric'], how='outer')\n",
        "all_results_df = all_results_df_slr.merge(all_results_df, on=['metric_id', 'metric'], how='outer')\n",
        "\n",
        "all_results_df = all_results_df.rename(columns=lambda x: x.lower().replace(' ','(_)').replace('(', '').replace(')', ''))\n",
        "display(all_results_df)\n",
        "all_results_df.to_excel(\"ml_model_confusion_matrix_results.xlsx\")\n",
        "\n",
        "# Load data from Excel to Google BigQuery\n",
        "all_results_from_excel = pd.read_excel(\"all_results.xlsx\")\n",
        "load_dataset_name = 'analysis_layer'\n",
        "load_table_name = 'ml_model_confusion_matrix_results'\n",
        "full_table_name = f\"{load_dataset_name}.{load_table_name}\"\n",
        "\n",
        "pandas_gbq.to_gbq(all_results_from_excel, full_table_name,\n",
        "                  project_id='birkbeck-msc-project-422917',\n",
        "                  if_exists='replace')\n",
        "\n",
        "print(f\"\\nData loaded to BigQuery table: {full_table_name}\")\n"
      ],
      "metadata": {
        "id": "jKYcSFBfg8AO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load updated Fold results to Google Big Query\n",
        "\n",
        "# Merge fold results from each model\n",
        "fold_results_df = pd.concat([fold_results_df_slr, fold_results_df_slrh, fold_results_df_wlr, fold_results_df_wlrh, fold_results_df_nn, fold_results_df_nnh, fold_results_df_rf, fold_results_df_rfh, fold_results_df_xg, fold_results_df_xgh])\n",
        "fold_results_df = fold_results_df.rename(columns=lambda x: x.lower().replace(' ','(_)').replace('(', '').replace(')', ''))\n",
        "display(fold_results_df)\n",
        "\n",
        "# Write to Excel\n",
        "fold_results_df.to_excel(\"ml_model_fold_results.xlsx\")\n",
        "\n",
        "# Load fold results data from Excel to Google BigQuery\n",
        "fold_results_from_excel = pd.read_excel(\"all_results_.xlsx\")\n",
        "load_dataset_name = 'analysis_layer'\n",
        "load_table_name = 'ml_model_fold_results'\n",
        "full_table_name = f\"{load_dataset_name}.{load_table_name}\"\n",
        "\n",
        "pandas_gbq.to_gbq(fold_results_from_excel, full_table_name,\n",
        "                  project_id='birkbeck-msc-project-422917',\n",
        "                  if_exists='replace')\n",
        "\n",
        "print(f\"\\nData loaded to BigQuery table: {full_table_name}\")\n"
      ],
      "metadata": {
        "id": "l5PqY_9ghWwq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}