{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/chorltonm/fa-cup-upsets/blob/main/notebooks/models/rating_models.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nEuX80dPfnRR"
      },
      "outputs": [],
      "source": [
        "# Import Libaries\n",
        "import os\n",
        "import sys\n",
        "import json\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import importlib\n",
        "import pandas_gbq\n",
        "\n",
        "\n",
        "from google.cloud import bigquery\n",
        "from google.oauth2 import service_account\n",
        "from google.colab import drive\n",
        "from google.colab import userdata\n",
        "\n",
        "# Install scikit learn\n",
        "from sklearn.model_selection import StratifiedShuffleSplit, StratifiedKFold\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install Trueskill library\n",
        "!pip install trueskill\n",
        "from trueskill import Rating, rate_1vs1"
      ],
      "metadata": {
        "id": "Dak7ocxQSN8m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gl-LEV7vfuVs"
      },
      "outputs": [],
      "source": [
        "# Mount Google Drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "# Change default ouput directory\n",
        "os.chdir('/content/drive/MyDrive/birkbeck_msc-project/output_files')\n",
        "\n",
        "# Change default ouput directory\n",
        "os.chdir('/content/drive/MyDrive/birkbeck_msc-project/python_files')\n",
        "\n",
        "\n",
        "# Import user defined python functions\n",
        "import model_evaluation_functions\n",
        "import ratings_functions\n",
        "importlib.reload(model_evaluation_functions)\n",
        "importlib.reload(ratings_functions)\n",
        "\n",
        "from ratings_functions import *\n",
        "from model_evaluation_functions import *\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VeWnIKHFi_uN"
      },
      "outputs": [],
      "source": [
        "# Authentication credentials and keys\n",
        "\n",
        "# Google Service Account\n",
        "\n",
        "# Load the JSON key from local Google Collab file\n",
        "key = json.load(open('/content/drive/MyDrive/service_account.json', 'r'))\n",
        "\n",
        "# Authenticate using the loaded key\n",
        "credentials = service_account.Credentials.from_service_account_info(key)\n",
        "\n",
        "# Set up the BigQuery client with the credentials to project\n",
        "client = bigquery.Client(credentials=credentials, project='birkbeck-msc-project-422917')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def load_predictions_and_ratings_to_bigquery(client, predictions_df, fold_ratings_df, ratings_model):\n",
        "\n",
        "  # Load predictions to Big Query\n",
        "  # Specify the target table\n",
        "  load_dataset_name = 'analysis_layer'\n",
        "  load_table_name = 'ratings_predictions'\n",
        "  load_table_ref = f\"{load_dataset_name}.{load_table_name}\"\n",
        "\n",
        "  # Delete predicitons method already inserted\n",
        "  delete_query = f\"DELETE FROM `{load_dataset_name}.{load_table_name}` WHERE ratings_model = '{ratings_model}'\"\n",
        "  delete_job = client.query(delete_query)\n",
        "  delete_result = delete_job.result()\n",
        "  predictions_total_rows_deleted = delete_result.num_dml_affected_rows\n",
        "  print(f\"Big Query target predictions table {load_dataset_name}.{load_table_name} rows deleted: {predictions_total_rows_deleted}\")\n",
        "\n",
        "  # Insert predictions for ratings model data to the existing table\n",
        "  job_config = bigquery.LoadJobConfig(\n",
        "          write_disposition=\"WRITE_APPEND\")\n",
        "  load_job = client.load_table_from_dataframe(\n",
        "          predictions_df, load_table_ref, job_config=job_config)\n",
        "  # Wait for the job to complete\n",
        "  load_job.result()\n",
        "  predictions_num_rows_inserted = load_job.output_rows\n",
        "  print(f\"{predictions_num_rows_inserted} rows appended to predictions table {load_table_ref} successfully.\")\n",
        "\n",
        "  # Load fold ratings to Big Query\n",
        "\n",
        "  # Specify the target table\n",
        "  load_dataset_name = 'analysis_layer'\n",
        "  load_table_name = 'ratings'\n",
        "  load_table_ref = f\"{load_dataset_name}.{load_table_name}\"\n",
        "\n",
        "  # Delete ratings method already inserted\n",
        "  delete_query = f\"DELETE FROM `{load_dataset_name}.{load_table_name}` WHERE ratings_model = '{ratings_model}'\"\n",
        "  delete_job = client.query(delete_query)\n",
        "  delete_result = delete_job.result()\n",
        "  ratings_total_rows_deleted = delete_result.num_dml_affected_rows\n",
        "  print(f\"Big Query target ratings table {load_dataset_name}.{load_table_name} rows deleted: {ratings_total_rows_deleted}\")\n",
        "\n",
        "\n",
        "  # Insert ratings for ratings model data to the existing table\n",
        "  job_config = bigquery.LoadJobConfig(\n",
        "          write_disposition=\"WRITE_APPEND\")\n",
        "  load_job = client.load_table_from_dataframe(\n",
        "          ratings_df, load_table_ref, job_config=job_config)\n",
        "\n",
        "  load_job.result()  # Wait for the job to complete\n",
        "\n",
        "  ratings_num_rows_inserted = load_job.output_rows\n",
        "  print(f\"{ratings_num_rows_inserted} rows appended to ratings table {load_table_ref} successfully.\")\n",
        "\n",
        "  return\n"
      ],
      "metadata": {
        "id": "IJkpPc-4oBLF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zkSB4iD1gE8P"
      },
      "outputs": [],
      "source": [
        "# FA Cup Data 13 season from 08/09 to 21/20\n",
        "\n",
        "fa_cup_scores = \"\"\"\n",
        "    SELECT * FROM preparation_layer.view_fa_cup_scores WHERE season_year NOT IN ('21/22', '22/23') ORDER BY sort_order ASC\n",
        "\"\"\"\n",
        "\n",
        "fa_cup_scores_df = client.query(fa_cup_scores).to_dataframe()\n",
        "display(fa_cup_scores_df)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_winner_upset (home_team_no, away_team_no, home_team_league_level, away_team_league_level, ranked_ratings):\n",
        "    home_team_info = next((info for info in ranked_ratings if info[0] == home_team_no), None)\n",
        "    away_team_info = next((info for info in ranked_ratings if info[0] == away_team_no), None)\n",
        "\n",
        "    if home_team_info is None or away_team_info is None:\n",
        "        raise ValueError(\"Team number not found in ratings list.\")\n",
        "\n",
        "    home_team_rating, home_team_rank = home_team_info[1], home_team_info[2]\n",
        "    away_team_rating, away_team_rank = away_team_info[1], away_team_info[2]\n",
        "\n",
        "    # Calculate the basic probability of the higer ranked team winning based on rank our of 64 teams\n",
        "\n",
        "    home_team_win_probability = 1 - (home_team_rank / (home_team_rank + away_team_rank))\n",
        "    away_team_win_probability = 1 - (away_team_rank / (home_team_rank + away_team_rank))\n",
        "\n",
        "\n",
        "    # Determine the predicted winner and if it's an upset\n",
        "    if home_team_rating > away_team_rating:\n",
        "        predicted_winner = home_team_no\n",
        "        predicted_upset = 1 if home_team_league_level > away_team_league_level else 0\n",
        "    else:\n",
        "        predicted_winner = away_team_no\n",
        "        predicted_upset = 1 if away_team_league_level > home_team_league_level else 0\n",
        "\n",
        "    # Probaility of upset based on league level and rank differnce\n",
        "    if home_team_league_level > away_team_league_level:\n",
        "       upset_probability = home_team_win_probability\n",
        "    else:\n",
        "       upset_probability = away_team_win_probability\n",
        "\n",
        "    return predicted_winner, predicted_upset, home_team_rating, away_team_rating, home_team_rank, away_team_rank, upset_probability"
      ],
      "metadata": {
        "id": "RQhWk7aqpAw-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run_rating_models_cross_validation (fa_cup_scores_df, ratings_function, ratings_model, num_folds, random_state=47):\n",
        "    # Create a StratifiedKFold object\n",
        "    skf = StratifiedKFold(n_splits=num_folds, shuffle=True, random_state=random_state)\n",
        "\n",
        "    # Initialize lists to store results\n",
        "    fold_train_accuracies = []\n",
        "    fold_train_recalls = []\n",
        "    fold_test_accuracies = []\n",
        "    fold_test_recalls = []\n",
        "    all_y_train_true = []\n",
        "    all_y_train_pred = []\n",
        "    all_y_train_pred_proba = []\n",
        "    all_y_test_true = []\n",
        "    all_y_test_pred = []\n",
        "    all_y_test_pred_proba = []\n",
        "    fold_ratings = []\n",
        "    test_predictions = []\n",
        "\n",
        "    # Iterate over the folds\n",
        "    for fold, (train_idx, test_idx) in enumerate(skf.split(fa_cup_scores_df, fa_cup_scores_df['actual_upset']), start=1):\n",
        "        print(f\"Fold {fold}/{num_folds}\")\n",
        "\n",
        "        # Split the data into training and test sets\n",
        "        train_data = fa_cup_scores_df.iloc[train_idx]\n",
        "        test_data = fa_cup_scores_df.iloc[test_idx]\n",
        "\n",
        "        # Call rating function\n",
        "        if ratings_model in ['borda_count', 'average_rank', 'local_kemeny_optimisation']:\n",
        "            ratings = ratings_function(fold)\n",
        "        else:\n",
        "            ratings = ratings_function(train_data)\n",
        "\n",
        "        # Add ranks based on the sorted order\n",
        "        sorted_ratings = sorted(ratings, key=lambda x: x[1], reverse=True)\n",
        "        ranked_ratings = [(team_no, rating, index + 1) for index, (team_no, rating) in enumerate(sorted_ratings)]\n",
        "        print(ranked_ratings)\n",
        "\n",
        "        # Append the ratings to the list\n",
        "        ratings_list = [(ratings_model, fold, team_no, rating) for team_no, rating in ratings]\n",
        "        fold_ratings.extend(ratings_list)\n",
        "\n",
        "         # Predict upsets for train and test data\n",
        "        # Predict upsets for train data\n",
        "        train_actual_upsets = []\n",
        "        train_predicted_upsets = []\n",
        "        train_upset_probabilities = []\n",
        "\n",
        "        for _, row in train_data.iterrows():\n",
        "            predicted_winner, predicted_upset, home_team_rating, away_team_rating,  home_team_rank, away_team_rank, upset_probability = predict_winner_upset(\n",
        "                row['home_team_no'], row['away_team_no'],\n",
        "                row['home_team_league_level'], row['away_team_league_level'],\n",
        "                ranked_ratings\n",
        "            )\n",
        "            actual_upset = row['actual_upset']\n",
        "            rating_diff = home_team_rating - away_team_rating\n",
        "            train_actual_upsets.append(actual_upset)\n",
        "            train_predicted_upsets.append(predicted_upset)\n",
        "            train_upset_probabilities.append(upset_probability)\n",
        "            #print(upset_probability)\n",
        "\n",
        "        # Calculate train metrics\n",
        "        train_accuracy = accuracy_score(train_actual_upsets, train_predicted_upsets)\n",
        "        train_recall = recall_score(train_actual_upsets, train_predicted_upsets)\n",
        "        fold_train_accuracies.append(train_accuracy)\n",
        "        fold_train_recalls.append(train_recall)\n",
        "        all_y_train_true.extend(train_actual_upsets)\n",
        "        all_y_train_pred.extend(train_predicted_upsets)\n",
        "        all_y_train_pred_proba.extend(train_upset_probabilities)\n",
        "\n",
        "        # Predict upsets for test data\n",
        "        test_actual_upsets = []\n",
        "        test_predicted_upsets = []\n",
        "        test_upset_probabilities = []\n",
        "\n",
        "        for _, row in test_data.iterrows():\n",
        "            predicted_winner, predicted_upset, home_team_rating, away_team_rating, home_team_rank, away_team_rank, upset_probability = predict_winner_upset(\n",
        "                row['home_team_no'], row['away_team_no'],\n",
        "                row['home_team_league_level'], row['away_team_league_level'],\n",
        "                ranked_ratings\n",
        "            )\n",
        "            actual_upset = row['actual_upset']\n",
        "            rating_diff = home_team_rating - away_team_rating\n",
        "            test_actual_upsets.append(actual_upset)\n",
        "            test_predicted_upsets.append(predicted_upset)\n",
        "            test_upset_probabilities.append(upset_probability)\n",
        "\n",
        "            test_predictions.append({\n",
        "              'ratings_model': ratings_model,\n",
        "              'fold_number': fold,\n",
        "              'match_id': row['match_id'],\n",
        "              'home_team_no': row['home_team_no'],\n",
        "              'home_team_league_level': row['home_team_league_level'],\n",
        "              'away_team_no': row['away_team_no'],\n",
        "              'away_team_league_level': row['away_team_league_level'],\n",
        "              'home_team_rating': home_team_rating,\n",
        "              'away_team_rating': away_team_rating,\n",
        "              'home_team_rank': home_team_rank,\n",
        "              'away_team_rank': away_team_rank,\n",
        "              'predicted_winner': predicted_winner,\n",
        "              'actual_winner': row['actual_winning_team_no'],\n",
        "              'actual_upset': actual_upset,\n",
        "              'predicted_upset': predicted_upset,\n",
        "              'upset_probability': upset_probability,\n",
        "            })\n",
        "\n",
        "        # Calculate test metrics\n",
        "        test_accuracy = accuracy_score(test_actual_upsets, test_predicted_upsets)\n",
        "        test_recall = recall_score(test_actual_upsets, test_predicted_upsets)\n",
        "        fold_test_accuracies.append(test_accuracy)\n",
        "        fold_test_recalls.append(test_recall)\n",
        "        all_y_test_true.extend(test_actual_upsets)\n",
        "        all_y_test_pred.extend(test_predicted_upsets)\n",
        "        all_y_test_pred_proba.extend(test_upset_probabilities)\n",
        "\n",
        "        print(f\"Fold {fold} Train Accuracy: {train_accuracy:.3f}, Train Recall: {train_recall:.3f}\")\n",
        "        print(f\"Fold {fold} Test Accuracy: {test_accuracy:.3f}, Test Recall: {test_recall:.3f}\")\n",
        "\n",
        "        print(all_y_train_true)\n",
        "        print(all_y_train_pred_proba)\n",
        "\n",
        "        test_predictions_df = pd.DataFrame(test_predictions)\n",
        "\n",
        "        # Create ratings DataFrame\n",
        "        ratings_df = pd.DataFrame(fold_ratings, columns=['ratings_model', 'fold_number', 'team_no', 'rating'])\n",
        "        ratings_df = ratings_df.sort_values(['team_no', 'fold_number'])\n",
        "        ratings_df['rank'] = ratings_df.groupby('fold_number')['rating'].rank(ascending=False, method='dense').astype(int)\n",
        "\n",
        "\n",
        "    return ratings_df, test_predictions_df, all_y_train_true, all_y_train_pred, all_y_test_true, all_y_test_pred, fold_train_accuracies, fold_train_recalls, fold_test_accuracies, fold_test_recalls, all_y_train_pred_proba, all_y_test_pred_proba, ratings_model"
      ],
      "metadata": {
        "id": "VLZMvNX7oR0y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Results & Rank Dataframes\n",
        "all_ranks_df = pd.DataFrame(columns=['team_no','fold_number'])\n",
        "display(all_ranks_df)\n",
        "\n",
        "all_results_df = pd.DataFrame(columns=['metric_id', 'metric'])\n",
        "display(all_results_df)"
      ],
      "metadata": {
        "id": "W20pKD_QwgrQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Basic Position Ratings\n",
        "ratings_model = 'basic_position'\n",
        "ratings_function = basic_position_ratings\n",
        "num_folds = 5\n",
        "\n",
        "# Call ratings function and run cross validation\n",
        "\n",
        "(ratings_df,test_predictions_df, all_y_train_true, all_y_train_pred, all_y_test_true, all_y_test_pred,\n",
        " fold_train_accuracies, fold_train_recalls, fold_test_accuracies, fold_test_recalls,\n",
        " all_y_train_pred_proba, all_y_test_pred_proba, ratings_model) = run_rating_models_cross_validation (fa_cup_scores_df, ratings_function, ratings_model, num_folds)\n",
        "\n",
        "\n",
        "# Display results\n",
        "test_predictions_df.index = range(1, len(test_predictions_df) + 1)\n",
        "display(test_predictions_df)\n",
        "\n",
        "ratings_df.index = range(1, len(ratings_df) + 1)\n",
        "display(ratings_df)\n",
        "\n",
        "# Load predictions and ratinbgs to Google Big Query\n",
        "#load_predictions_and_ratings_to_bigquery(client, test_predictions_df, ratings_df, ratings_model)\n",
        "\n",
        "# Create dataframe to compare rank and use for final rank aggregation\n",
        "rank_df = ratings_df[['team_no', 'fold_number', 'rank']]\n",
        "rank_df = rank_df.rename(columns={'rank': ratings_model})\n",
        "all_ranks_df = all_ranks_df.merge(rank_df, on=['team_no','fold_number'], how='outer')\n",
        "display(all_ranks_df)\n",
        "\n",
        "# Create confusion matrix from results\n",
        "model_name_ranking = ratings_model\n",
        "results_df, cm_fig, roc_fig = create_model_results_df(all_y_train_true, all_y_train_pred, all_y_test_true, all_y_test_pred, fold_train_accuracies, fold_train_recalls, fold_test_accuracies, fold_test_recalls, all_y_train_pred_proba, all_y_test_pred_proba, model_name_ranking)\n",
        "\n",
        "results_df = results_df.reset_index()\n",
        "results_df['metric_id'] = results_df.index + 1\n",
        "results_df = results_df[['metric_id', 'metric', ratings_model]]\n",
        "\n",
        "all_results_df = all_results_df.merge(results_df, on=['metric_id', 'metric'], how='outer')\n",
        "display(all_results_df)\n",
        "\n"
      ],
      "metadata": {
        "id": "OK204TqPmAsc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Massey Ratings\n",
        "ratings_model = 'massey'\n",
        "ratings_function = massey_ratings\n",
        "num_folds = 5\n",
        "\n",
        "# Call ratings function and run cross validation\n",
        "\n",
        "(ratings_df,test_predictions_df, all_y_train_true, all_y_train_pred, all_y_test_true, all_y_test_pred,\n",
        " fold_train_accuracies, fold_train_recalls, fold_test_accuracies, fold_test_recalls,\n",
        " all_y_train_pred_proba, all_y_test_pred_proba, ratings_model) = run_rating_models_cross_validation (fa_cup_scores_df, ratings_function, ratings_model, num_folds)\n",
        "\n",
        "\n",
        "# Display results\n",
        "test_predictions_df.index = range(1, len(test_predictions_df) + 1)\n",
        "display(test_predictions_df)\n",
        "\n",
        "ratings_df.index = range(1, len(ratings_df) + 1)\n",
        "display(ratings_df)\n",
        "\n",
        "# Load predictions and ratinbgs to Google Big Query\n",
        "#load_predictions_and_ratings_to_bigquery(client, test_predictions_df, ratings_df, ratings_model)\n",
        "\n",
        "# Create dataframe to compare rank and use for final rank aggregation\n",
        "rank_df = ratings_df[['team_no', 'fold_number', 'rank']]\n",
        "rank_df = rank_df.rename(columns={'rank': ratings_model})\n",
        "all_ranks_df = all_ranks_df.merge(rank_df, on=['team_no','fold_number'], how='outer')\n",
        "display(all_ranks_df)\n",
        "\n",
        "# Create confusion matrix from results\n",
        "model_name_ranking = ratings_model\n",
        "results_df, cm_fig, roc_fig = create_model_results_df(all_y_train_true, all_y_train_pred, all_y_test_true, all_y_test_pred, fold_train_accuracies, fold_train_recalls, fold_test_accuracies, fold_test_recalls, all_y_train_pred_proba, all_y_test_pred_proba, model_name_ranking)\n",
        "\n",
        "results_df = results_df.reset_index()\n",
        "results_df['metric_id'] = results_df.index + 1\n",
        "results_df = results_df[['metric_id', 'metric', ratings_model]]\n",
        "\n",
        "all_results_df = all_results_df.merge(results_df, on=['metric_id', 'metric'], how='outer')\n",
        "display(all_results_df)"
      ],
      "metadata": {
        "id": "PnW1lIkvYMo_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Colley Ratings\n",
        "ratings_model = 'colley'\n",
        "ratings_function = colley_ratings\n",
        "num_folds = 5\n",
        "\n",
        "# Call ratings function and run cross validation\n",
        "\n",
        "(ratings_df,test_predictions_df, all_y_train_true, all_y_train_pred, all_y_test_true, all_y_test_pred,\n",
        " fold_train_accuracies, fold_train_recalls, fold_test_accuracies, fold_test_recalls,\n",
        " all_y_train_pred_proba, all_y_test_pred_proba, ratings_model) = run_rating_models_cross_validation (fa_cup_scores_df, ratings_function, ratings_model, num_folds)\n",
        "\n",
        "# Display results\n",
        "test_predictions_df.index = range(1, len(test_predictions_df) + 1)\n",
        "display(test_predictions_df)\n",
        "\n",
        "ratings_df.index = range(1, len(ratings_df) + 1)\n",
        "display(ratings_df)\n",
        "\n",
        "# Load predictions and ratinbgs to Google Big Query\n",
        "#load_predictions_and_ratings_to_bigquery(client, test_predictions_df, ratings_df, ratings_model)\n",
        "\n",
        "# Create dataframe to compare rank and use for final rank aggregation\n",
        "rank_df = ratings_df[['team_no', 'fold_number', 'rank']]\n",
        "rank_df = rank_df.rename(columns={'rank': ratings_model})\n",
        "all_ranks_df = all_ranks_df.merge(rank_df, on=['team_no','fold_number'], how='outer')\n",
        "display(all_ranks_df)\n",
        "\n",
        "# Create confusion matrix from results\n",
        "model_name_ranking = ratings_model\n",
        "results_df, cm_fig, roc_fig = create_model_results_df(all_y_train_true, all_y_train_pred, all_y_test_true, all_y_test_pred, fold_train_accuracies, fold_train_recalls, fold_test_accuracies, fold_test_recalls, all_y_train_pred_proba, all_y_test_pred_proba, model_name_ranking)\n",
        "\n",
        "results_df = results_df.reset_index()\n",
        "results_df['metric_id'] = results_df.index + 1\n",
        "results_df = results_df[['metric_id', 'metric', ratings_model]]\n",
        "\n",
        "all_results_df = all_results_df.merge(results_df, on=['metric_id', 'metric'], how='outer')\n",
        "display(all_results_df)"
      ],
      "metadata": {
        "id": "tVYZYFfVYUj9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Keener Ratings\n",
        "ratings_model = 'keener'\n",
        "ratings_function = keener_ratings\n",
        "num_folds = 5\n",
        "\n",
        "# Call ratings function and run cross validation\n",
        "(ratings_df,test_predictions_df, all_y_train_true, all_y_train_pred, all_y_test_true, all_y_test_pred,\n",
        " fold_train_accuracies, fold_train_recalls, fold_test_accuracies, fold_test_recalls,\n",
        " all_y_train_pred_proba, all_y_test_pred_proba, ratings_model) = run_rating_models_cross_validation (fa_cup_scores_df, ratings_function, ratings_model, num_folds)\n",
        "\n",
        "\n",
        "# Display results\n",
        "test_predictions_df.index = range(1, len(test_predictions_df) + 1)\n",
        "display(test_predictions_df)\n",
        "\n",
        "ratings_df.index = range(1, len(ratings_df) + 1)\n",
        "display(ratings_df)\n",
        "\n",
        "# Load predictions and ratinbgs to Google Big Query\n",
        "#load_predictions_and_ratings_to_bigquery(client, test_predictions_df, ratings_df, ratings_model)\n",
        "\n",
        "# Create dataframe to compare rank and use for final rank aggregation\n",
        "rank_df = ratings_df[['team_no', 'fold_number', 'rank']]\n",
        "rank_df = rank_df.rename(columns={'rank': ratings_model})\n",
        "all_ranks_df = all_ranks_df.merge(rank_df, on=['team_no','fold_number'], how='outer')\n",
        "display(all_ranks_df)\n",
        "\n",
        "# Create confusion matrix from results\n",
        "model_name_ranking = ratings_model\n",
        "results_df, cm_fig, roc_fig = create_model_results_df(all_y_train_true, all_y_train_pred, all_y_test_true, all_y_test_pred, fold_train_accuracies, fold_train_recalls, fold_test_accuracies, fold_test_recalls, all_y_train_pred_proba, all_y_test_pred_proba, model_name_ranking)\n",
        "\n",
        "results_df = results_df.reset_index()\n",
        "results_df['metric_id'] = results_df.index + 1\n",
        "results_df = results_df[['metric_id', 'metric', ratings_model]]\n",
        "\n",
        "all_results_df = all_results_df.merge(results_df, on=['metric_id', 'metric'], how='outer')\n",
        "display(all_results_df)"
      ],
      "metadata": {
        "id": "WsV_BMUrt390"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Trueskill Ratings\n",
        "ratings_model = 'trueskill'\n",
        "ratings_function = trueskill_ratings\n",
        "num_folds = 5\n",
        "\n",
        "# Call ratings function and run cross validation\n",
        "(ratings_df,test_predictions_df, all_y_train_true, all_y_train_pred, all_y_test_true, all_y_test_pred,\n",
        " fold_train_accuracies, fold_train_recalls, fold_test_accuracies, fold_test_recalls,\n",
        " all_y_train_pred_proba, all_y_test_pred_proba, ratings_model) = run_rating_models_cross_validation (fa_cup_scores_df, ratings_function, ratings_model, num_folds)\n",
        "\n",
        "\n",
        "# Display results\n",
        "test_predictions_df.index = range(1, len(test_predictions_df) + 1)\n",
        "display(test_predictions_df)\n",
        "\n",
        "ratings_df.index = range(1, len(ratings_df) + 1)\n",
        "display(ratings_df)\n",
        "\n",
        "# Load predictions and ratinbgs to Google Big Query\n",
        "#load_predictions_and_ratings_to_bigquery(client, test_predictions_df, ratings_df, ratings_model)\n",
        "\n",
        "# Create dataframe to compare rank and use for final rank aggregation\n",
        "rank_df = ratings_df[['team_no', 'fold_number', 'rank']]\n",
        "rank_df = rank_df.rename(columns={'rank': ratings_model})\n",
        "all_ranks_df = all_ranks_df.merge(rank_df, on=['team_no','fold_number'], how='outer')\n",
        "display(all_ranks_df)\n",
        "\n",
        "# Create confusion matrix from results\n",
        "model_name_ranking = ratings_model\n",
        "results_df, cm_fig, roc_fig = create_model_results_df(all_y_train_true, all_y_train_pred, all_y_test_true, all_y_test_pred, fold_train_accuracies, fold_train_recalls, fold_test_accuracies, fold_test_recalls, all_y_train_pred_proba, all_y_test_pred_proba, model_name_ranking)\n",
        "\n",
        "results_df = results_df.reset_index()\n",
        "results_df['metric_id'] = results_df.index + 1\n",
        "results_df = results_df[['metric_id', 'metric', ratings_model]]\n",
        "\n",
        "all_results_df = all_results_df.merge(results_df, on=['metric_id', 'metric'], how='outer')\n",
        "display(all_results_df)\n"
      ],
      "metadata": {
        "id": "Won8EiSZuHEl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kOer_ThessNN"
      },
      "outputs": [],
      "source": [
        "# Load all ranks df to Big Query\n",
        "load_dataset_name = 'analysis_layer'\n",
        "load_table_name = 'ratings_model_ranks'\n",
        "full_table_name = f\"{load_dataset_name}.{load_table_name}\"\n",
        "pandas_gbq.to_gbq(all_ranks_df, full_table_name, project_id='birkbeck-msc-project-422917', if_exists='replace')\n",
        "\n",
        "# Load confusion matrix df to Big Query\n",
        "load_dataset_name = 'analysis_layer'\n",
        "load_table_name = 'ratings_model_confusion_matrix_results'\n",
        "full_table_name = f\"{load_dataset_name}.{load_table_name}\"\n",
        "pandas_gbq.to_gbq(all_results_df, full_table_name, project_id='birkbeck-msc-project-422917', if_exists='replace')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Rank Aggregation functions called after ratings function have run and results inserted into Big Query\n",
        "\n",
        "def borda_count_aggregation (fold_counter):\n",
        "\n",
        "    from google.cloud import bigquery\n",
        "\n",
        "    # Authenticate using the loaded key\n",
        "    credentials = service_account.Credentials.from_service_account_info(key)\n",
        "    # Set up the BigQuery client with the credentials to project\n",
        "    client = bigquery.Client(credentials=credentials, project='birkbeck-msc-project-422917')\n",
        "\n",
        "    fold_number = fold_counter\n",
        "    all_ranks = \"\"\"\n",
        "              select * from analysis_layer.ratings_model_ranks\n",
        "              \"\"\"\n",
        "\n",
        "    all_ranks_df = client.query(all_ranks).to_dataframe()\n",
        "    all_ranks_df_fold = all_ranks_df[all_ranks_df['fold_number'] == fold_number].sort_values('team_no', ascending=True)\n",
        "    all_ranks_df_fold['rank_total'] = all_ranks_df_fold[['basic_position','massey','colley','keener','trueskill']].sum(axis=1)\n",
        "    all_ranks_df_fold['overall_rank'] = all_ranks_df_fold['rank_total'].rank(method='first', ascending=True).astype(int)\n",
        "    all_ranks_df_fold['rating'] = 1 / all_ranks_df_fold['overall_rank']\n",
        "\n",
        "\n",
        "    # Create a list of tuples in the format (team_no, rating) for function\n",
        "    ratings = list(zip(all_ranks_df_fold['team_no'], all_ranks_df_fold['rating']))\n",
        "\n",
        "    return ratings\n",
        "\n",
        "def average_rank_aggregation (fold_counter):\n",
        "\n",
        "    from google.cloud import bigquery\n",
        "\n",
        "    # Authenticate using the loaded key\n",
        "    credentials = service_account.Credentials.from_service_account_info(key)\n",
        "    # Set up the BigQuery client with the credentials to project\n",
        "    client = bigquery.Client(credentials=credentials, project='birkbeck-msc-project-422917')\n",
        "\n",
        "    fold_number = fold_counter\n",
        "    all_ranks = \"\"\"\n",
        "              select * from analysis_layer.ratings_model_ranks\n",
        "              \"\"\"\n",
        "\n",
        "    all_ranks_df = client.query(all_ranks).to_dataframe()\n",
        "    all_ranks_df_fold = all_ranks_df[all_ranks_df['fold_number'] == fold_number].sort_values('team_no', ascending=True)\n",
        "    all_ranks_df_fold['rank_average'] = all_ranks_df_fold[['basic_position','massey','colley','keener','trueskill']].mean(axis=1)\n",
        "    all_ranks_df_fold['overall_rank'] = all_ranks_df_fold['rank_average'].rank(method='first', ascending=True).astype(int)\n",
        "    all_ranks_df_fold['rating'] = 1 / all_ranks_df_fold['overall_rank']\n",
        "    display(all_ranks_df_fold)\n",
        "\n",
        "\n",
        "    # Create a list of tuples in the format (team_no, rating) for function\n",
        "    ratings = list(zip(all_ranks_df_fold['team_no'], all_ranks_df_fold['rating']))\n",
        "\n",
        "    return ratings\n"
      ],
      "metadata": {
        "id": "5wHYox1G34XH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Borda Count Aggregation\n",
        "ratings_model = 'borda_count'\n",
        "ratings_function = borda_count_aggregation\n",
        "num_folds = 5\n",
        "\n",
        "# Call ratings function and run cross validation\n",
        "(ratings_df,test_predictions_df, all_y_train_true, all_y_train_pred, all_y_test_true, all_y_test_pred,\n",
        " fold_train_accuracies, fold_train_recalls, fold_test_accuracies, fold_test_recalls,\n",
        " all_y_train_pred_proba, all_y_test_pred_proba, ratings_model) = run_rating_models_cross_validation (fa_cup_scores_df, ratings_function, ratings_model, num_folds)\n",
        "\n",
        "\n",
        "# Display results\n",
        "test_predictions_df.index = range(1, len(test_predictions_df) + 1)\n",
        "display(test_predictions_df)\n",
        "\n",
        "ratings_df.index = range(1, len(ratings_df) + 1)\n",
        "display(ratings_df)\n",
        "\n",
        "# Load predictions and ratinbgs to Google Big Query\n",
        "#load_predictions_and_ratings_to_bigquery(client, test_predictions_df, ratings_df, ratings_model)\n",
        "\n",
        "# Create dataframe to compare rank and use for final rank aggregation\n",
        "rank_df = ratings_df[['team_no', 'fold_number', 'rank']]\n",
        "rank_df = rank_df.rename(columns={'rank': ratings_model})\n",
        "all_ranks_df = all_ranks_df.merge(rank_df, on=['team_no','fold_number'], how='outer')\n",
        "display(all_ranks_df)\n",
        "\n",
        "# Create confusion matrix from results\n",
        "model_name_ranking = ratings_model\n",
        "results_df, cm_fig, roc_fig = create_model_results_df(all_y_train_true, all_y_train_pred, all_y_test_true, all_y_test_pred, fold_train_accuracies, fold_train_recalls, fold_test_accuracies, fold_test_recalls, all_y_train_pred_proba, all_y_test_pred_proba, model_name_ranking)\n",
        "\n",
        "results_df = results_df.reset_index()\n",
        "results_df['metric_id'] = results_df.index + 1\n",
        "results_df = results_df[['metric_id', 'metric', ratings_model]]\n",
        "\n",
        "all_results_df = all_results_df.merge(results_df, on=['metric_id', 'metric'], how='outer')\n",
        "display(all_results_df)"
      ],
      "metadata": {
        "id": "NhwPdT5QfVKq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Average Rank Aggregation\n",
        "ratings_model = 'average_rank'\n",
        "ratings_function = average_rank_aggregation\n",
        "num_folds = 5\n",
        "\n",
        "# Call ratings function and run cross validation\n",
        "(ratings_df,test_predictions_df, all_y_train_true, all_y_train_pred, all_y_test_true, all_y_test_pred,\n",
        " fold_train_accuracies, fold_train_recalls, fold_test_accuracies, fold_test_recalls,\n",
        " all_y_train_pred_proba, all_y_test_pred_proba, ratings_model) = run_rating_models_cross_validation (fa_cup_scores_df, ratings_function, ratings_model, num_folds)\n",
        "\n",
        "\n",
        "# Display results\n",
        "test_predictions_df.index = range(1, len(test_predictions_df) + 1)\n",
        "display(test_predictions_df)\n",
        "\n",
        "ratings_df.index = range(1, len(ratings_df) + 1)\n",
        "display(ratings_df)\n",
        "\n",
        "# Load predictions and ratinbgs to Google Big Query\n",
        "#load_predictions_and_ratings_to_bigquery(client, test_predictions_df, ratings_df, ratings_model)\n",
        "\n",
        "# Create dataframe to compare rank and use for final rank aggregation\n",
        "rank_df = ratings_df[['team_no', 'fold_number', 'rank']]\n",
        "rank_df = rank_df.rename(columns={'rank': ratings_model})\n",
        "all_ranks_df = all_ranks_df.merge(rank_df, on=['team_no','fold_number'], how='outer')\n",
        "display(all_ranks_df)\n",
        "\n",
        "# Create confusion matrix from results\n",
        "model_name_ranking = ratings_model\n",
        "results_df, cm_fig, roc_fig = create_model_results_df(all_y_train_true, all_y_train_pred, all_y_test_true, all_y_test_pred, fold_train_accuracies, fold_train_recalls, fold_test_accuracies, fold_test_recalls, all_y_train_pred_proba, all_y_test_pred_proba, model_name_ranking)\n",
        "\n",
        "results_df = results_df.reset_index()\n",
        "results_df['metric_id'] = results_df.index + 1\n",
        "results_df = results_df[['metric_id', 'metric', ratings_model]]\n",
        "\n",
        "all_results_df = all_results_df.merge(results_df, on=['metric_id', 'metric'], how='outer')\n",
        "display(all_results_df)"
      ],
      "metadata": {
        "id": "utXVU5-gVh-Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load updated all ranks df to Big Query so Borda Count can be subsequently queried in local kemeny optimisation\n",
        "load_dataset_name = 'analysis_layer'\n",
        "load_table_name = 'ratings_model_ranks'\n",
        "full_table_name = f\"{load_dataset_name}.{load_table_name}\"\n",
        "pandas_gbq.to_gbq(all_ranks_df, full_table_name, project_id='birkbeck-msc-project-422917', if_exists='replace')"
      ],
      "metadata": {
        "id": "LtmLSzRA5Cuh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Local Kemeny Optimisation\n",
        "\n",
        "def local_kemeny_optimisation (fold_number, max_iterations=10):\n",
        "\n",
        "    def kendall_tau_distance(ranking1, ranking2):\n",
        "        distance = 0\n",
        "        for i in range(len(ranking1)):\n",
        "            for j in range(i + 1, len(ranking1)):\n",
        "                if (ranking1[i] < ranking1[j]) != (ranking2[i] < ranking2[j]):\n",
        "                    distance += 1\n",
        "        return distance\n",
        "\n",
        "    def total_kendall_tau_distance(candidate, rankings):\n",
        "        return sum(kendall_tau_distance(candidate, ranking) for ranking in rankings.values())\n",
        "\n",
        "    def local_kemeny_optimisation(rankings, initial_ranking, max_iterations):\n",
        "        n_items = len(initial_ranking)\n",
        "\n",
        "        current_ranking = initial_ranking.copy()\n",
        "        current_distance = total_kendall_tau_distance(current_ranking, rankings)\n",
        "\n",
        "        for iteration in range(max_iterations):\n",
        "            improved = False\n",
        "            for i in range(n_items - 1):\n",
        "                new_ranking = current_ranking.copy()\n",
        "                new_ranking[i], new_ranking[i+1] = new_ranking[i+1], new_ranking[i]\n",
        "                new_distance = total_kendall_tau_distance(new_ranking, rankings)\n",
        "\n",
        "                if new_distance < current_distance:\n",
        "                    current_ranking = new_ranking\n",
        "                    current_distance = new_distance\n",
        "                    improved = True\n",
        "                    print(f\"Iteration {iteration + 1}: Improved distance to {current_distance}\")\n",
        "                    break\n",
        "\n",
        "            if not improved:\n",
        "                print(f\"Stopped after {iteration + 1} iterations: No improvement\")\n",
        "                break\n",
        "\n",
        "        if iteration == max_iterations - 1:\n",
        "            print(f\"Stopped after reaching maximum iterations ({max_iterations})\")\n",
        "\n",
        "        return current_ranking\n",
        "\n",
        "    # Authenticate using the loaded key\n",
        "    #credentials = service_account.Credentials.from_service_account_info(key)\n",
        "    # Set up the BigQuery client with the credentials to project\n",
        "    client = bigquery.Client(credentials=credentials, project='birkbeck-msc-project-422917')\n",
        "\n",
        "    # Query to get the data\n",
        "    all_ranks_query = f\"\"\"\n",
        "        SELECT * FROM analysis_layer.ratings_model_ranks WHERE fold_number = {fold_number}\n",
        "    \"\"\"\n",
        "\n",
        "    all_ranks_df = client.query(all_ranks_query).to_dataframe()\n",
        "\n",
        "    # Create dictionary of rankings\n",
        "    rankings = {\n",
        "        'basic_position': all_ranks_df['basic_position'].tolist(),\n",
        "        'massey': all_ranks_df['massey'].tolist(),\n",
        "        'colley': all_ranks_df['colley'].tolist(),\n",
        "        'keener': all_ranks_df['keener'].tolist(),\n",
        "        'trueskill': all_ranks_df['trueskill'].tolist()\n",
        "    }\n",
        "\n",
        "    # Use Borda count as initial ranking\n",
        "    borda_count = all_ranks_df['borda_count'].tolist()\n",
        "\n",
        "    # Perform local Kemeny optimization\n",
        "    optimized_ranking = local_kemeny_optimisation(rankings, borda_count, max_iterations)\n",
        "    print(\"\\nTotal Kendall tau distance (Borda Count):\", total_kendall_tau_distance(borda_count, rankings))\n",
        "    print(\"Total Kendall tau distance (Optimized):\", total_kendall_tau_distance(optimized_ranking, rankings))\n",
        "\n",
        "    # Show which teams changed positions\n",
        "    team_nos = all_ranks_df['team_no'].tolist()\n",
        "\n",
        "    # Create a mapping from team number to rank for both rankings\n",
        "    borda_mapping = {team: rank for rank, team in enumerate(borda_count)}\n",
        "    optimized_mapping = {team: rank for rank, team in enumerate(optimized_ranking)}\n",
        "\n",
        "    changes = []\n",
        "    for team in team_nos:\n",
        "        borda_rank = borda_mapping.get(team, -1)\n",
        "        optimized_rank = optimized_mapping.get(team, -1)\n",
        "        if borda_rank != optimized_rank:\n",
        "            changes.append((team, borda_rank, optimized_rank))\n",
        "\n",
        "    print(\"\\nTeams that changed positions (Team, Old Position, New Position):\")\n",
        "    for change in changes:\n",
        "        print(f\"Team {change[0]}: {change[1]} -> {change[2]}\")\n",
        "\n",
        "    # Create a DataFrame with the optimized ranking\n",
        "    optimized_df = pd.DataFrame({\n",
        "        'team_no': team_nos,\n",
        "        'overall_rank': optimized_ranking\n",
        "    })\n",
        "\n",
        "    # Calculate the rating based on the optimized ranking\n",
        "    optimized_df['rating'] = 1 / optimized_df['overall_rank']\n",
        "\n",
        "    # Create a list of tuples in the format (team_no, rating) for function\n",
        "    ratings = list(zip(optimized_df['team_no'], optimized_df['rating']))\n",
        "\n",
        "    return ratings\n"
      ],
      "metadata": {
        "id": "ibBrmuaX163V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Local Kemeny Optimisation\n",
        "\n",
        "ratings_model = 'local_kemeny_optimisation'\n",
        "ratings_function = local_kemeny_optimisation\n",
        "num_folds = 5\n",
        "\n",
        "# Call ratings function and run cross validation\n",
        "(ratings_df,test_predictions_df, all_y_train_true, all_y_train_pred, all_y_test_true, all_y_test_pred,\n",
        " fold_train_accuracies, fold_train_recalls, fold_test_accuracies, fold_test_recalls,\n",
        " all_y_train_pred_proba, all_y_test_pred_proba, ratings_model) = run_rating_models_cross_validation (fa_cup_scores_df, ratings_function, ratings_model, num_folds)\n",
        "\n",
        "\n",
        "# Display results\n",
        "test_predictions_df.index = range(1, len(test_predictions_df) + 1)\n",
        "display(test_predictions_df)\n",
        "\n",
        "ratings_df.index = range(1, len(ratings_df) + 1)\n",
        "display(ratings_df)\n",
        "\n",
        "# Load predictions and ratinbgs to Google Big Query\n",
        "#load_predictions_and_ratings_to_bigquery(client, test_predictions_df, ratings_df, ratings_model)\n",
        "\n",
        "# Create dataframe to compare rank and use for final rank aggregation\n",
        "rank_df = ratings_df[['team_no', 'fold_number', 'rank']]\n",
        "rank_df = rank_df.rename(columns={'rank': ratings_model})\n",
        "all_ranks_df = all_ranks_df.merge(rank_df, on=['team_no','fold_number'], how='outer')\n",
        "display(all_ranks_df)\n",
        "\n",
        "# Create confusion matrix from results\n",
        "model_name_ranking = ratings_model\n",
        "results_df, cm_fig, roc_fig = create_model_results_df(all_y_train_true, all_y_train_pred, all_y_test_true, all_y_test_pred, fold_train_accuracies, fold_train_recalls, fold_test_accuracies, fold_test_recalls, all_y_train_pred_proba, all_y_test_pred_proba, model_name_ranking)\n",
        "\n",
        "results_df = results_df.reset_index()\n",
        "results_df['metric_id'] = results_df.index + 1\n",
        "results_df = results_df[['metric_id', 'metric', ratings_model]]\n",
        "\n",
        "all_results_df = all_results_df.merge(results_df, on=['metric_id', 'metric'], how='outer')\n",
        "display(all_results_df)"
      ],
      "metadata": {
        "id": "-l44e7fi5y4n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load final ranks including Local Kemeny Optimisation\n",
        "load_dataset_name = 'analysis_layer'\n",
        "load_table_name = 'ratings_model_ranks'\n",
        "full_table_name = f\"{load_dataset_name}.{load_table_name}\"\n",
        "pandas_gbq.to_gbq(all_ranks_df, full_table_name, project_id='birkbeck-msc-project-422917', if_exists='replace')"
      ],
      "metadata": {
        "id": "wdaot3i69jpv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Change default ouput directory\n",
        "os.chdir('/content/drive/MyDrive/birkbeck_msc-project/output_files')\n",
        "# Save ranks to excel\n",
        "all_results_df.to_excel(\"all_ranking_results.xlsx\")"
      ],
      "metadata": {
        "id": "P1KPQ9Q8zE7s"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "https://github.com/chorltonm/fa-cup-upsets/blob/main/notebooks/models/rating_models.ipynb",
      "authorship_tag": "ABX9TyMK1DAbkPZ69rZ4Tq/5bUXt",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}