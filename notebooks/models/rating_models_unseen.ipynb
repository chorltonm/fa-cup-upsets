{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/chorltonm/fa-cup-upsets/blob/main/notebooks/models/rating_models_unseen.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nEuX80dPfnRR"
      },
      "outputs": [],
      "source": [
        "# Import Libaries\n",
        "import os\n",
        "import sys\n",
        "import json\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import importlib\n",
        "import pandas_gbq\n",
        "\n",
        "from google.cloud import bigquery\n",
        "from google.oauth2 import service_account\n",
        "from google.colab import drive\n",
        "from google.colab import userdata\n",
        "\n",
        "# Install scikit learn\n",
        "from sklearn.model_selection import StratifiedShuffleSplit, StratifiedKFold\n",
        "from sklearn.metrics import accuracy_score, recall_score, confusion_matrix, classification_report\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install Trueskill library\n",
        "!pip install trueskill\n",
        "from trueskill import Rating, rate_1vs1"
      ],
      "metadata": {
        "id": "Dak7ocxQSN8m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gl-LEV7vfuVs"
      },
      "outputs": [],
      "source": [
        "# Mount Google Drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "# Change default ouput directory\n",
        "os.chdir('/content/drive/MyDrive/birkbeck_msc-project/output_files')\n",
        "\n",
        "# Change default ouput directory\n",
        "os.chdir('/content/drive/MyDrive/birkbeck_msc-project/python_files')\n",
        "\n",
        "# Get the current working directory\n",
        "current_directory = os.getcwd()\n",
        "\n",
        "# Print the current working directory\n",
        "print(\"Current working directory:\", current_directory)\n",
        "\n",
        "# List the contents of the directory\n",
        "contents = os.listdir(current_directory)\n",
        "\n",
        "# Print the contents\n",
        "print(\"Contents of the directory:\", contents)\n",
        "\n",
        "# Import user defined python functions. Used importlib as having stability issues with simple import and not picking up the files\n",
        "\n",
        "spec = importlib.util.spec_from_file_location(\"create_model_results\", \"/content/drive/MyDrive/birkbeck_msc-project/python_files/create_model_results.py\")\n",
        "create_model_results = importlib.util.module_from_spec(spec)\n",
        "spec.loader.exec_module(create_model_results)\n",
        "\n",
        "#from create_model_results import create_model_results_df\n",
        "\n",
        "\n",
        "spec = importlib.util.spec_from_file_location(\"ratings_functions\", \"/content/drive/MyDrive/birkbeck_msc-project/python_files/ratings_functions.py\")\n",
        "ratings_functions = importlib.util.module_from_spec(spec)\n",
        "spec.loader.exec_module(ratings_functions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VeWnIKHFi_uN"
      },
      "outputs": [],
      "source": [
        "# Authentication credentials and keys\n",
        "# Google Service Account\n",
        "# Load the JSON key from local Google Collab file\n",
        "key = json.load(open('/content/drive/MyDrive/service_account.json', 'r'))\n",
        "\n",
        "# Authenticate using the loaded key\n",
        "credentials = service_account.Credentials.from_service_account_info(key)\n",
        "\n",
        "# Set up the BigQuery client with the credentials to project\n",
        "client = bigquery.Client(credentials=credentials, project='birkbeck-msc-project-422917')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def load_predictions_and_ratings_to_bigquery(client, predictions_df, fold_ratings_df, ratings_model):\n",
        "\n",
        "  # Load predictions to Big Query\n",
        "  # Specify the target table\n",
        "  load_dataset_name = 'analysis_layer'\n",
        "  load_table_name = 'ratings_predictions_unseen'\n",
        "  load_table_ref = f\"{load_dataset_name}.{load_table_name}\"\n",
        "\n",
        "  # Delete predicitons method already inserted\n",
        "  delete_query = f\"DELETE FROM `{load_dataset_name}.{load_table_name}` WHERE ratings_model = '{ratings_model}'\"\n",
        "  delete_job = client.query(delete_query)\n",
        "  delete_result = delete_job.result()\n",
        "  predictions_total_rows_deleted = delete_result.num_dml_affected_rows\n",
        "  print(f\"Big Query target predictions table {load_dataset_name}.{load_table_name} rows deleted: {predictions_total_rows_deleted}\")\n",
        "\n",
        "  # Insert predictions for ratings model data to the existing table\n",
        "  job_config = bigquery.LoadJobConfig(\n",
        "          write_disposition=\"WRITE_APPEND\")\n",
        "  load_job = client.load_table_from_dataframe(\n",
        "          predictions_df, load_table_ref, job_config=job_config)\n",
        "  # Wait for the job to complete\n",
        "  load_job.result()\n",
        "  predictions_num_rows_inserted = load_job.output_rows\n",
        "  print(f\"{predictions_num_rows_inserted} rows appended to predictions table {load_table_ref} successfully.\")\n",
        "\n",
        "  # Load fold ratings to Big Query\n",
        "\n",
        "  # Specify the target table\n",
        "  load_dataset_name = 'analysis_layer'\n",
        "  load_table_name = 'ratings_unseen'\n",
        "  load_table_ref = f\"{load_dataset_name}.{load_table_name}\"\n",
        "\n",
        "  # Delete ratings method already inserted\n",
        "  delete_query = f\"DELETE FROM `{load_dataset_name}.{load_table_name}` WHERE ratings_model = '{ratings_model}'\"\n",
        "  delete_job = client.query(delete_query)\n",
        "  delete_result = delete_job.result()\n",
        "  ratings_total_rows_deleted = delete_result.num_dml_affected_rows\n",
        "  print(f\"Big Query target ratings table {load_dataset_name}.{load_table_name} rows deleted: {ratings_total_rows_deleted}\")\n",
        "\n",
        "\n",
        "  # Insert ratings for ratings model data to the existing table\n",
        "  job_config = bigquery.LoadJobConfig(\n",
        "          write_disposition=\"WRITE_APPEND\")\n",
        "  load_job = client.load_table_from_dataframe(\n",
        "          ratings_df, load_table_ref, job_config=job_config)\n",
        "\n",
        "  load_job.result()  # Wait for the job to complete\n",
        "\n",
        "  ratings_num_rows_inserted = load_job.output_rows\n",
        "  print(f\"{ratings_num_rows_inserted} rows appended to ratings table {load_table_ref} successfully.\")\n",
        "\n",
        "  return\n"
      ],
      "metadata": {
        "id": "IJkpPc-4oBLF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_winner_upset (home_team_no, away_team_no, home_team_league_level, away_team_league_level, ranked_ratings):\n",
        "    home_team_info = next((info for info in ranked_ratings if info[0] == home_team_no), None)\n",
        "    away_team_info = next((info for info in ranked_ratings if info[0] == away_team_no), None)\n",
        "\n",
        "    if home_team_info is None or away_team_info is None:\n",
        "        raise ValueError(\"Team number not found in ratings list.\")\n",
        "\n",
        "    home_team_rating, home_team_rank = home_team_info[1], home_team_info[2]\n",
        "    away_team_rating, away_team_rank = away_team_info[1], away_team_info[2]\n",
        "\n",
        "    # Calculate the basic probability of the higer ranked team winning based proportionally as part of the total rank. Calculation reflects the fact lowest number is higher rank\n",
        "\n",
        "    home_team_upset_probability = 1 - (home_team_rank / (home_team_rank + away_team_rank))\n",
        "    away_team_upset_probability = 1 - (away_team_rank / (home_team_rank + away_team_rank))\n",
        "\n",
        "\n",
        "    # Determine the predicted winner and if it's an upset\n",
        "    if home_team_rating > away_team_rating:\n",
        "        predicted_winner = home_team_no\n",
        "        predicted_upset = 1 if home_team_league_level > away_team_league_level else 0\n",
        "    else:\n",
        "        predicted_winner = away_team_no\n",
        "        predicted_upset = 1 if away_team_league_level > home_team_league_level else 0\n",
        "\n",
        "    # Probaility of upset based on league level and rank differnce\n",
        "    if home_team_league_level > away_team_league_level:\n",
        "       upset_probability = home_team_upset_probability\n",
        "    else:\n",
        "       upset_probability = away_team_upset_probability\n",
        "\n",
        "    return predicted_winner, predicted_upset, home_team_rating, away_team_rating, home_team_rank, away_team_rank, upset_probability"
      ],
      "metadata": {
        "id": "RQhWk7aqpAw-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run_rating_models_single_split (train_data, test_data, ratings_function, ratings_model):\n",
        "    # Initialize lists to store results\n",
        "    train_accuracies = []\n",
        "    train_recalls = []\n",
        "    test_accuracies = []\n",
        "    test_recalls = []\n",
        "    all_y_train_true = []\n",
        "    all_y_train_pred = []\n",
        "    all_y_train_pred_proba = []\n",
        "    all_y_test_true = []\n",
        "    all_y_test_pred = []\n",
        "    all_y_test_pred_proba = []\n",
        "    ratings = []\n",
        "    test_predictions = []\n",
        "\n",
        "    # Call rating function\n",
        "    if ratings_model in ['borda_count', 'average_rank', 'local_kemeny_optimisation']:\n",
        "        model_ratings = ratings_function(1)  # Use 1 as a placeholder for fold and used when testign in unseen and single split\n",
        "    else:\n",
        "        model_ratings = ratings_function(train_data)\n",
        "\n",
        "    # Add ranks based on the sorted order\n",
        "    sorted_ratings = sorted(model_ratings, key=lambda x: x[1], reverse=True)\n",
        "    ranked_ratings = [(team_no, rating, index + 1) for index, (team_no, rating) in enumerate(sorted_ratings)]\n",
        "\n",
        "    # Append the ratings to the list\n",
        "    ratings_list = [(ratings_model, 1, team_no, rating) for team_no, rating in model_ratings]\n",
        "    ratings.extend(ratings_list)\n",
        "\n",
        "    # Predict upsets for train data\n",
        "    train_actual_upsets = []\n",
        "    train_predicted_upsets = []\n",
        "    train_upset_probabilities = []\n",
        "\n",
        "    for _, row in train_data.iterrows():\n",
        "        predicted_winner, predicted_upset, home_team_rating, away_team_rating, home_team_rank, away_team_rank, upset_probability = predict_winner_upset(\n",
        "            row['home_team_no'], row['away_team_no'],\n",
        "            row['home_team_league_level'], row['away_team_league_level'],\n",
        "            ranked_ratings\n",
        "        )\n",
        "        actual_upset = row['actual_upset']\n",
        "        train_actual_upsets.append(actual_upset)\n",
        "        train_predicted_upsets.append(predicted_upset)\n",
        "        train_upset_probabilities.append(upset_probability)\n",
        "\n",
        "    # Calculate train metrics\n",
        "    train_accuracy = accuracy_score(train_actual_upsets, train_predicted_upsets)\n",
        "    train_recall = recall_score(train_actual_upsets, train_predicted_upsets)\n",
        "    train_accuracies.append(train_accuracy)\n",
        "    train_recalls.append(train_recall)\n",
        "    all_y_train_true.extend(train_actual_upsets)\n",
        "    all_y_train_pred.extend(train_predicted_upsets)\n",
        "    all_y_train_pred_proba.extend(train_upset_probabilities)\n",
        "\n",
        "    # Predict upsets for test data\n",
        "    test_actual_upsets = []\n",
        "    test_predicted_upsets = []\n",
        "    test_upset_probabilities = []\n",
        "\n",
        "    for _, row in test_data.iterrows():\n",
        "        predicted_winner, predicted_upset, home_team_rating, away_team_rating, home_team_rank, away_team_rank, upset_probability = predict_winner_upset(\n",
        "            row['home_team_no'], row['away_team_no'],\n",
        "            row['home_team_league_level'], row['away_team_league_level'],\n",
        "            ranked_ratings\n",
        "        )\n",
        "        actual_upset = row['actual_upset']\n",
        "        test_actual_upsets.append(actual_upset)\n",
        "        test_predicted_upsets.append(predicted_upset)\n",
        "        test_upset_probabilities.append(upset_probability)\n",
        "\n",
        "        test_predictions.append({\n",
        "            'ratings_model': ratings_model,\n",
        "            'match_id': row['match_id'],\n",
        "            'home_team_no': row['home_team_no'],\n",
        "            'home_team_league_level': row['home_team_league_level'],\n",
        "            'away_team_no': row['away_team_no'],\n",
        "            'away_team_league_level': row['away_team_league_level'],\n",
        "            'home_team_rating': home_team_rating,\n",
        "            'away_team_rating': away_team_rating,\n",
        "            'home_team_rank': home_team_rank,\n",
        "            'away_team_rank': away_team_rank,\n",
        "            'predicted_winner': predicted_winner,\n",
        "            'actual_winner': row['actual_winning_team_no'],\n",
        "            'actual_upset': actual_upset,\n",
        "            'predicted_upset': predicted_upset,\n",
        "            'upset_probability': upset_probability,\n",
        "        })\n",
        "\n",
        "    # Calculate test metrics\n",
        "    test_accuracy = accuracy_score(test_actual_upsets, test_predicted_upsets)\n",
        "    test_recall = recall_score(test_actual_upsets, test_predicted_upsets)\n",
        "    test_accuracies.append(test_accuracy)\n",
        "    test_recalls.append(test_recall)\n",
        "    all_y_test_true.extend(test_actual_upsets)\n",
        "    all_y_test_pred.extend(test_predicted_upsets)\n",
        "    all_y_test_pred_proba.extend(test_upset_probabilities)\n",
        "\n",
        "    print(f\"Train Accuracy: {train_accuracy:.3f}, Train Recall: {train_recall:.3f}\")\n",
        "    print(f\"Test Accuracy: {test_accuracy:.3f}, Test Recall: {test_recall:.3f}\")\n",
        "\n",
        "    test_predictions_df = pd.DataFrame(test_predictions)\n",
        "\n",
        "    # Create ratings DataFrame\n",
        "    ratings_df = pd.DataFrame(ratings, columns=['ratings_model', 'fold_number', 'team_no', 'rating'])\n",
        "    ratings_df = ratings_df.sort_values(['team_no', 'fold_number'])\n",
        "    ratings_df['rank'] = ratings_df.groupby('fold_number')['rating'].rank(ascending=False, method='dense').astype(int)\n",
        "\n",
        "    return ratings_df, test_predictions_df, all_y_train_true, all_y_train_pred, all_y_test_true, all_y_test_pred, train_accuracies, train_recalls, test_accuracies, test_recalls, all_y_train_pred_proba, all_y_test_pred_proba, ratings_model"
      ],
      "metadata": {
        "id": "m98qpwQO54v1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Results & Rank Dataframes\n",
        "all_ranks_df = pd.DataFrame(columns=['team_no','fold_number'])\n",
        "display(all_ranks_df)\n",
        "\n",
        "all_results_df = pd.DataFrame(columns=['metric_id', 'metric'])\n",
        "display(all_results_df)\n",
        "\n",
        "test_flag = 'Unseen'"
      ],
      "metadata": {
        "id": "W20pKD_QwgrQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train and unseen data\n",
        "\n",
        "fa_cup_scores_all = \"\"\"\n",
        "    SELECT * FROM preparation_layer.view_fa_cup_scores ORDER BY sort_order ASC\n",
        "\"\"\"\n",
        "\n",
        "fa_cup_scores_all_df = client.query(fa_cup_scores_all).to_dataframe()\n",
        "display(fa_cup_scores_all_df)\n",
        "\n",
        "\n",
        "fa_cup_scores__all_train_df = fa_cup_scores_all_df[~fa_cup_scores_all_df['season_year'].isin(['21/22', '22/23'])]\n",
        "fa_cup_scores_test_all_unseen_df = fa_cup_scores_all_df[fa_cup_scores_all_df['season_year'].isin(['21/22', '22/23'])]"
      ],
      "metadata": {
        "id": "R3KHTd0X5Wq0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Basic Position Ratings\n",
        "ratings_model = 'basic_position'\n",
        "ratings_function = ratings_functions.basic_position_ratings\n",
        "test_flag = 'Unseen'\n",
        "\n",
        "# Call ratings function and train test split\n",
        "(ratings_df, test_predictions_df, all_y_train_true, all_y_train_pred, all_y_test_true, all_y_test_pred,\n",
        " train_accuracies, train_recalls, test_accuracies, test_recalls,\n",
        " all_y_train_pred_proba, all_y_test_pred_proba, ratings_model) =  run_rating_models_single_split (fa_cup_scores__all_train_df, fa_cup_scores_test_all_unseen_df, ratings_function, ratings_model)\n",
        "\n",
        "\n",
        "# Display results\n",
        "test_predictions_df.index = range(1, len(test_predictions_df) + 1)\n",
        "test_predictions_df_basic_position = test_predictions_df.index = range(1, len(test_predictions_df) + 1)\n",
        "display(test_predictions_df_basic_position)\n",
        "\n",
        "ratings_df.index = range(1, len(ratings_df) + 1)\n",
        "display(ratings_df)\n",
        "\n",
        "# Load predictions and ratinbgs to Google Big Query\n",
        "load_predictions_and_ratings_to_bigquery(client, test_predictions_df, ratings_df, ratings_model)\n",
        "\n",
        "# Create dataframe to compare rank and use for final rank aggregation\n",
        "rank_df = ratings_df[['team_no', 'fold_number', 'rank']]\n",
        "rank_df = rank_df.rename(columns={'rank': ratings_model})\n",
        "all_ranks_df = all_ranks_df.merge(rank_df, on=['team_no','fold_number'], how='outer')\n",
        "display(all_ranks_df)\n",
        "\n",
        "# Create confusion matrix from results\n",
        "model_name_ranking = ratings_model\n",
        "results_df, cm_fig, roc_fig = create_model_results.create_model_results_df (all_y_train_true, all_y_train_pred, all_y_test_true, all_y_test_pred, train_accuracies, train_recalls, test_accuracies, test_recalls, all_y_train_pred_proba, all_y_test_pred_proba, model_name_ranking, test_flag)\n",
        "\n",
        "results_df = results_df.reset_index()\n",
        "results_df['metric_id'] = results_df.index + 1\n",
        "results_df = results_df[['metric_id', 'metric', ratings_model]]\n",
        "\n",
        "all_results_df = all_results_df.merge(results_df, on=['metric_id', 'metric'], how='outer')\n",
        "display(all_results_df)\n",
        "\n"
      ],
      "metadata": {
        "id": "OK204TqPmAsc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Massey Ratings\n",
        "ratings_model = 'massey'\n",
        "ratings_function = ratings_functions.massey_ratings\n",
        "\n",
        "# Call ratings function and train test split\n",
        "(ratings_df, test_predictions_df, all_y_train_true, all_y_train_pred, all_y_test_true, all_y_test_pred,\n",
        " train_accuracies, train_recalls, test_accuracies, test_recalls,\n",
        " all_y_train_pred_proba, all_y_test_pred_proba, ratings_model) =  run_rating_models_single_split (fa_cup_scores__all_train_df, fa_cup_scores_test_all_unseen_df, ratings_function, ratings_model)\n",
        "\n",
        "\n",
        "# Display results\n",
        "test_predictions_df.index = range(1, len(test_predictions_df) + 1)\n",
        "test_predictions_df_massey = test_predictions_df.index = range(1, len(test_predictions_df) + 1)\n",
        "display(test_predictions_df_massey)\n",
        "\n",
        "ratings_df.index = range(1, len(ratings_df) + 1)\n",
        "display(ratings_df)\n",
        "\n",
        "# Load predictions and ratinbgs to Google Big Query\n",
        "load_predictions_and_ratings_to_bigquery(client, test_predictions_df, ratings_df, ratings_model)\n",
        "\n",
        "# Create dataframe to compare rank and use for final rank aggregation\n",
        "rank_df = ratings_df[['team_no', 'fold_number', 'rank']]\n",
        "rank_df = rank_df.rename(columns={'rank': ratings_model})\n",
        "all_ranks_df = all_ranks_df.merge(rank_df, on=['team_no','fold_number'], how='outer')\n",
        "display(all_ranks_df)\n",
        "\n",
        "# Create confusion matrix from results\n",
        "model_name_ranking = ratings_model\n",
        "results_df, cm_fig, roc_fig = create_model_results.create_model_results_df (all_y_train_true, all_y_train_pred, all_y_test_true, all_y_test_pred, train_accuracies, train_recalls, test_accuracies, test_recalls, all_y_train_pred_proba, all_y_test_pred_proba, model_name_ranking, test_flag)\n",
        "\n",
        "results_df = results_df.reset_index()\n",
        "results_df['metric_id'] = results_df.index + 1\n",
        "results_df = results_df[['metric_id', 'metric', ratings_model]]\n",
        "\n",
        "all_results_df = all_results_df.merge(results_df, on=['metric_id', 'metric'], how='outer')\n",
        "display(all_results_df)"
      ],
      "metadata": {
        "id": "PnW1lIkvYMo_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Colley Ratings\n",
        "ratings_model = 'colley'\n",
        "ratings_function = ratings_functions.colley_ratings\n",
        "\n",
        "# Call ratings function and train test split\n",
        "(ratings_df, test_predictions_df, all_y_train_true, all_y_train_pred, all_y_test_true, all_y_test_pred,\n",
        " train_accuracies, train_recalls, test_accuracies, test_recalls,\n",
        " all_y_train_pred_proba, all_y_test_pred_proba, ratings_model) =  run_rating_models_single_split (fa_cup_scores__all_train_df, fa_cup_scores_test_all_unseen_df, ratings_function, ratings_model)\n",
        "\n",
        "# Display results\n",
        "test_predictions_df.index = range(1, len(test_predictions_df) + 1)\n",
        "test_predictions_df_colley = test_predictions_df.index = range(1, len(test_predictions_df) + 1)\n",
        "display(test_predictions_df_colley)\n",
        "\n",
        "ratings_df.index = range(1, len(ratings_df) + 1)\n",
        "display(ratings_df)\n",
        "\n",
        "# Load predictions and ratinbgs to Google Big Query\n",
        "load_predictions_and_ratings_to_bigquery(client, test_predictions_df, ratings_df, ratings_model)\n",
        "\n",
        "# Create dataframe to compare rank and use for final rank aggregation\n",
        "rank_df = ratings_df[['team_no', 'fold_number', 'rank']]\n",
        "rank_df = rank_df.rename(columns={'rank': ratings_model})\n",
        "all_ranks_df = all_ranks_df.merge(rank_df, on=['team_no','fold_number'], how='outer')\n",
        "display(all_ranks_df)\n",
        "\n",
        "# Create confusion matrix from results\n",
        "model_name_ranking = ratings_model\n",
        "results_df, cm_fig, roc_fig = create_model_results.create_model_results_df (all_y_train_true, all_y_train_pred, all_y_test_true, all_y_test_pred, train_accuracies, train_recalls, test_accuracies, test_recalls, all_y_train_pred_proba, all_y_test_pred_proba, model_name_ranking, test_flag)\n",
        "\n",
        "results_df = results_df.reset_index()\n",
        "results_df['metric_id'] = results_df.index + 1\n",
        "results_df = results_df[['metric_id', 'metric', ratings_model]]\n",
        "\n",
        "all_results_df = all_results_df.merge(results_df, on=['metric_id', 'metric'], how='outer')\n",
        "display(all_results_df)"
      ],
      "metadata": {
        "id": "tVYZYFfVYUj9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Keener Ratings\n",
        "ratings_model = 'keener'\n",
        "ratings_function = ratings_functions.keener_ratings\n",
        "\n",
        "# Call ratings function and train test split\n",
        "(ratings_df, test_predictions_df, all_y_train_true, all_y_train_pred, all_y_test_true, all_y_test_pred,\n",
        " train_accuracies, train_recalls, test_accuracies, test_recalls,\n",
        " all_y_train_pred_proba, all_y_test_pred_proba, ratings_model) =  run_rating_models_single_split (fa_cup_scores__all_train_df, fa_cup_scores_test_all_unseen_df, ratings_function, ratings_model)\n",
        "\n",
        "# Display results\n",
        "test_predictions_df.index = range(1, len(test_predictions_df) + 1)\n",
        "test_predictions_df_keener = test_predictions_df.index = range(1, len(test_predictions_df) + 1)\n",
        "display(test_predictions_df_keener)\n",
        "\n",
        "ratings_df.index = range(1, len(ratings_df) + 1)\n",
        "display(ratings_df)\n",
        "\n",
        "# Load predictions and ratinbgs to Google Big Query\n",
        "load_predictions_and_ratings_to_bigquery(client, test_predictions_df, ratings_df, ratings_model)\n",
        "\n",
        "# Create dataframe to compare rank and use for final rank aggregation\n",
        "rank_df = ratings_df[['team_no', 'fold_number', 'rank']]\n",
        "rank_df = rank_df.rename(columns={'rank': ratings_model})\n",
        "all_ranks_df = all_ranks_df.merge(rank_df, on=['team_no','fold_number'], how='outer')\n",
        "display(all_ranks_df)\n",
        "\n",
        "# Create confusion matrix from results\n",
        "model_name_ranking = ratings_model\n",
        "results_df, cm_fig, roc_fig = create_model_results.create_model_results_df (all_y_train_true, all_y_train_pred, all_y_test_true, all_y_test_pred, train_accuracies, train_recalls, test_accuracies, test_recalls, all_y_train_pred_proba, all_y_test_pred_proba, model_name_ranking, test_flag)\n",
        "\n",
        "results_df = results_df.reset_index()\n",
        "results_df['metric_id'] = results_df.index + 1\n",
        "results_df = results_df[['metric_id', 'metric', ratings_model]]\n",
        "\n",
        "all_results_df = all_results_df.merge(results_df, on=['metric_id', 'metric'], how='outer')\n",
        "display(all_results_df)"
      ],
      "metadata": {
        "id": "WsV_BMUrt390"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Trueskill Ratings\n",
        "ratings_model = 'trueskill'\n",
        "ratings_function = ratings_functions.trueskill_ratings\n",
        "\n",
        "# Call ratings function and train test split\n",
        "(ratings_df, test_predictions_df, all_y_train_true, all_y_train_pred, all_y_test_true, all_y_test_pred,\n",
        " train_accuracies, train_recalls, test_accuracies, test_recalls,\n",
        " all_y_train_pred_proba, all_y_test_pred_proba, ratings_model) =  run_rating_models_single_split (fa_cup_scores__all_train_df, fa_cup_scores_test_all_unseen_df, ratings_function, ratings_model)\n",
        "\n",
        "# Display results\n",
        "test_predictions_df.index = range(1, len(test_predictions_df) + 1)\n",
        "test_predictions_df_trueskill = test_predictions_df.index = range(1, len(test_predictions_df) + 1)\n",
        "display(test_predictions_df_trueskill)\n",
        "\n",
        "ratings_df.index = range(1, len(ratings_df) + 1)\n",
        "display(ratings_df)\n",
        "\n",
        "# Load predictions and ratinbgs to Google Big Query\n",
        "load_predictions_and_ratings_to_bigquery(client, test_predictions_df, ratings_df, ratings_model)\n",
        "\n",
        "# Create dataframe to compare rank and use for final rank aggregation\n",
        "rank_df = ratings_df[['team_no', 'fold_number', 'rank']]\n",
        "rank_df = rank_df.rename(columns={'rank': ratings_model})\n",
        "all_ranks_df = all_ranks_df.merge(rank_df, on=['team_no','fold_number'], how='outer')\n",
        "display(all_ranks_df)\n",
        "\n",
        "# Create confusion matrix from results\n",
        "model_name_ranking = ratings_model\n",
        "results_df, cm_fig, roc_fig = create_model_results.create_model_results_df (all_y_train_true, all_y_train_pred, all_y_test_true, all_y_test_pred, train_accuracies, train_recalls, test_accuracies, test_recalls, all_y_train_pred_proba, all_y_test_pred_proba, model_name_ranking, test_flag)\n",
        "\n",
        "results_df = results_df.reset_index()\n",
        "results_df['metric_id'] = results_df.index + 1\n",
        "results_df = results_df[['metric_id', 'metric', ratings_model]]\n",
        "\n",
        "all_results_df = all_results_df.merge(results_df, on=['metric_id', 'metric'], how='outer')\n",
        "display(all_results_df)\n"
      ],
      "metadata": {
        "id": "Won8EiSZuHEl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kOer_ThessNN"
      },
      "outputs": [],
      "source": [
        "# Load all ranks df to Big Query\n",
        "load_dataset_name = 'analysis_layer'\n",
        "load_table_name = 'ratings_model_ranks_unseen'\n",
        "full_table_name = f\"{load_dataset_name}.{load_table_name}\"\n",
        "pandas_gbq.to_gbq(all_ranks_df, full_table_name, project_id='birkbeck-msc-project-422917', if_exists='replace')\n",
        "\n",
        "# Load confusion matrix df to Big Query\n",
        "load_dataset_name = 'analysis_layer'\n",
        "load_table_name = 'ratings_model_confusion_matrix_results_unseen'\n",
        "full_table_name = f\"{load_dataset_name}.{load_table_name}\"\n",
        "pandas_gbq.to_gbq(all_results_df, full_table_name, project_id='birkbeck-msc-project-422917', if_exists='replace')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Rank Aggregation functions called after ratings function have run and results inserted into Big Query\n",
        "\n",
        "def borda_count_aggregation (fold_counter):\n",
        "\n",
        "    from google.cloud import bigquery\n",
        "\n",
        "    # Authenticate using the loaded key\n",
        "    credentials = service_account.Credentials.from_service_account_info(key)\n",
        "    # Set up the BigQuery client with the credentials to project\n",
        "    client = bigquery.Client(credentials=credentials, project='birkbeck-msc-project-422917')\n",
        "\n",
        "    fold_number = fold_counter\n",
        "    all_ranks = \"\"\"\n",
        "              select * from analysis_layer.ratings_model_ranks_unseen\n",
        "              \"\"\"\n",
        "\n",
        "    all_ranks_df = client.query(all_ranks).to_dataframe()\n",
        "    all_ranks_df_fold = all_ranks_df[all_ranks_df['fold_number'] == fold_number].sort_values('team_no', ascending=True)\n",
        "    all_ranks_df_fold['rank_total'] = all_ranks_df_fold[['basic_position','massey','colley','keener','trueskill']].sum(axis=1)\n",
        "    all_ranks_df_fold['overall_rank'] = all_ranks_df_fold['rank_total'].rank(method='first', ascending=True).astype(int)\n",
        "    all_ranks_df_fold['rating'] = 1 / all_ranks_df_fold['overall_rank']\n",
        "\n",
        "\n",
        "    # Create a list of tuples in the format (team_no, rating) for function\n",
        "    ratings = list(zip(all_ranks_df_fold['team_no'], all_ranks_df_fold['rating']))\n",
        "\n",
        "    return ratings\n",
        "\n",
        "def average_rank_aggregation (fold_counter):\n",
        "\n",
        "    from google.cloud import bigquery\n",
        "\n",
        "    # Authenticate using the loaded key\n",
        "    credentials = service_account.Credentials.from_service_account_info(key)\n",
        "    # Set up the BigQuery client with the credentials to project\n",
        "    client = bigquery.Client(credentials=credentials, project='birkbeck-msc-project-422917')\n",
        "\n",
        "    fold_number = fold_counter\n",
        "    all_ranks = \"\"\"\n",
        "              select * from analysis_layer.ratings_model_ranks_unseen\n",
        "              \"\"\"\n",
        "\n",
        "    all_ranks_df = client.query(all_ranks).to_dataframe()\n",
        "    all_ranks_df_fold = all_ranks_df[all_ranks_df['fold_number'] == fold_number].sort_values('team_no', ascending=True)\n",
        "    all_ranks_df_fold['rank_average'] = all_ranks_df_fold[['basic_position','massey','colley','keener','trueskill']].mean(axis=1)\n",
        "    all_ranks_df_fold['overall_rank'] = all_ranks_df_fold['rank_average'].rank(method='first', ascending=True).astype(int)\n",
        "    all_ranks_df_fold['rating'] = 1 / all_ranks_df_fold['overall_rank']\n",
        "    display(all_ranks_df_fold)\n",
        "\n",
        "\n",
        "    # Create a list of tuples in the format (team_no, rating) for function\n",
        "    ratings = list(zip(all_ranks_df_fold['team_no'], all_ranks_df_fold['rating']))\n",
        "\n",
        "    return ratings\n"
      ],
      "metadata": {
        "id": "5wHYox1G34XH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Borda Count Aggregation\n",
        "ratings_model = 'borda_count'\n",
        "ratings_function = borda_count_aggregation\n",
        "\n",
        "# Call ratings function and train test split\n",
        "(ratings_df, test_predictions_df, all_y_train_true, all_y_train_pred, all_y_test_true, all_y_test_pred,\n",
        " train_accuracies, train_recalls, test_accuracies, test_recalls,\n",
        " all_y_train_pred_proba, all_y_test_pred_proba, ratings_model) =  run_rating_models_single_split (fa_cup_scores__all_train_df, fa_cup_scores_test_all_unseen_df, ratings_function, ratings_model)\n",
        "\n",
        "# Display results\n",
        "test_predictions_df.index = range(1, len(test_predictions_df) + 1)\n",
        "test_predictions_df_borda_count = test_predictions_df.index = range(1, len(test_predictions_df) + 1)\n",
        "display(test_predictions_df_borda_count)\n",
        "\n",
        "ratings_df.index = range(1, len(ratings_df) + 1)\n",
        "display(ratings_df)\n",
        "\n",
        "# Load predictions and ratinbgs to Google Big Query\n",
        "load_predictions_and_ratings_to_bigquery(client, test_predictions_df, ratings_df, ratings_model)\n",
        "\n",
        "# Create dataframe to compare rank and use for final rank aggregation\n",
        "rank_df = ratings_df[['team_no', 'fold_number', 'rank']]\n",
        "rank_df = rank_df.rename(columns={'rank': ratings_model})\n",
        "all_ranks_df = all_ranks_df.merge(rank_df, on=['team_no','fold_number'], how='outer')\n",
        "display(all_ranks_df)\n",
        "\n",
        "# Create confusion matrix from results\n",
        "model_name_ranking = ratings_model\n",
        "results_df, cm_fig, roc_fig = create_model_results.create_model_results_df (all_y_train_true, all_y_train_pred, all_y_test_true, all_y_test_pred, train_accuracies, train_recalls, test_accuracies, test_recalls, all_y_train_pred_proba, all_y_test_pred_proba, model_name_ranking, test_flag)\n",
        "\n",
        "results_df = results_df.reset_index()\n",
        "results_df['metric_id'] = results_df.index + 1\n",
        "results_df = results_df[['metric_id', 'metric', ratings_model]]\n",
        "\n",
        "all_results_df = all_results_df.merge(results_df, on=['metric_id', 'metric'], how='outer')\n",
        "display(all_results_df)"
      ],
      "metadata": {
        "id": "NhwPdT5QfVKq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Average Rank Aggregation\n",
        "ratings_model = 'borda_count'\n",
        "ratings_function = average_rank_aggregation\n",
        "\n",
        "# Call ratings function and train test split\n",
        "(ratings_df, test_predictions_df, all_y_train_true, all_y_train_pred, all_y_test_true, all_y_test_pred,\n",
        " train_accuracies, train_recalls, test_accuracies, test_recalls,\n",
        " all_y_train_pred_proba, all_y_test_pred_proba, ratings_model) =  run_rating_models_single_split (fa_cup_scores__all_train_df, fa_cup_scores_test_all_unseen_df, ratings_function, ratings_model)\n",
        "\n",
        "\n",
        "# Display results\n",
        "test_predictions_df.index = range(1, len(test_predictions_df) + 1)\n",
        "test_predictions_df_average_rank = test_predictions_df.index = range(1, len(test_predictions_df) + 1)\n",
        "display(test_predictions_df_average_rank)\n",
        "\n",
        "ratings_df.index = range(1, len(ratings_df) + 1)\n",
        "display(ratings_df)\n",
        "\n",
        "# Load predictions and ratinbgs to Google Big Query\n",
        "load_predictions_and_ratings_to_bigquery(client, test_predictions_df, ratings_df, ratings_model)\n",
        "\n",
        "# Create dataframe to compare rank and use for final rank aggregation\n",
        "rank_df = ratings_df[['team_no', 'fold_number', 'rank']]\n",
        "rank_df = rank_df.rename(columns={'rank': ratings_model})\n",
        "all_ranks_df = all_ranks_df.merge(rank_df, on=['team_no','fold_number'], how='outer')\n",
        "display(all_ranks_df)\n",
        "\n",
        "# Create confusion matrix from results\n",
        "model_name_ranking = ratings_model\n",
        "results_df, cm_fig, roc_fig = create_model_results.create_model_results_df (all_y_train_true, all_y_train_pred, all_y_test_true, all_y_test_pred, train_accuracies, train_recalls, test_accuracies, test_recalls, all_y_train_pred_proba, all_y_test_pred_proba, model_name_ranking, test_flag)\n",
        "\n",
        "results_df = results_df.reset_index()\n",
        "results_df['metric_id'] = results_df.index + 1\n",
        "results_df = results_df[['metric_id', 'metric', ratings_model]]\n",
        "\n",
        "all_results_df = all_results_df.merge(results_df, on=['metric_id', 'metric'], how='outer')\n",
        "display(all_results_df)"
      ],
      "metadata": {
        "id": "utXVU5-gVh-Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load updated all ranks df to Big Query so Borda Count can be subsequently queried in local kemeny optimisation\n",
        "load_dataset_name = 'analysis_layer'\n",
        "load_table_name = 'ratings_model_ranks_unseen'\n",
        "full_table_name = f\"{load_dataset_name}.{load_table_name}\"\n",
        "pandas_gbq.to_gbq(all_ranks_df, full_table_name, project_id='birkbeck-msc-project-422917', if_exists='replace')"
      ],
      "metadata": {
        "id": "LtmLSzRA5Cuh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Local Kemeny Optimisation\n",
        "\n",
        "def local_kemeny_optimisation (fold_number, max_iterations=10):\n",
        "\n",
        "    def kendall_tau_distance(ranking1, ranking2):\n",
        "        distance = 0\n",
        "        for i in range(len(ranking1)):\n",
        "            for j in range(i + 1, len(ranking1)):\n",
        "                if (ranking1[i] < ranking1[j]) != (ranking2[i] < ranking2[j]):\n",
        "                    distance += 1\n",
        "        return distance\n",
        "\n",
        "    def total_kendall_tau_distance(candidate, rankings):\n",
        "        return sum(kendall_tau_distance(candidate, ranking) for ranking in rankings.values())\n",
        "\n",
        "    def local_kemeny_optimisation(rankings, initial_ranking, max_iterations):\n",
        "        n_items = len(initial_ranking)\n",
        "\n",
        "        current_ranking = initial_ranking.copy()\n",
        "        current_distance = total_kendall_tau_distance(current_ranking, rankings)\n",
        "\n",
        "        for iteration in range(max_iterations):\n",
        "            improved = False\n",
        "            for i in range(n_items - 1):\n",
        "                new_ranking = current_ranking.copy()\n",
        "                new_ranking[i], new_ranking[i+1] = new_ranking[i+1], new_ranking[i]\n",
        "                new_distance = total_kendall_tau_distance(new_ranking, rankings)\n",
        "\n",
        "                if new_distance < current_distance:\n",
        "                    current_ranking = new_ranking\n",
        "                    current_distance = new_distance\n",
        "                    improved = True\n",
        "                    print(f\"Iteration {iteration + 1}: Improved distance to {current_distance}\")\n",
        "                    break\n",
        "\n",
        "            if not improved:\n",
        "                print(f\"Stopped after {iteration + 1} iterations: No improvement\")\n",
        "                break\n",
        "\n",
        "        if iteration == max_iterations - 1:\n",
        "            print(f\"Stopped after reaching maximum iterations ({max_iterations})\")\n",
        "\n",
        "        return current_ranking\n",
        "\n",
        "    # Authenticate using the loaded key\n",
        "    # credentials = service_account.Credentials.from_service_account_info(key)\n",
        "    # Set up the BigQuery client with the credentials to project\n",
        "    client = bigquery.Client(credentials=credentials, project='birkbeck-msc-project-422917')\n",
        "\n",
        "    # Query to get the data\n",
        "    all_ranks_query = f\"\"\"\n",
        "        SELECT * FROM analysis_layer.ratings_model_ranks_unseen WHERE fold_number = {fold_number}\n",
        "    \"\"\"\n",
        "\n",
        "    all_ranks_df = client.query(all_ranks_query).to_dataframe()\n",
        "\n",
        "    # Create dictionary of rankings\n",
        "    rankings = {\n",
        "        'basic_position': all_ranks_df['basic_position'].tolist(),\n",
        "        'massey': all_ranks_df['massey'].tolist(),\n",
        "        'colley': all_ranks_df['colley'].tolist(),\n",
        "        'keener': all_ranks_df['keener'].tolist(),\n",
        "        'trueskill': all_ranks_df['trueskill'].tolist()\n",
        "    }\n",
        "\n",
        "    # Use Borda count as initial ranking\n",
        "    borda_count = all_ranks_df['borda_count'].tolist()\n",
        "\n",
        "    # Perform local Kemeny optimization\n",
        "    optimized_ranking = local_kemeny_optimisation(rankings, borda_count, max_iterations)\n",
        "    print(\"\\nTotal Kendall tau distance (Borda Count):\", total_kendall_tau_distance(borda_count, rankings))\n",
        "    print(\"Total Kendall tau distance (Optimized):\", total_kendall_tau_distance(optimized_ranking, rankings))\n",
        "\n",
        "    # Show which teams changed positions\n",
        "    team_nos = all_ranks_df['team_no'].tolist()\n",
        "\n",
        "    # Create a mapping from team number to rank for both rankings\n",
        "    borda_mapping = {team: rank for rank, team in enumerate(borda_count)}\n",
        "    optimized_mapping = {team: rank for rank, team in enumerate(optimized_ranking)}\n",
        "\n",
        "    changes = []\n",
        "    for team in team_nos:\n",
        "        borda_rank = borda_mapping.get(team, -1)\n",
        "        optimized_rank = optimized_mapping.get(team, -1)\n",
        "        if borda_rank != optimized_rank:\n",
        "            changes.append((team, borda_rank, optimized_rank))\n",
        "\n",
        "    print(\"\\nTeams that changed positions (Team, Old Position, New Position):\")\n",
        "    for change in changes:\n",
        "        print(f\"Team {change[0]}: {change[1]} -> {change[2]}\")\n",
        "\n",
        "    # Create a DataFrame with the optimized ranking\n",
        "    optimized_df = pd.DataFrame({\n",
        "        'team_no': team_nos,\n",
        "        'overall_rank': optimized_ranking\n",
        "    })\n",
        "\n",
        "    # Calculate the rating based on the optimized ranking\n",
        "    optimized_df['rating'] = 1 / optimized_df['overall_rank']\n",
        "\n",
        "    # Create a list of tuples in the format (team_no, rating) for function\n",
        "    ratings = list(zip(optimized_df['team_no'], optimized_df['rating']))\n",
        "\n",
        "    return ratings\n"
      ],
      "metadata": {
        "id": "ibBrmuaX163V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Local Kemeny Optimisation\n",
        "ratings_model = 'local_kemeny_optimisation'\n",
        "ratings_function = local_kemeny_optimisation\n",
        "\n",
        "# Call ratings function and train test split\n",
        "(ratings_df, test_predictions_df, all_y_train_true, all_y_train_pred, all_y_test_true, all_y_test_pred,\n",
        " train_accuracies, train_recalls, test_accuracies, test_recalls,\n",
        " all_y_train_pred_proba, all_y_test_pred_proba, ratings_model) =  run_rating_models_single_split (fa_cup_scores__all_train_df, fa_cup_scores_test_all_unseen_df, ratings_function, ratings_model)\n",
        "\n",
        "\n",
        "# Display results\n",
        "test_predictions_df.index = range(1, len(test_predictions_df) + 1)\n",
        "test_predictions_df_local_kemeny_optimisation = test_predictions_df.index = range(1, len(test_predictions_df) + 1)\n",
        "display(test_predictions_df_local_kemeny_optimisation)\n",
        "\n",
        "ratings_df.index = range(1, len(ratings_df) + 1)\n",
        "display(ratings_df)\n",
        "\n",
        "# Load predictions and ratinbgs to Google Big Query\n",
        "load_predictions_and_ratings_to_bigquery(client, test_predictions_df, ratings_df, ratings_model)\n",
        "\n",
        "# Create dataframe to compare rank and use for final rank aggregation\n",
        "rank_df = ratings_df[['team_no', 'fold_number', 'rank']]\n",
        "rank_df = rank_df.rename(columns={'rank': ratings_model})\n",
        "all_ranks_df = all_ranks_df.merge(rank_df, on=['team_no','fold_number'], how='outer')\n",
        "display(all_ranks_df)\n",
        "\n",
        "# Create confusion matrix from results\n",
        "model_name_ranking = ratings_model\n",
        "results_df, cm_fig, roc_fig = create_model_results.create_model_results_df (all_y_train_true, all_y_train_pred, all_y_test_true, all_y_test_pred, train_accuracies, train_recalls, test_accuracies, test_recalls, all_y_train_pred_proba, all_y_test_pred_proba, model_name_ranking, test_flag)\n",
        "\n",
        "results_df = results_df.reset_index()\n",
        "results_df['metric_id'] = results_df.index + 1\n",
        "results_df = results_df[['metric_id', 'metric', ratings_model]]\n",
        "\n",
        "all_results_df = all_results_df.merge(results_df, on=['metric_id', 'metric'], how='outer')\n",
        "display(all_results_df)"
      ],
      "metadata": {
        "id": "-l44e7fi5y4n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load final ranks including Local Kemeny Optimisation\n",
        "load_dataset_name = 'analysis_layer'\n",
        "load_table_name = 'ratings_model_ranks_unseen'\n",
        "full_table_name = f\"{load_dataset_name}.{load_table_name}\"\n",
        "pandas_gbq.to_gbq(all_ranks_df, full_table_name, project_id='birkbeck-msc-project-422917', if_exists='replace')"
      ],
      "metadata": {
        "id": "wdaot3i69jpv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Change default ouput directory\n",
        "os.chdir('/content/drive/MyDrive/birkbeck_msc-project/output_files')\n",
        "# Save ranks to excel\n",
        "all_results_df.to_excel(\"all_rating_ranking_results_unseen.xlsx\")"
      ],
      "metadata": {
        "id": "P1KPQ9Q8zE7s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load updated Fold results to Google Big Query\n",
        "\n",
        "# Merge fold results from each model\n",
        "all_data_act_pred_df = pd.concat([test_predictions_df_basic_position, test_predictions_df_massey, test_predictions_df_colley, test_predictions_df_keener, test_predictions_df_borda_count, test_predictions_df_average_rank, test_predictions_df_local_kemeny_optimisation])\n",
        "all_data_act_pred_df = all_data_act_pred_df.rename(columns=lambda x: x.lower().replace(' ','(_)').replace('(', '').replace(')', ''))\n",
        "display(all_data_act_pred_df)\n",
        "\n",
        "# Write to Excel\n",
        "all_data_act_pred_df.to_excel(\"all_rating_ranking_model_all_predictions_unseen.xlsx\")\n",
        "\n",
        "# Load fold results data from Excel to Google BigQuery\n",
        "fold_results_from_excel = pd.read_excel(\"all_rating_ranking_model_all_predictions_unseen.xlsx\")\n",
        "load_dataset_name = 'analysis_layer'\n",
        "load_table_name = 'all_rating_ranking_model_all_predictions_unseen'\n",
        "full_table_name = f\"{load_dataset_name}.{load_table_name}\"\n",
        "\n",
        "pandas_gbq.to_gbq(fold_results_from_excel, full_table_name,\n",
        "                  project_id='birkbeck-msc-project-422917',\n",
        "                  if_exists='replace')\n",
        "\n",
        "print(f\"\\nData loaded to BigQuery table: {full_table_name}\")"
      ],
      "metadata": {
        "id": "PYkfFhoD33dl"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "L4",
      "mount_file_id": "https://github.com/chorltonm/fa-cup-upsets/blob/main/notebooks/models/rating_models.ipynb",
      "authorship_tag": "ABX9TyPyG/vXqv2w3J+Zqdyq20+5",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}